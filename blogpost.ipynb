{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlyY-v4OEIDw"
   },
   "source": [
    "# Reproducing 'Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network'\n",
    "\n",
    "In this notebook we reproduce 91 generated images from the paper **Real-Time Single Image and Video Super-Resolution \n",
    "Using an Efficient Sub-Pixel Convolutional Neural Network**. The code has been created following the paper with gaps \n",
    "filled in by ourselves.\n",
    "The paper presents a convulutional neural network capable of real-time Super-Resolution (SR).\n",
    "They designed a CNN architecture where the feature maps are extracted in the Low-Resolution(LR) space, \n",
    "and introducing an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale\n",
    "the final LR feature maps into the High-Resolution(HR) output.\n",
    "A researcher would like a reproduction of this to test on his high-res dataset.\n",
    "\n",
    "---\n",
    "### SRCNN\n",
    "Recovers SR output image from an upscaled and interpolated version of LR image. \n",
    "Uses bicubic interpolation (as a special case of the deconvolution layer) to upscale image.\n",
    "Deconvolution layer a.k.a. Transposed convolution layer: \\\\\n",
    "_insert image_\n",
    "\n",
    "\n",
    "### ESPCN\n",
    "First apply a  layer CNN directly to LR image, then apply a sub-pixel convolution layer that upscales the LR feature \n",
    "maps to produce the SR output image. (to avoid upscaling LR before feeding it into the network) \\\\\n",
    "_insert image_ \\\\\n",
    "For a network composed of L layers, the first L−1 layers can be described as follows: \\\\\n",
    "$f^1(\\boldsymbol{I}^{LR};W_1, b_1)= \\phi (W_1 \\ast \\boldsymbol{I}^{LR}+b_1)$ \\\\\n",
    "$f^l(\\boldsymbol{I}^{LR};W_{1:l}, b_{1:l})= \\phi (W_1 \\ast f^{l-1}(\\boldsymbol{I}^{LR})+b_l)$ \\\\\n",
    "Where $W_l , b_l , l \\in (1, L - 1)$ are learnable network weights and biases respectively.  \n",
    "$W_l$ is a 2D convolution tensor of size $n_{l-1} \\times n_l \\times k_l \\times k_l$ , where $n_l$ is the number of \n",
    "features at layer $l$, $n_0 = C$, and $k_l$ is the filter size at layer $l$. \n",
    "The biases $b_l$ are vectors of length $n_l$ . The nonlinearity function (or activation function) $\\phi$ is applied \n",
    "element-wise and is fixed. The last layer $f^L$ has to convert the LR feature maps to a HR image $\\boldsymbol{I}^{SR}$.\n",
    "\n",
    "---\n",
    "\n",
    "## Experiment Setup \n",
    "\n",
    "Input = H x W x C\n",
    "Output = rH x rW x C\n",
    "\n",
    "Apply l layer CNN directly to Low Resolution (LR)\n",
    "Apply sub-pixel convolution layer upscaling LR feature maps\n",
    "\n",
    "Each layer except the last: \\\\\n",
    "$f^1(\\boldsymbol{I}^{LR};W_1, b_1)= \\phi (W_1 \\ast \\boldsymbol{I}^{LR}+b_1)$ \\\\\n",
    "$f^l(\\boldsymbol{I}^{LR};W_{1:l}, b_{1:l})= \\phi (W_1 \\ast f^{l-1}(\\boldsymbol{I}^{LR})+b_l)$\n",
    "\n",
    "W & b learnable parameters\n",
    "\n",
    "W is a 2D convolution tensor of size n_(l-1) x n_l  x k_l x k_l\n",
    "\n",
    "n_l is the amount of features of layer l, n_0 = C, k_l is the filter size at l\n",
    "\n",
    "biases are of length n_l\n",
    "\n",
    "activation function is fixed and element-wise\n",
    "\n",
    "Last layer converts LR to HR\n",
    "\n",
    "Efficient sub-pixel convolution layer: (biggest addition)\n",
    "\n",
    "Convolution with stride 1/r over LR with filter W_s of size k_s and weight spacing 1/r \\\\\n",
    "Weights between pixels are not activated and do not need to be calculated \\\\\n",
    "The number of activation patterns is exactly r^2. \\\\\n",
    "Each pattern has at most ceil(k_s/r)^2 weights \\\\\n",
    "Patterns are periodically activated during convolution of the filter depending on the subpixel location mod(x, r), mod(y,r), where x and y are the output pixel coordinates in HR. \\\\\n",
    "Solution for mod(k_s, r) = 0 \\\\\n",
    "Last layer: note NO ACTIVATION \\\\\n",
    " $I^{SR} = f^L(I^{LR}) = PS(W_L * f^{L-1}(I^{LR})+b_L)$ \\\\\n",
    "PS is a periodic shuffling operator (sort of mapping) \\\\\n",
    "_formule naar latex_ \\\\\n",
    "W_L has shape n_(L-1) x r^2C x k_L x k_L \\\\\n",
    "k_L = k_s/r and mod(k_s, r) = 0\n",
    "\n",
    "Loss function: (mean squared error) \\\\\n",
    "_formule naar latex_ \\\\\n",
    "Preshuffle training data avoiding the use of PS\n",
    "\n",
    "---\n",
    "## Discussion\n",
    "We did not manage to reproduce the results from table 1 from the [original paper](https://arxiv.org/pdf/1609.05158v2.pdf), column ESPCN (91). When trying to recreate the results from the paper, we recreated the model as described in the paper, however some hyperparameters and architectural decisions were not given. The following information was missing from the paper and made it harder to reproduce the results:\n",
    "* **Optimizer**: It is not stated what optimizer is used when training the network.\n",
    "* **How the learning rate changes from high to low**: It is not stated how the learning rate changes between its start- and end-value. A parameter ‘mu’ is mentioned, where the learning rate decreases with an unknown amount when the improvement is smaller than ‘mu’. However, the value of ‘mu’ is not given.\n",
    "* **Gaussian blur**: To simulate the downscale image when training in a realistic manner, the training images are blurred. However, the intensity of this blur is not given.\n",
    "\n",
    "In order to find optimal values for these parameters we export the values of these parameters and their respective results to a *.csv* file after each training session. This allowed us to easily run the training multiple times with different parameter sets and compare their respective results. Though we were able to optimize our network by finding better parameters, we were still unable to reproduce the results from the paper.\n",
    "\n",
    "When implementing and training **ESPCN** we encountered a few other parameters that were not necessarily missing from the paper, but would have been beneficial to have more information on, namely:\n",
    "\n",
    "* **Batch size**: The batch size was not given, nor was it clear from the paper whether batches were used at all. This information would have been useful to interpret their training times and compare them to ours.\n",
    "* **Model validation**: The T91 dataset was used for training. However, it was not specified whether this dataset was split up into a train, test and validation set. Was (k-fold) cross validation used, was some other validation method used or did they train on the full dataset without validation?\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "# The model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xw7N_3dGGdtk"
   },
   "source": [
    "Everything we need is imported and help methods are created\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuwZj7xuGAix"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.filters import *\n",
    "from skimage.transform import *\n",
    "import os\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YE5tg_8GkFF"
   },
   "source": [
    "hyperparameters to set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmxbwLXHGMwY"
   },
   "outputs": [],
   "source": [
    "r = 3  # upscaling ratio\n",
    "blur = 0.25  # gaussian blur\n",
    "lr_start = 0.01\n",
    "lr_end = 0.0001\n",
    "mu = 1e-6  # threshold for lowering the lr\n",
    "no_learning_threshold = 1e-8  # threshold for stopping training of no improvement has been made for 'repeats' epochs\n",
    "repeats = 100\n",
    "batch_size = 1\n",
    "train_test_fraction = 0.8\n",
    "C = 3 # amount of colour channels\n",
    "dataset = \"T91\"\n",
    "epoch_save_interval = 100\n",
    "minibatch_size = 100\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6GNLWCmGnz9"
   },
   "source": [
    "Periodic Shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PS(T, r):\n",
    "    T = np.transpose(T, (1, 2, 0))\n",
    "    rW = r * len(T)\n",
    "    rH = r * len(T[0])\n",
    "    C = len(T[0][0]) / (r * r)\n",
    "\n",
    "    # make sure C is an integer and cast if this is the case\n",
    "    assert (C == int(C))\n",
    "    C = int(C)\n",
    "\n",
    "    res = np.zeros((rW, rH, C))\n",
    "\n",
    "    for x in range(len(res)):\n",
    "        for y in range(len(res[x])):\n",
    "            for c in range(len(res[x][y])):\n",
    "                res[x][y][c] = \\\n",
    "                    T[x // r][y // r][C * r * (y % r) + C * (x % r) + c]\n",
    "    return res\n",
    "\n",
    "\n",
    "def PS_inv(img, r):\n",
    "    r2 = r * r\n",
    "    W = len(img) / r\n",
    "    H = len(img[0]) / r\n",
    "    C = len(img[0][0])\n",
    "    Cr2 = C * r2\n",
    "\n",
    "    # Make sure H and W are integers\n",
    "    assert (int(H) == H and int(W) == W)\n",
    "    H, W = int(H), int(W)\n",
    "\n",
    "    res = np.zeros((W, H, Cr2))\n",
    "\n",
    "    for x in range(len(img)):\n",
    "        for y in range(len(img[x])):\n",
    "            for c in range(len(img[x][y])):\n",
    "                res[x // r][y // r][C * r * (y % r) + C * (x % r) + c] = img[x][y][c]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data en sample uitleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKgQGMS9GM8r"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Downsample images\n",
    "- gaussian blur\n",
    "- resize by downsample factor (using interpolation)\n",
    "How To Use:\n",
    "    function lr_dataset_from_path takes a path to the dataset of HR image png files and returns an ndarray to use for training the model\n",
    "For debugging/showing examples:\n",
    "    (see bottom of file)\n",
    "    save_png set to True to save resulting lr images in specified directory.\n",
    "    !check the param_ varaiables\n",
    "\"\"\"\n",
    "\n",
    "SUBSAMPLING_STRIDE_SIZE = 14\n",
    "SUBSAMPLING_SAMPLE_SIZE = 17\n",
    "\n",
    "\n",
    "# hr_dataset_path: dir to the hr_dataset png files\n",
    "# downscale: downscale factor, e.g. if original image 64*64 and downscale=2 then result will be 32*32\n",
    "# returns list of numpy.ndarray representing the lr_images\n",
    "def lr_dataset_from_path(hr_dataset_path, downscale):\n",
    "    original_filenames = os.listdir(hr_dataset_path)\n",
    "    original_images = []\n",
    "    for file in original_filenames:\n",
    "        original_images.append(plt.imread(hr_dataset_path + '/' + file))\n",
    "    return lr_images(original_images, downscale)  # ndarray of images\n",
    "\n",
    "\n",
    "def torchDataloader_from_path(hr_dataset_path, downscale, gaussian_sigma, batch_size):\n",
    "    original_filenames = os.listdir(hr_dataset_path)\n",
    "    original_images = []\n",
    "    for file in original_filenames:\n",
    "        original_images.append(plt.imread(hr_dataset_path + '/' + file))\n",
    "\n",
    "    # subsample\n",
    "    subsamples_hr = []\n",
    "    subsamples_hr_rev_shuff = []\n",
    "    for i in range(len(original_images)):\n",
    "        temp_subsamples = subsample(original_images[i], downscale)\n",
    "        subsamples_hr += temp_subsamples\n",
    "        for sample_indx in range(len(temp_subsamples)):\n",
    "            subsamples_hr_rev_shuff.append(PS_inv(temp_subsamples[sample_indx], downscale))  # labels\n",
    "    lr_dataset = lr_images(subsamples_hr, downscale, gaussian_sigma)  # ndarray of images\n",
    "    return toDataloader(lr_dataset, subsamples_hr_rev_shuff, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Takes list of images and provide LR images in form of numpy array\n",
    "def lr_images(images_real, downscale, gaussianSigma):\n",
    "    lr_images = []\n",
    "    for img in range(len(images_real)):\n",
    "        img_blurred = gaussian(images_real[img], sigma=gaussianSigma,\n",
    "                               multichannel=True)  # multichannel blurr so that 3rd channel is not blurred\n",
    "        lr_images.append(resize(img_blurred, (img_blurred.shape[0] // downscale, img_blurred.shape[1] // downscale)))\n",
    "    return lr_images\n",
    "\n",
    "\n",
    "# extract a 17r*17r subsample from original image, no overlap so every pixel appears at most once in output\n",
    "def subsample(image_real, downscale):\n",
    "    subsample_size = SUBSAMPLING_SAMPLE_SIZE * downscale\n",
    "    subsample_stride = SUBSAMPLING_STRIDE_SIZE * downscale\n",
    "    subsamples = []\n",
    "    for y in range(math.floor((image_real.shape[0] - (subsample_size - subsample_stride)) / subsample_stride)):\n",
    "        for x in range(math.floor((image_real.shape[1] - (subsample_size - subsample_stride)) / subsample_stride)):\n",
    "            ss = image_real[(y * subsample_stride):(y * subsample_stride) + subsample_size,\n",
    "                 (x * subsample_stride):(x * subsample_stride) + subsample_size]\n",
    "            subsamples.append(ss)\n",
    "\n",
    "    return subsamples\n",
    "\n",
    "\n",
    "# returns a torch Dataloader (to iterate over training data) using the training data samples and traing data labels\n",
    "def toDataloader(train_data, train_labels, batch_size):\n",
    "    labeled_data = []\n",
    "    for i in range(len(train_data)):\n",
    "        labeled_data.append([np.transpose(train_data[i], (2, 0, 1)), np.transpose(train_labels[i], (2, 0, 1))])\n",
    "    trainDataloader = DataLoader(labeled_data, batch_size=batch_size, shuffle=True)\n",
    "    return trainDataloader\n",
    "\n",
    "# Load the data\n",
    "dataloader = torchDataloader_from_path('./datasets/' + dataset, r, blur, batch_size)\n",
    "train_size = int(train_test_fraction * len(dataloader.dataset))\n",
    "test_size = len(dataloader.dataset) - train_size\n",
    "train_set, test_set = torch.utils.data.random_split(dataloader.dataset, [train_size, test_size])\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J017G3b9Grk6"
   },
   "source": [
    "neural network uitleg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-dbqVPjGNCd"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, r, C):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(C, 64, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, r * r * C, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQYuI340GvRZ"
   },
   "source": [
    "training uitleg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_UAB7-mzGM_0"
   },
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print(\"starting training at: \" + str(start_time))\n",
    "\n",
    "net = Net(r, C)\n",
    "net.double()\n",
    "\n",
    "if use_gpu:\n",
    "    net = net.cuda()\n",
    "\n",
    "# define loss fuction\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr_start, momentum=0.75) # momentum???\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "epoch = 0\n",
    "last_epoch_loss_test = float(\"inf\")\n",
    "last_epoch_loss_train = float(\"inf\")\n",
    "ni_counter = 0  # counts the amount of epochs no where no improvement has been made\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "dt_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "models_folder = \"models\"\n",
    "model_name = \"{}_espcnn_r{}\".format(dt_string, r)\n",
    "\n",
    "try:\n",
    "    os.mkdir(models_folder + '/' + model_name)\n",
    "except:\n",
    "    print(\"Folder {} already exists, overwritting model data\".format(models_folder + '/' + model_name))\n",
    "model_dest = models_folder + '/' + model_name + \"/model_epoch_\"\n",
    "best_model_dest = models_folder + '/' + model_name + \"/best_model\"\n",
    "lr = lr_start\n",
    "\n",
    "best_test_loss = 100000  # start with dummy value, keep track of best loss on test dataset\n",
    "best_epoch = 0\n",
    "while True:  # loop over the dataset multiple times\n",
    "    epoch_loss_train = 0.0\n",
    "    running_loss_train = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs.double())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        epoch_loss_train += outputs.shape[0] * loss.item()\n",
    "        running_loss_train += loss.item()\n",
    "        if i % minibatch_size == minibatch_size - 1:  # print every 2000 mini-batches\n",
    "            print('[%d, %5d] train_loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, running_loss_train / minibatch_size))\n",
    "            running_loss_train = 0.0\n",
    "    epoch_loss_train = epoch_loss_train / len(train_dataloader.dataset)\n",
    "    print(epoch + 1, epoch_loss_train)\n",
    "\n",
    "    epoch_loss_test = 0.0\n",
    "    running_loss_test = 0.0\n",
    "    for i, data in enumerate(test_dataloader, 0):  # get loss on test dataset\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs.double())\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # print statistics\n",
    "        epoch_loss_test += outputs.shape[0] * loss.item()\n",
    "        running_loss_test += loss.item()\n",
    "        if i % minibatch_size == minibatch_size - 1:  # print every 2000 mini-batches\n",
    "            print('[%d, %5d] test_loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, running_loss_test / minibatch_size))\n",
    "            running_loss_test = 0.0\n",
    "    epoch_loss_test = epoch_loss_test / len(test_dataloader.dataset)\n",
    "    print(epoch + 1, epoch_loss_test)\n",
    "\n",
    "    improvement = best_test_loss - epoch_loss_test\n",
    "\n",
    "    if epoch_loss_test < best_test_loss:  # save best model, 'best' meaning lowest loss on test set\n",
    "        best_test_loss = epoch_loss_test\n",
    "        torch.save(net.state_dict(), best_model_dest)  # overwrite best model so the best model filename doesn't change\n",
    "        best_epoch = epoch\n",
    "        best_epoch_train_loss = epoch_loss_train\n",
    "\n",
    "    print(\"epoch \" + str(epoch + 1) + \": improvement = \" + str(improvement))\n",
    "    if improvement < no_learning_threshold:\n",
    "        ni_counter += 1\n",
    "    else:\n",
    "        ni_counter = 0\n",
    "\n",
    "    if ni_counter >= repeats:  # stop training if no improvement has been made for 100 epochs\n",
    "        break\n",
    "\n",
    "    # If  the improvement is too small, make the learning rate smaller\n",
    "    if improvement < mu and lr > lr_end:\n",
    "        lr = lr / 10\n",
    "        print(\"Learning rate decreased to:\", lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    losses_train.append(epoch_loss_train)\n",
    "    losses_test.append(epoch_loss_test)\n",
    "    last_epoch_loss_train = epoch_loss_train\n",
    "    last_epoch_loss_test = epoch_loss_test\n",
    "\n",
    "    if epoch % epoch_save_interval == 0:\n",
    "        torch.save(net.state_dict(), model_dest + str(epoch + 1))\n",
    "    epoch += 1\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print('Finished training at: ' + str(end_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uitleg Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(original, compressed):\n",
    "    mse = np.mean((original - compressed) ** 2)\n",
    "    if (mse == 0):  # MSE is zero means no noise is present in the signal .\n",
    "        # Therefore PSNR have no importance.\n",
    "        return 100\n",
    "    max_pixel = 255.0\n",
    "    psnr = 20 * log10(max_pixel / sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "\n",
    "def average_PSNR(folder, net, r, gaussianSigma):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = plt.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            img = resize(img, ((img.shape[0] // r) * r, (img.shape[1] // r) * r))\n",
    "            images.append(img)\n",
    "\n",
    "    sumPSNR = 0\n",
    "    for og_img in images:\n",
    "        img_blurred = gaussian(og_img, sigma=gaussianSigma,\n",
    "                               multichannel=True)  # multichannel blurr so that 3rd channel is not blurred\n",
    "        img = resize(img_blurred, (img_blurred.shape[0] // r, img_blurred.shape[1] // r))\n",
    "        if (len(img.shape) == 2):  # convert image to rgb if it is grayscale\n",
    "            img = np.stack((img, img, img), axis=2)\n",
    "            og_img = np.stack((og_img, og_img, og_img), axis=2)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = torch.Tensor(img).unsqueeze(0).double()\n",
    "        result = net(img).detach().numpy()\n",
    "        sumPSNR += PSNR(PS(result[0], r) * 255, og_img * 255)\n",
    "\n",
    "    return sumPSNR / len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the results and save them to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving train and test loss')\n",
    "np.save(models_folder + '/' + model_name + '/loss_train', losses_train)\n",
    "np.save(models_folder + '/' + model_name + '/loss_test', losses_test)\n",
    "\n",
    "net.load_state_dict(torch.load(best_model_dest))\n",
    "net.eval()\n",
    "\n",
    "net.cpu()\n",
    "set5_PSNR = average_PSNR(\"./datasets/testing/Set5\", net, r, blur)\n",
    "set14_PSNR = average_PSNR(\"./datasets/testing/Set14\", net, r, blur)\n",
    "\n",
    "print(\"Finished validation \\n\")\n",
    "\n",
    "print(\"dataset:               \" + dataset)\n",
    "print(\"psnr Set5:             \" + str(set5_PSNR))\n",
    "print(\"psnr Set14:            \" + str(set14_PSNR))\n",
    "print(\"best epoch:            \" + str(best_epoch))  # epoch with the lowest loss on the test dataset\n",
    "print(\"loss on training set:  \" + str(best_epoch_train_loss))  # loss for the best epoch\n",
    "print(\"loss on test set:      \" + str(best_test_loss))  # loss for the best epoch\n",
    "print(\"r:                     \" + str(r))\n",
    "print(\"blur:                  \" + str(blur))\n",
    "print(\"lr_start:              \" + str(lr_start))\n",
    "print(\"lr_end:                \" + str(lr_end))\n",
    "print(\"mu:                    \" + str(mu))\n",
    "print(\"no_learning_threshold: \" + str(no_learning_threshold))\n",
    "print(\"epochs:                \" + str(epoch + 1))\n",
    "print(\"training duration:     \" + str(end_time - start_time))\n",
    "print(\"batch_size:            \" + str(batch_size))\n",
    "print(\"train_test_fraction:   \" + str(train_test_fraction))\n",
    "print(\"model:                 \" + model_name)\n",
    "\n",
    "with open(models_folder + '/' + model_name + '/results.csv', mode='w') as csv_file:\n",
    "    fieldnames = ['dataset', 'psnr_Set5', 'psnr_Set14', 'best_epoch', 'training_loss', 'test_loss', 'r', 'blur', 'lr_start', 'lr_end', 'mu', 'no_learning_threshold', 'epochs', 'training_duration', 'batch_size', 'train_test_fraction', 'model']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({\n",
    "        'dataset': dataset,\n",
    "        'psnr_Set5': set5_PSNR,\n",
    "        'psnr_Set14': set14_PSNR,\n",
    "        'best_epoch': best_epoch,\n",
    "        'training_loss': best_epoch_train_loss,\n",
    "        'test_loss': best_test_loss,\n",
    "        'r': r,\n",
    "        'blur': blur,\n",
    "        'lr_start': lr_start,\n",
    "        'lr_end': lr_end,\n",
    "        'mu': mu,\n",
    "        'no_learning_threshold': no_learning_threshold,\n",
    "        'epochs': (epoch + 1),\n",
    "        'training_duration': (end_time - start_time),\n",
    "        'batch_size': batch_size,\n",
    "        'train_test_fraction': train_test_fraction,\n",
    "        'model': model_name})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NB69kyUtGzRF"
   },
   "source": [
    "resultaten op een image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XE42YlokGM7e"
   },
   "outputs": [],
   "source": [
    "_resultaten block_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYKfnMHWG3Ht"
   },
   "source": [
    "resultaten op een video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJqExq7cGM58"
   },
   "outputs": [],
   "source": [
    "_block met video resultaten_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "orV0awMeG7GJ"
   },
   "source": [
    "We succeeded in reproducing the 91 pictures and the high-res video from the researcher requesting this."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "blogpost.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "#Reproducing ESPCN\n",
     "\n",
     "In this notebook we reproduce 91 generated images from the paper **Real-Time Single Image and Video Super-Resolution \n",
     "Using an Efficient Sub-Pixel Convolutional Neural Network**. The code has been created following the paper with gaps \n",
     "filled in by ourselves.\n",
     "The paper presents a convulutional neural network capable of real-time Super-Resolution (SR).\n",
     "They designed a CNN architecture where the feature maps are extracted in the Low-Resolution(LR) space, \n",
     "and introducing an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale\n",
     "the final LR feature maps into the High-Resolution(HR) output.\n",
     "\n",
     "---\n",
     "### SRCNN\n",
     "Recovers SR output image from an upscaled and interpolated version of LR image. \n",
     "Uses bicubic interpolation (as a special case of the deconvolution layer) to upscale image.\n",
     "Deconvolution layer a.k.a. Transposed convolution layer:\n",
     "_insert image_\n",
     "\n",
     "\n",
     "### ESPCN\n",
     "First apply a  layer CNN directly to LR image, then apply a sub-pixel convolution layer that upscales the LR feature \n",
     "maps to produce the SR output image. (to avoid upscaling LR before feeding it into the network)\n",
     "_insert image_\n",
     "For a network composed of L layers, the first L−1 layers can be described as follows:\n",
     "$f^1(\\boldsymbol{I}^{LR};W_1, b_1)= \\phi (W_1 \\ast \\boldsymbol{I}^{LR}+b_1)$\n",
     "$f^l(\\boldsymbol{I}^{LR};W_{1:l}, b_{1:l})= \\phi (W_1 \\ast f^{l-1}(\\boldsymbol{I}^{LR})+b_l)$\n",
     "Where $W_l , b_l , l \\in (1, L - 1)$ are learnable network weights and biases respectively.  \n",
     "$W_l$ is a 2D convolution tensor of size $n_{l-1} \\times n_l \\times k_l \\times k_l$ , where $n_l$ is the number of \n",
     "features at layer $l$, $n_0 = C$, and $k_l$ is the filter size at layer $l$. \n",
     "The biases $b_l$ are vectors of length $n_l$ . The nonlinearity function (or activation function) $\\phi$ is applied \n",
     "element-wise and is fixed. The last layer $f^L$ has to convert the LR feature maps to a HR image $\\boldsymble{I}^{SR}$.\n",
     "\n",
     "---\n",
     "\n",
     "## Experiment Setup \n",
     "\n",
     "Input = H x W x C\n",
     "Output = rH x rW x C\n",
     "\n",
     "Apply l layer CNN directly to Low Resolution (LR)\n",
     "Apply sub-pixel convolution layer upscaling LR feature maps\n",
     "\n",
     "Each layer except the last:\n",
     "$f^1(\\boldsymbol{I}^{LR};W_1, b_1)= \\phi (W_1 \\ast \\boldsymbol{I}^{LR}+b_1)$\n",
     "$f^l(\\boldsymbol{I}^{LR};W_{1:l}, b_{1:l})= \\phi (W_1 \\ast f^{l-1}(\\boldsymbol{I}^{LR})+b_l)$\n",
     "\n",
     "W & b learnable parameters\n",
     "W is a 2D convolution tensor of size n_(l-1) x n_l  x k_l x k_l\n",
     "    n_l is the amount of features of layer l, n_0 = C, k_l is the filter size at l\n",
     "    biases are of length n_l\n",
     "    activation function is fixed and element-wise\n",
     "\n",
     "Last layer converts LR to HR\n",
     "\n",
     "Efficient sub-pixel convolution layer: (biggest addition)\n",
     "Convolution with stride 1/r over LR with filter W_s of size k_s and weight spacing 1/r\n",
     "Weights between pixels are not activated and do not need to be calculated\n",
     "The number of activation patterns is exactly r^2.\n",
     "Each pattern has at most ceil(k_s/r)^2 weights\n",
     "Patterns are periodically activated during convolution of the filter depending on the subpixel location mod(x, r), mod(y,r), where x and y are the output pixel coordinates in HR.\n",
     "Solution for mod(k_s, r) = 0\n",
     "Last layer: note NO ACTIVATION\n",
     " $$I^{SR} = f^L(I^{LR}) = PS(W_L * f^{L-1}(I^{LR})+b_L)$$\n",
     "PS is a periodic shuffling operator (sort of mapping)\n",
     "_formule naar latex_\n",
     "W_L has shape n_(L-1) x r^2C x k_L x k_L\n",
     "k_L = k_s/r and mod(k_s, r) = 0\n",
     "\n",
     "Loss function: (mean squared error)\n",
     "_formule naar latex_\n",
     "Preshuffle training data avoiding the use of PS\n",
     "\n",
     "---\n",
     "# The model\n",
     "\n",
     "Everything we need is imported\n",
     "_imports block_\n",
     "\n",
     "---\n",
     "\n",
     "hyperparameters to set\n",
     "\n",
     "_hyperparameters block_"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
