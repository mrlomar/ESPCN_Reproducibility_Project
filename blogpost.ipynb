{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlyY-v4OEIDw"
   },
   "source": [
    "![figure 0](rsc/teaser.png)\n",
    "\n",
    "Reproduced by TU Delft Deep learning course students:\n",
    "Marijn de Schipper, M.deschipper@student.tudelft.nl, 4578279\n",
    "Tom Edixhoven, t.f.edixhoven@student. tudelft.nl, 4610075\n",
    "Gwennan Smitskamp, g.m.smitskamp@student.tudelft.nl, 4349822\n",
    "Olivier Dikken, o.d.f.dikken@student.tudelft.nl, 4223209\n",
    "\n",
    "\n",
    "# Reproducing 'Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network'\n",
    "\n",
    "In this notebook, we reproduce the fourth column of Table 1 from the paper [**Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network**](https://arxiv.org/pdf/1609.05158v2.pdf). The code has been created following the paper with gaps \n",
    "filled in by ourselves.\n",
    "The paper presents a convolutional neural network capable of real-time Super-Resolution (SR).\n",
    "They designed a CNN architecture where the feature maps are extracted in the Low-Resolution(LR) space \n",
    "and introducing an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale\n",
    "the final LR feature maps into the High-Resolution(HR) output.\n",
    "A researcher would like a reproduction of this to test on his high-res dataset.\n",
    "\n",
    "\n",
    "---\n",
    "### ESPCN\n",
    "For the problem of single image super-resolution (SISR), the authors of this paper set out to improve the network known as SRCN, discussed in [this paper](https://ieeexplore.ieee.org/abstract/document/7115171). SRCNN approaches the problem by increasing the resolution of the LR image before the first layer of the network. The authors argue this is inefficient since the processing speed of convolutional networks directly depends on the resolution of the input image. Therefore they propose a faster network called ESPCN.\n",
    "\n",
    "The new architecture improves this by increasing the resolution from LR to HR only at the very end of the network and super-resolves HR data from LR feature maps, by first applying a multi-layer CNN directly to LR image and then applying a sub-pixel convolution layer that upscales the LR feature maps to produce the SR output image.  Figure 1 (retrieved from [the original paper](https://arxiv.org/pdf/1609.05158v2.pdf)) shows a visualisation of the network, the rest of this section is dedicated to the mathematical description of the network.\n",
    "\n",
    "![figure 1](rsc/ESPCN.PNG)\n",
    "\n",
    "For a network composed of L layers, $n_{L-1}$ upscaling filters for the $n_{L-1}$ feature maps are learned. The first L−1 layers can be described as follows:  \n",
    "  \n",
    "$f^1(\\boldsymbol{I}^{LR};W_1, b_1)= \\phi (W_1 \\ast \\boldsymbol{I}^{LR}+b_1)$  \n",
    "$f^l(\\boldsymbol{I}^{LR};W_{1:l}, b_{1:l})= \\phi (W_1 \\ast f^{l-1}(\\boldsymbol{I}^{LR})+b_l)$  \n",
    "  \n",
    "Where $W_l , b_l , l \\in (1, L - 1)$ are learnable network weights and biases respectively.  \n",
    "  \n",
    "$W_l$ is a 2D convolution tensor of size $n_{l-1} \\times n_l \\times k_l \\times k_l$ , where $n_l$ is the number of \n",
    "features at layer $l$, $n_0 = C$, and $k_l$ is the filter size at layer $l$.  \n",
    "The biases $b_l$ are vectors of length $n_l$ . The nonlinearity function (or activation function) $\\phi$ is applied \n",
    "element-wise and is fixed. The last layer $f^L$ converts the LR feature maps to an HR image $\\boldsymbol{I}^{SR}$.  \n",
    "  \n",
    "The last layer is an efficient sub-pixel convolution layer. The authors propose a convolution with stride $\\frac{1}{r}$ in de LR space with a filter $W_s$ of size $k_s$ with a weight spacing $\\frac{1}{r}$. This way, each activation pattern is periodically activated during the convolution of the filter depending on different sub-pixel locations. A way to implement this can be described in the following way:  \n",
    "$\\boldsymbol{I}^{SR}=f^L (\\boldsymbol{I}^{LR}) = \\mathcal{PS} (W_L \\ast f^{L-1} (\\boldsymbol{I}^{LR}) + b_L)$\n",
    "  \n",
    "Where $\\mathcal{PS}$ is an periodic shuffling operator that rearranges the elements of a $H \\times W \\times C \\cdot r^2$ tensor to a tensor of shape $rH \\times rW \\times C$. \n",
    "The periodic shuffle can be denoted as follows:\n",
    "$\\mathcal{PS} (T)_{x,y,c} = T_{\\lfloor x/r \\rfloor, \\lfloor y/r \\rfloor , C\\cdot r \\cdot mod(y,r) + C\\cdot mod(x,r) + c}$.\n",
    "The convolution operator $W_L$ thus has shape $n_{L - 1} \\times C \\cdot r^2 \\times k_L \\times k_L$. \n",
    "\n",
    "The proposed network can be referred to as an efficient sub-pixel convolutional neural network, also referred to as *ESPCN*. \n",
    "\n",
    "A pixel-wise mean squared error function is used to train the network:\n",
    "$\\ell(W_{1:L}, b_{1:L}) = \\frac{1}{r^2HW}\\sum^{rH}_{x=1}\\sum^{rW}_{x=1} (\\boldsymbol{I}^{HR}_{x,y} - f^L_{x,y}(\\boldsymbol{I}^{LR}))^2$.  \n",
    "  \n",
    "To increase the performance while learning the periodic shuffling at the end can be avoided during training since the labelled training data can be inversely shuffled to match the output of the last layer before it is shuffled, therefore avoiding unnecessary calculations.\n",
    "\n",
    "---\n",
    "\n",
    "## Experiment Setup \n",
    "\n",
    "Reproducing the results consists of two main steps, namely:  \n",
    "1. generating the training data\n",
    "2. training the network on said data  \n",
    "\n",
    "Both of these steps are discussed below individually.\n",
    "\n",
    "### 1. Generating the training data\n",
    "Since the T91 data set only consists of 91 images, the images need to be transformed in various ways to generate a sufficient amount of training data. Three steps are applied to the T91 data set in order to generate the training data.\n",
    "1. A gaussian blur (sigma \\= \\[$0.1$ - $1$\\]) is applied to the original images to simulate optical blur of a low-resolution camera and the images are downscaled by a factor $3$.\n",
    "2. $17$ by $17$ training images are subsampled from the downscaled images, corresponding to $17r$ by $17r$ pixel subsamples of the original HR images\n",
    "3. An inverse periodic shuffle is applied on the $17r$ by $17r$ pixel subsamples of original HR images to obtain training data labels\n",
    "\n",
    "![figure 2](rsc/ss_hr_to_lr.png)\n",
    "\n",
    "### 2. Training the network\n",
    "Once the training data is generated we can begin training. The model is trained to predict the inverse shuffled HR subsamples from the corresponding blurred LR subsamples. To reproduce the original paper as accurately as possible, the exact implementation details from their paper were implemented. However, some of the information necessary for reproducing these results was missing. This missing information and our approach are detailed further in *Discussion*.  \n",
    "\n",
    "### Results\n",
    "From the losses shown in the figure below we can conclude that our network is able to learn on the generated data.\n",
    "\n",
    "![figure 3](rsc/train_and_test_loss.png)\n",
    "\n",
    "After training for approximately 3000 epochs, we achieved the results in the *Our ESPCN* column:\n",
    "\n",
    "| Data Set | Scale | Bi-cubic | SRCNN (91)| Paper ESPCN | Our ESPCN |\n",
    "|----------|-------|----------|-----------|-------------|-----------|\n",
    "|Set5      | 3     | 27.53    | 32.39     | 32.55       | 32.25     |\n",
    "|Set14     | 3     | 25.25    | 29.00     | 29.08       | 28.50     |\n",
    "|BSD300    | 3     | 25.57    | 28.21     | 28.26       | 28.02     |\n",
    "|BSD500    | 3     | 25.54    | 28.28     | 28.34       | 28.08     |\n",
    "\n",
    "The results are similar to the *Paper ESPCN*, but since our model is not an improvement over *SRCNN* we do not feel that we were able to reproduce the results from the paper. The reason why we were unable to reproduce the results and our approach to optimizing our own results are discussed in *Discussion*.\n",
    "\n",
    "---\n",
    "## Discussion\n",
    "We did not manage to reproduce the results from Table 1 from the [original paper](https://arxiv.org/pdf/1609.05158v2.pdf), column ESPCN (91). When trying to recreate the results of the paper, we recreated the model as described in the paper. However, some hyperparameters and architectural decisions were not given. The following information was missing from the paper and made it harder to reproduce the results:\n",
    "* **Optimizer**: It is not stated what optimizer is used when training the network.\n",
    "* **How the learning rate changes from high to low**: It is not stated how the learning rate changes between its start- and end-value. A parameter ‘mu’ is mentioned, where the learning rate decreases with an unknown amount when the improvement is smaller than ‘mu’. However, the value of ‘mu’ is not given.\n",
    "* **Gaussian blur**: To simulate the downscale image when training in a realistic manner, the training images are blurred. However, the intensity of this blur is not given.\n",
    "\n",
    "In order to find optimal values for these parameters, we export the values of these parameters and their respective results to a *.csv* file after each training session. This allowed us to easily run the training multiple times with different parameter sets and compare their respective results. Though we were able to optimize our network by finding better parameters, we were still unable to reproduce the results from the paper.\n",
    "\n",
    "When implementing and training *ESPCN* we encountered a few other parameters that were not necessarily missing from the paper, but would have been beneficial to have more information on, namely:\n",
    "\n",
    "* **Batch size**: The batch size was not given, nor was it clear from the paper whether batches were used at all. This information would have been useful to interpret their training times and compare them to ours.\n",
    "* **Model validation**: The T91 dataset was used for training. However, it was not specified whether this dataset was split up into a train, test and validation set. Was (k-fold) cross-validation used, was some other validation method used or did they train on the full dataset without validation?\n",
    "\n",
    "### Chosen parameters values\n",
    "After experimenting with multiple runs these were the observations we made w.r.t. the impact and optimal values of the missing (hyper)parameters:\n",
    "\n",
    "__Optimizer__: Adam is used instead of SGD as proposed in the paper. For SGD the results obtained with the high-level description in the paper were far below the results in the paper - SGD needs to be fine-tuned so it is important the paper should have mentioned the specific parameters and method. The adam optimizer works in a similar way but the only parameter to be set is the final learning rate which is set to $1e-4$ as in the paper. \n",
    "\n",
    "__Gaussian blur sigma__: values between $0$ and $2$ were tried for the gaussian sigma, with values between $0.25$ and $1$ showing similar performance, a sigma of $1$ achieved the highest performance which is used for our final reproduction model.\n",
    "\n",
    "A __no-learning threshold__ of $1e-7$ is chosen meaning the training stops after $100$ epochs where the max improvement per epoch is below this threshold. Choosing a higher threshold causes the training process to end with suboptimal results and experiments run with a smaller threshold did not finish within a day.\n",
    "\n",
    "A __batch size__ of $10$ was chosen to speed up training. Using lower batch sizes (below $4$) considerably increased the training time and much higher batch sizes (e.g. $50$, $100$) resulted in a higher loss while training.\n",
    "\n",
    "A __train/test split__ is not mentioned by the paper, we tried splits $70$-$30$, $80$-$20$, $90$-$10$ with the $90$-$10$ split giving the best PSNR.\n",
    "\n",
    "---\n",
    "# The model\n",
    "Below the complete code for creating the network and loading the data is discussed. The concepts used are\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xw7N_3dGGdtk"
   },
   "source": [
    "First, all dependencies are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuwZj7xuGAix"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.filters import *\n",
    "from skimage.transform import *\n",
    "import os\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YE5tg_8GkFF"
   },
   "source": [
    "### Hyper parameters\n",
    "Then all hyperparameters and parameters, as well as constants are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmxbwLXHGMwY"
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "C = 3 # amount of colour channels\n",
    "dataset = \"T91\" # the folder containing all images used for training\n",
    "epoch_save_interval = 100 # the interval after which an intermediate save is made\n",
    "logging_interval = 100 # the interval after which we log the losses the the console\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# HYPERPARAMETERS and PARAMETERS\n",
    "r = 3  # upscaling ratio\n",
    "blur = 1  # gaussian blur\n",
    "adam_learning_rate = 1e-4 #adam learning rate, equal to paper's final learning rate\n",
    "no_learning_threshold = 1e-8  # threshold for stopping training when no improvement has been made for 'repeats' epochs\n",
    "repeats = 100 # the number of consecutive epochs without (enough) improvement necessary to stop training \n",
    "batch_size = 4 # size of batches\n",
    "train_test_fraction = 0.9 # the fraction of the data that is used for training, the remainder is used as a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6GNLWCmGnz9"
   },
   "source": [
    "### Periodic Shuffle\n",
    "In the code below the Periodic Shuffle and its inverse are defined as functions to be used when loading in the data.\n",
    "As mentioned before, the Periodic Shuffle is defined as follows:\n",
    "$\\mathcal{PS} (T)_{x,y,c} = T_{\\lfloor x/r \\rfloor, \\lfloor y/r \\rfloor , C\\cdot r \\cdot mod(y,r) + C\\cdot mod(x,r) + c}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PS(T, r):\n",
    "    T = np.transpose(T, (1, 2, 0))\n",
    "    rW = r * len(T)\n",
    "    rH = r * len(T[0])\n",
    "    C = len(T[0][0]) / (r * r)\n",
    "\n",
    "    # make sure C is an integer and cast if this is the case\n",
    "    assert (C == int(C))\n",
    "    C = int(C)\n",
    "\n",
    "    res = np.zeros((rW, rH, C))\n",
    "\n",
    "    for x in range(len(res)):\n",
    "        for y in range(len(res[x])):\n",
    "            for c in range(len(res[x][y])):\n",
    "                res[x][y][c] = \\\n",
    "                    T[x // r][y // r][C * r * (y % r) + C * (x % r) + c]\n",
    "    return res\n",
    "\n",
    "\n",
    "def PS_inv(img, r):\n",
    "    r2 = r * r\n",
    "    W = len(img) / r\n",
    "    H = len(img[0]) / r\n",
    "    C = len(img[0][0])\n",
    "    Cr2 = C * r2\n",
    "\n",
    "    # Make sure H and W are integers\n",
    "    assert (int(H) == H and int(W) == W)\n",
    "    H, W = int(H), int(W)\n",
    "\n",
    "    res = np.zeros((W, H, Cr2))\n",
    "\n",
    "    for x in range(len(img)):\n",
    "        for y in range(len(img[x])):\n",
    "            for c in range(len(img[x][y])):\n",
    "                res[x // r][y // r][C * r * (y % r) + C * (x % r) + c] = img[x][y][c]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "The images from T91 are loaded in, then they are used to create a train and test set. This process is described under *Experiment Setup*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKgQGMS9GM8r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Downsample images\n",
    "- gaussian blur\n",
    "- resize by downsample factor (using interpolation)\n",
    "How To Use:\n",
    "    function lr_dataset_from_path takes a path to the dataset of HR image png files and returns an ndarray to use for training the model\n",
    "For debugging/showing examples:\n",
    "    (see bottom of file)\n",
    "    save_png set to True to save resulting lr images in specified directory.\n",
    "    !check the param_ varaiables\n",
    "\"\"\"\n",
    "\n",
    "SUBSAMPLING_STRIDE_SIZE = 14\n",
    "SUBSAMPLING_SAMPLE_SIZE = 17\n",
    "\n",
    "\n",
    "# hr_dataset_path: dir to the hr_dataset png files\n",
    "# downscale: downscale factor, e.g. if original image 64*64 and downscale=2 then result will be 32*32\n",
    "# returns list of numpy.ndarray representing the lr_images\n",
    "def lr_dataset_from_path(hr_dataset_path, downscale):\n",
    "    original_filenames = os.listdir(hr_dataset_path)\n",
    "    original_images = []\n",
    "    for file in original_filenames:\n",
    "        original_images.append(plt.imread(hr_dataset_path + '/' + file))\n",
    "    return lr_images(original_images, downscale)  # ndarray of images\n",
    "\n",
    "\n",
    "def torchDataloader_from_path(hr_dataset_path, downscale, gaussian_sigma, batch_size):\n",
    "    original_filenames = os.listdir(hr_dataset_path)\n",
    "    original_images = []\n",
    "    for file in original_filenames:\n",
    "        original_images.append(plt.imread(hr_dataset_path + '/' + file))\n",
    "\n",
    "    # subsample\n",
    "    subsamples_hr = []\n",
    "    subsamples_hr_rev_shuff = []\n",
    "    for i in range(len(original_images)):\n",
    "        temp_subsamples = subsample(original_images[i], downscale)\n",
    "        subsamples_hr += temp_subsamples\n",
    "        for sample_indx in range(len(temp_subsamples)):\n",
    "            subsamples_hr_rev_shuff.append(PS_inv(temp_subsamples[sample_indx], downscale))  # labels\n",
    "    lr_dataset = lr_images(subsamples_hr, downscale, gaussian_sigma)  # ndarray of images\n",
    "    return toDataloader(lr_dataset, subsamples_hr_rev_shuff, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Takes list of images and provide LR images in form of numpy array\n",
    "def lr_images(images_real, downscale, gaussianSigma):\n",
    "    lr_images = []\n",
    "    for img in range(len(images_real)):\n",
    "        img_blurred = gaussian(images_real[img], sigma=gaussianSigma,\n",
    "                               multichannel=True)  # multichannel blurr so that 3rd channel is not blurred\n",
    "        lr_images.append(resize(img_blurred, (img_blurred.shape[0] // downscale, img_blurred.shape[1] // downscale)))\n",
    "    return lr_images\n",
    "\n",
    "\n",
    "# extract a 17r*17r subsample from original image, no overlap so every pixel appears at most once in output\n",
    "def subsample(image_real, downscale):\n",
    "    subsample_size = SUBSAMPLING_SAMPLE_SIZE * downscale\n",
    "    subsample_stride = SUBSAMPLING_STRIDE_SIZE * downscale\n",
    "    subsamples = []\n",
    "    for y in range(math.floor((image_real.shape[0] - (subsample_size - subsample_stride)) / subsample_stride)):\n",
    "        for x in range(math.floor((image_real.shape[1] - (subsample_size - subsample_stride)) / subsample_stride)):\n",
    "            ss = image_real[(y * subsample_stride):(y * subsample_stride) + subsample_size,\n",
    "                 (x * subsample_stride):(x * subsample_stride) + subsample_size]\n",
    "            subsamples.append(ss)\n",
    "\n",
    "    return subsamples\n",
    "\n",
    "\n",
    "# returns a torch Dataloader (to iterate over training data) using the training data samples and traing data labels\n",
    "def toDataloader(train_data, train_labels, batch_size):\n",
    "    labeled_data = []\n",
    "    for i in range(len(train_data)):\n",
    "        labeled_data.append([np.transpose(train_data[i], (2, 0, 1)), np.transpose(train_labels[i], (2, 0, 1))])\n",
    "    trainDataloader = DataLoader(labeled_data, batch_size=batch_size, shuffle=True)\n",
    "    return trainDataloader\n",
    "\n",
    "# Load the data\n",
    "dataloader = torchDataloader_from_path('./datasets/' + dataset, r, blur, batch_size)\n",
    "train_size = int(train_test_fraction * len(dataloader.dataset))\n",
    "test_size = len(dataloader.dataset) - train_size\n",
    "train_set, test_set = torch.utils.data.random_split(dataloader.dataset, [train_size, test_size])\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "print(\"Data loaded\")\n",
    "\n",
    "# show first image from the dat\n",
    "plt.imshow(np.transpose(dataloader.dataset[0][0], (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J017G3b9Grk6"
   },
   "source": [
    "### The Network\n",
    "A CNN is defined with 3 layers. The first layer takes in the coloured image (C = 3). This layer has 64 feature maps with a kernel size of 5. The second layer has 32 feature maps with a kernel size of 3. The last layer has C($r^2$) feature maps with a kernel size of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-dbqVPjGNCd"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, r, C):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(C, 64, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, r * r * C, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQYuI340GvRZ"
   },
   "source": [
    "### The Training\n",
    "In the following code, the network is trained. During this training the losses, some models and the best model are saved. On intervals, the current progress is logged also to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_UAB7-mzGM_0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training at: 2020-04-15 13:36:51.793167\n",
      "[1,   100] train_loss: 0.09192\n",
      "[1,   200] train_loss: 0.03460\n",
      "[1,   300] train_loss: 0.02375\n",
      "[1,   400] train_loss: 0.01626\n",
      "[1,   500] train_loss: 0.01319\n",
      "1 0.03391371858653152\n",
      "1 0.011432795600525587\n",
      "epoch 1: improvement = inf\n",
      "[2,   100] train_loss: 0.01062\n",
      "[2,   200] train_loss: 0.00940\n",
      "[2,   300] train_loss: 0.00884\n",
      "[2,   400] train_loss: 0.00827\n",
      "[2,   500] train_loss: 0.00733\n",
      "2 0.008744237745936764\n",
      "2 0.007362502180989752\n",
      "epoch 2: improvement = 0.004070293419535835\n",
      "[3,   100] train_loss: 0.00717\n",
      "[3,   200] train_loss: 0.00684\n",
      "[3,   300] train_loss: 0.00644\n",
      "[3,   400] train_loss: 0.00639\n",
      "[3,   500] train_loss: 0.00648\n",
      "3 0.00659303392857825\n",
      "3 0.00605898152169883\n",
      "epoch 3: improvement = 0.001303520659290922\n",
      "[4,   100] train_loss: 0.00561\n",
      "[4,   200] train_loss: 0.00576\n",
      "[4,   300] train_loss: 0.00558\n",
      "[4,   400] train_loss: 0.00578\n",
      "[4,   500] train_loss: 0.00518\n",
      "4 0.005560017243378403\n",
      "4 0.005175066792493966\n",
      "epoch 4: improvement = 0.0008839147292048636\n",
      "[5,   100] train_loss: 0.00498\n",
      "[5,   200] train_loss: 0.00523\n",
      "[5,   300] train_loss: 0.00492\n",
      "[5,   400] train_loss: 0.00439\n",
      "[5,   500] train_loss: 0.00467\n",
      "5 0.004817177057984549\n",
      "5 0.004483009693335125\n",
      "epoch 5: improvement = 0.0006920570991588411\n",
      "[6,   100] train_loss: 0.00449\n",
      "[6,   200] train_loss: 0.00455\n",
      "[6,   300] train_loss: 0.00412\n",
      "[6,   400] train_loss: 0.00430\n",
      "[6,   500] train_loss: 0.00391\n",
      "6 0.00421879652758956\n",
      "6 0.0039456630856513865\n",
      "epoch 6: improvement = 0.0005373466076837386\n",
      "[7,   100] train_loss: 0.00376\n",
      "[7,   200] train_loss: 0.00410\n",
      "[7,   300] train_loss: 0.00371\n",
      "[7,   400] train_loss: 0.00339\n",
      "[7,   500] train_loss: 0.00379\n",
      "7 0.0037428646204801324\n",
      "7 0.0035322534554019973\n",
      "epoch 7: improvement = 0.00041340963024938913\n",
      "[8,   100] train_loss: 0.00354\n",
      "[8,   200] train_loss: 0.00367\n",
      "[8,   300] train_loss: 0.00316\n",
      "[8,   400] train_loss: 0.00329\n",
      "[8,   500] train_loss: 0.00340\n",
      "8 0.0034080342160146327\n",
      "8 0.0032879055682482287\n",
      "epoch 8: improvement = 0.00024434788715376864\n",
      "[9,   100] train_loss: 0.00319\n",
      "[9,   200] train_loss: 0.00314\n",
      "[9,   300] train_loss: 0.00347\n",
      "[9,   400] train_loss: 0.00315\n",
      "[9,   500] train_loss: 0.00285\n",
      "9 0.003173900245494643\n",
      "9 0.0030492975883610876\n",
      "epoch 9: improvement = 0.0002386079798871411\n",
      "[10,   100] train_loss: 0.00309\n",
      "[10,   200] train_loss: 0.00283\n",
      "[10,   300] train_loss: 0.00318\n",
      "[10,   400] train_loss: 0.00319\n",
      "[10,   500] train_loss: 0.00299\n",
      "10 0.003025826165329991\n",
      "10 0.0029359569182035887\n",
      "epoch 10: improvement = 0.00011334067015749891\n",
      "[11,   100] train_loss: 0.00285\n",
      "[11,   200] train_loss: 0.00285\n",
      "[11,   300] train_loss: 0.00302\n",
      "[11,   400] train_loss: 0.00288\n",
      "[11,   500] train_loss: 0.00287\n",
      "11 0.0028720324749289\n",
      "11 0.002906398379788465\n",
      "epoch 11: improvement = 2.9558538415123525e-05\n",
      "[12,   100] train_loss: 0.00285\n",
      "[12,   200] train_loss: 0.00273\n",
      "[12,   300] train_loss: 0.00261\n",
      "[12,   400] train_loss: 0.00294\n",
      "[12,   500] train_loss: 0.00270\n",
      "12 0.00278260115606717\n",
      "12 0.002694841052732827\n",
      "epoch 12: improvement = 0.00021155732705563802\n",
      "[13,   100] train_loss: 0.00270\n",
      "[13,   200] train_loss: 0.00270\n",
      "[13,   300] train_loss: 0.00269\n",
      "[13,   400] train_loss: 0.00281\n",
      "[13,   500] train_loss: 0.00268\n",
      "13 0.0027135539590898107\n",
      "13 0.00264210577765314\n",
      "epoch 13: improvement = 5.273527507968726e-05\n",
      "[14,   100] train_loss: 0.00256\n",
      "[14,   200] train_loss: 0.00266\n",
      "[14,   300] train_loss: 0.00274\n",
      "[14,   400] train_loss: 0.00267\n",
      "[14,   500] train_loss: 0.00264\n",
      "14 0.002657141575373522\n",
      "14 0.0026142470387223566\n",
      "epoch 14: improvement = 2.7858738930783278e-05\n",
      "[15,   100] train_loss: 0.00272\n",
      "[15,   200] train_loss: 0.00261\n",
      "[15,   300] train_loss: 0.00259\n",
      "[15,   400] train_loss: 0.00268\n",
      "[15,   500] train_loss: 0.00265\n",
      "15 0.0026531978439424207\n",
      "15 0.0025298364412282884\n",
      "epoch 15: improvement = 8.441059749406815e-05\n",
      "[16,   100] train_loss: 0.00251\n",
      "[16,   200] train_loss: 0.00235\n",
      "[16,   300] train_loss: 0.00257\n",
      "[16,   400] train_loss: 0.00266\n",
      "[16,   500] train_loss: 0.00262\n",
      "16 0.0025590473774173244\n",
      "16 0.0025179279655480956\n",
      "epoch 16: improvement = 1.1908475680192862e-05\n",
      "[17,   100] train_loss: 0.00255\n",
      "[17,   200] train_loss: 0.00253\n",
      "[17,   300] train_loss: 0.00238\n",
      "[17,   400] train_loss: 0.00272\n",
      "[17,   500] train_loss: 0.00253\n",
      "17 0.0025333040816261708\n",
      "17 0.002517141593811878\n",
      "epoch 17: improvement = 7.863717362177787e-07\n",
      "[18,   100] train_loss: 0.00255\n",
      "[18,   200] train_loss: 0.00251\n",
      "[18,   300] train_loss: 0.00241\n",
      "[18,   400] train_loss: 0.00265\n",
      "[18,   500] train_loss: 0.00238\n",
      "18 0.002502390933712543\n",
      "18 0.0024653858594418554\n",
      "epoch 18: improvement = 5.1755734370022375e-05\n",
      "[19,   100] train_loss: 0.00256\n",
      "[19,   200] train_loss: 0.00237\n",
      "[19,   300] train_loss: 0.00265\n",
      "[19,   400] train_loss: 0.00254\n",
      "[19,   500] train_loss: 0.00238\n",
      "19 0.0025032672297334493\n",
      "19 0.002663897713720241\n",
      "epoch 19: improvement = -0.00019851185427838562\n",
      "[20,   100] train_loss: 0.00236\n",
      "[20,   200] train_loss: 0.00251\n",
      "[20,   300] train_loss: 0.00255\n",
      "[20,   400] train_loss: 0.00250\n",
      "[20,   500] train_loss: 0.00245\n",
      "20 0.0024778151924562497\n",
      "20 0.0024269960980003633\n",
      "epoch 20: improvement = 3.838976144149211e-05\n",
      "[21,   100] train_loss: 0.00240\n",
      "[21,   200] train_loss: 0.00244\n",
      "[21,   300] train_loss: 0.00251\n",
      "[21,   400] train_loss: 0.00241\n",
      "[21,   500] train_loss: 0.00256\n",
      "21 0.0024432538565877077\n",
      "21 0.002383958377318971\n",
      "epoch 21: improvement = 4.303772068139228e-05\n",
      "[22,   100] train_loss: 0.00235\n",
      "[22,   200] train_loss: 0.00243\n",
      "[22,   300] train_loss: 0.00238\n",
      "[22,   400] train_loss: 0.00259\n",
      "[22,   500] train_loss: 0.00244\n",
      "22 0.0024596859119484582\n",
      "22 0.002373545029478955\n",
      "epoch 22: improvement = 1.041334784001582e-05\n",
      "[23,   100] train_loss: 0.00234\n",
      "[23,   200] train_loss: 0.00226\n",
      "[23,   300] train_loss: 0.00250\n",
      "[23,   400] train_loss: 0.00256\n",
      "[23,   500] train_loss: 0.00234\n",
      "23 0.0024103878819582377\n",
      "23 0.0023657122617602687\n",
      "epoch 23: improvement = 7.832767718686517e-06\n",
      "[24,   100] train_loss: 0.00238\n",
      "[24,   200] train_loss: 0.00240\n",
      "[24,   300] train_loss: 0.00249\n",
      "[24,   400] train_loss: 0.00227\n",
      "[24,   500] train_loss: 0.00241\n",
      "24 0.0023929551611452174\n",
      "24 0.002359470650984342\n",
      "epoch 24: improvement = 6.241610775926509e-06\n",
      "[25,   100] train_loss: 0.00241\n",
      "[25,   200] train_loss: 0.00236\n",
      "[25,   300] train_loss: 0.00233\n",
      "[25,   400] train_loss: 0.00249\n",
      "[25,   500] train_loss: 0.00244\n",
      "25 0.002396710369800107\n",
      "25 0.00237854317989516\n",
      "epoch 25: improvement = -1.9072528910817673e-05\n",
      "[26,   100] train_loss: 0.00220\n",
      "[26,   200] train_loss: 0.00242\n",
      "[26,   300] train_loss: 0.00247\n",
      "[26,   400] train_loss: 0.00243\n",
      "[26,   500] train_loss: 0.00236\n",
      "26 0.0023841330264528933\n",
      "26 0.002339299058816261\n",
      "epoch 26: improvement = 2.0171592168081315e-05\n",
      "[27,   100] train_loss: 0.00235\n",
      "[27,   200] train_loss: 0.00224\n",
      "[27,   300] train_loss: 0.00239\n",
      "[27,   400] train_loss: 0.00235\n",
      "[27,   500] train_loss: 0.00250\n",
      "27 0.0023744821534758304\n",
      "27 0.0023210224397418834\n",
      "epoch 27: improvement = 1.8276619074377495e-05\n",
      "[28,   100] train_loss: 0.00217\n",
      "[28,   200] train_loss: 0.00216\n",
      "[28,   300] train_loss: 0.00233\n",
      "[28,   400] train_loss: 0.00266\n",
      "[28,   500] train_loss: 0.00251\n",
      "28 0.0023541352165082174\n",
      "28 0.0023054645999436947\n",
      "epoch 28: improvement = 1.55578397981887e-05\n",
      "[29,   100] train_loss: 0.00231\n",
      "[29,   200] train_loss: 0.00270\n",
      "[29,   300] train_loss: 0.00216\n",
      "[29,   400] train_loss: 0.00246\n",
      "[29,   500] train_loss: 0.00227\n",
      "29 0.002363036482504927\n",
      "29 0.002312184995121461\n",
      "epoch 29: improvement = -6.720395177766368e-06\n",
      "[30,   100] train_loss: 0.00208\n",
      "[30,   200] train_loss: 0.00232\n",
      "[30,   300] train_loss: 0.00236\n",
      "[30,   400] train_loss: 0.00244\n",
      "[30,   500] train_loss: 0.00242\n",
      "30 0.0023412284018843894\n",
      "30 0.0022993055604486985\n",
      "epoch 30: improvement = 6.159039494996188e-06\n",
      "[31,   100] train_loss: 0.00219\n",
      "[31,   200] train_loss: 0.00230\n",
      "[31,   300] train_loss: 0.00225\n",
      "[31,   400] train_loss: 0.00214\n",
      "[31,   500] train_loss: 0.00263\n",
      "31 0.002328076168874355\n",
      "31 0.002347097078556568\n",
      "epoch 31: improvement = -4.7791518107869694e-05\n",
      "[32,   100] train_loss: 0.00222\n",
      "[32,   200] train_loss: 0.00238\n",
      "[32,   300] train_loss: 0.00240\n",
      "[32,   400] train_loss: 0.00256\n",
      "[32,   500] train_loss: 0.00220\n",
      "32 0.0023428009221115793\n",
      "32 0.0022749737921571303\n",
      "epoch 32: improvement = 2.4331768291568218e-05\n",
      "[33,   100] train_loss: 0.00204\n",
      "[33,   200] train_loss: 0.00233\n",
      "[33,   300] train_loss: 0.00247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33,   400] train_loss: 0.00241\n",
      "[33,   500] train_loss: 0.00229\n",
      "33 0.0023176383633686935\n",
      "33 0.002276033883832119\n",
      "epoch 33: improvement = -1.0600916749885572e-06\n",
      "[34,   100] train_loss: 0.00208\n",
      "[34,   200] train_loss: 0.00231\n",
      "[34,   300] train_loss: 0.00254\n",
      "[34,   400] train_loss: 0.00249\n",
      "[34,   500] train_loss: 0.00222\n",
      "34 0.002318673580663145\n",
      "34 0.0024868943893680303\n",
      "epoch 34: improvement = -0.00021192059721090008\n",
      "[35,   100] train_loss: 0.00242\n",
      "[35,   200] train_loss: 0.00223\n",
      "[35,   300] train_loss: 0.00246\n",
      "[35,   400] train_loss: 0.00238\n",
      "[35,   500] train_loss: 0.00223\n",
      "35 0.0023159554035573683\n",
      "35 0.002259167353615213\n",
      "epoch 35: improvement = 1.5806438541917343e-05\n",
      "[36,   100] train_loss: 0.00219\n",
      "[36,   200] train_loss: 0.00226\n",
      "[36,   300] train_loss: 0.00220\n",
      "[36,   400] train_loss: 0.00267\n",
      "[36,   500] train_loss: 0.00246\n",
      "36 0.0023226660124624106\n",
      "36 0.0022690879610556794\n",
      "epoch 36: improvement = -9.920607440466484e-06\n",
      "[37,   100] train_loss: 0.00227\n",
      "[37,   200] train_loss: 0.00224\n",
      "[37,   300] train_loss: 0.00207\n",
      "[37,   400] train_loss: 0.00221\n",
      "[37,   500] train_loss: 0.00258\n",
      "37 0.0022892967764844415\n",
      "37 0.0022536195569580275\n",
      "epoch 37: improvement = 5.547796657185376e-06\n",
      "[38,   100] train_loss: 0.00231\n",
      "[38,   200] train_loss: 0.00239\n",
      "[38,   300] train_loss: 0.00244\n",
      "[38,   400] train_loss: 0.00219\n",
      "[38,   500] train_loss: 0.00213\n",
      "38 0.0022919192307746866\n",
      "38 0.0022364019594935977\n",
      "epoch 38: improvement = 1.7217597464429853e-05\n",
      "[39,   100] train_loss: 0.00211\n",
      "[39,   200] train_loss: 0.00245\n",
      "[39,   300] train_loss: 0.00227\n",
      "[39,   400] train_loss: 0.00228\n",
      "[39,   500] train_loss: 0.00229\n",
      "39 0.0022858052098110623\n",
      "39 0.002244981215109082\n",
      "epoch 39: improvement = -8.579255615484387e-06\n",
      "[40,   100] train_loss: 0.00214\n",
      "[40,   200] train_loss: 0.00234\n",
      "[40,   300] train_loss: 0.00205\n",
      "[40,   400] train_loss: 0.00248\n",
      "[40,   500] train_loss: 0.00220\n",
      "40 0.0022733395488644067\n",
      "40 0.00223224606430752\n",
      "epoch 40: improvement = 4.155895186077653e-06\n",
      "[41,   100] train_loss: 0.00237\n",
      "[41,   200] train_loss: 0.00251\n",
      "[41,   300] train_loss: 0.00222\n",
      "[41,   400] train_loss: 0.00224\n",
      "[41,   500] train_loss: 0.00223\n",
      "41 0.002289078324514779\n",
      "41 0.0022221712146466943\n",
      "epoch 41: improvement = 1.0074849660825745e-05\n",
      "[42,   100] train_loss: 0.00242\n",
      "[42,   200] train_loss: 0.00219\n",
      "[42,   300] train_loss: 0.00228\n",
      "[42,   400] train_loss: 0.00214\n",
      "[42,   500] train_loss: 0.00223\n",
      "42 0.002269844467681125\n",
      "42 0.0023259004357894567\n",
      "epoch 42: improvement = -0.00010372922114276241\n",
      "[43,   100] train_loss: 0.00212\n",
      "[43,   200] train_loss: 0.00240\n",
      "[43,   300] train_loss: 0.00243\n",
      "[43,   400] train_loss: 0.00226\n",
      "[43,   500] train_loss: 0.00219\n",
      "43 0.002278584247712906\n",
      "43 0.0022330929389159744\n",
      "epoch 43: improvement = -1.0921724269280075e-05\n",
      "[44,   100] train_loss: 0.00225\n",
      "[44,   200] train_loss: 0.00223\n",
      "[44,   300] train_loss: 0.00224\n",
      "[44,   400] train_loss: 0.00233\n",
      "[44,   500] train_loss: 0.00232\n",
      "44 0.0022597391671399943\n",
      "44 0.0022326343077413205\n",
      "epoch 44: improvement = -1.0463093094626244e-05\n",
      "[45,   100] train_loss: 0.00216\n",
      "[45,   200] train_loss: 0.00226\n",
      "[45,   300] train_loss: 0.00234\n",
      "[45,   400] train_loss: 0.00236\n",
      "[45,   500] train_loss: 0.00209\n",
      "45 0.0022613038217571203\n",
      "45 0.0022980045859082743\n",
      "epoch 45: improvement = -7.583337126158005e-05\n",
      "[46,   100] train_loss: 0.00214\n",
      "[46,   200] train_loss: 0.00234\n",
      "[46,   300] train_loss: 0.00234\n",
      "[46,   400] train_loss: 0.00225\n",
      "[46,   500] train_loss: 0.00223\n",
      "46 0.002260087354914199\n",
      "46 0.002212656422886918\n",
      "epoch 46: improvement = 9.514791759776195e-06\n",
      "[47,   100] train_loss: 0.00220\n",
      "[47,   200] train_loss: 0.00223\n",
      "[47,   300] train_loss: 0.00227\n",
      "[47,   400] train_loss: 0.00237\n",
      "[47,   500] train_loss: 0.00212\n",
      "47 0.002264030860162708\n",
      "47 0.0022151937367115063\n",
      "epoch 47: improvement = -2.5373138245882165e-06\n",
      "[48,   100] train_loss: 0.00201\n",
      "[48,   200] train_loss: 0.00221\n",
      "[48,   300] train_loss: 0.00247\n",
      "[48,   400] train_loss: 0.00232\n",
      "[48,   500] train_loss: 0.00226\n",
      "48 0.0022503446275519905\n",
      "48 0.0022075500053737765\n",
      "epoch 48: improvement = 5.106417513141628e-06\n",
      "[49,   100] train_loss: 0.00232\n",
      "[49,   200] train_loss: 0.00232\n",
      "[49,   300] train_loss: 0.00195\n",
      "[49,   400] train_loss: 0.00233\n",
      "[49,   500] train_loss: 0.00231\n",
      "49 0.0022504226756081607\n",
      "49 0.002221492661988533\n",
      "epoch 49: improvement = -1.3942656614756351e-05\n",
      "[50,   100] train_loss: 0.00247\n",
      "[50,   200] train_loss: 0.00228\n",
      "[50,   300] train_loss: 0.00204\n",
      "[50,   400] train_loss: 0.00235\n",
      "[50,   500] train_loss: 0.00207\n",
      "50 0.0022438442499456025\n",
      "50 0.0021957646275814866\n",
      "epoch 50: improvement = 1.1785377792289907e-05\n",
      "[51,   100] train_loss: 0.00221\n",
      "[51,   200] train_loss: 0.00221\n",
      "[51,   300] train_loss: 0.00228\n",
      "[51,   400] train_loss: 0.00233\n",
      "[51,   500] train_loss: 0.00230\n",
      "51 0.0022443882561030354\n",
      "51 0.0021941769753903895\n",
      "epoch 51: improvement = 1.5876521910970776e-06\n",
      "[52,   100] train_loss: 0.00239\n",
      "[52,   200] train_loss: 0.00224\n",
      "[52,   300] train_loss: 0.00204\n",
      "[52,   400] train_loss: 0.00229\n",
      "[52,   500] train_loss: 0.00241\n",
      "52 0.0022628566705090655\n",
      "52 0.0021940754259752652\n",
      "epoch 52: improvement = 1.015494151242538e-07\n",
      "[53,   100] train_loss: 0.00237\n",
      "[53,   200] train_loss: 0.00225\n",
      "[53,   300] train_loss: 0.00223\n",
      "[53,   400] train_loss: 0.00208\n",
      "[53,   500] train_loss: 0.00217\n",
      "53 0.0022286407737606797\n",
      "53 0.0022078720381979115\n",
      "epoch 53: improvement = -1.3796612222646224e-05\n",
      "[54,   100] train_loss: 0.00236\n",
      "[54,   200] train_loss: 0.00224\n",
      "[54,   300] train_loss: 0.00204\n",
      "[54,   400] train_loss: 0.00215\n",
      "[54,   500] train_loss: 0.00250\n",
      "54 0.0022413415556645174\n",
      "54 0.0021921504530364047\n",
      "epoch 54: improvement = 1.92497293886049e-06\n",
      "[55,   100] train_loss: 0.00235\n",
      "[55,   200] train_loss: 0.00210\n",
      "[55,   300] train_loss: 0.00206\n",
      "[55,   400] train_loss: 0.00227\n",
      "[55,   500] train_loss: 0.00222\n",
      "55 0.002232383678877061\n",
      "55 0.002307436515076782\n",
      "epoch 55: improvement = -0.00011528606204037721\n",
      "[56,   100] train_loss: 0.00230\n",
      "[56,   200] train_loss: 0.00220\n",
      "[56,   300] train_loss: 0.00229\n",
      "[56,   400] train_loss: 0.00211\n",
      "[56,   500] train_loss: 0.00213\n",
      "56 0.0022423563342250834\n",
      "56 0.0022072575007078613\n",
      "epoch 56: improvement = -1.5107047671456507e-05\n",
      "[57,   100] train_loss: 0.00219\n",
      "[57,   200] train_loss: 0.00233\n",
      "[57,   300] train_loss: 0.00222\n",
      "[57,   400] train_loss: 0.00206\n",
      "[57,   500] train_loss: 0.00239\n",
      "57 0.002223516476625588\n",
      "57 0.0022143815248203158\n",
      "epoch 57: improvement = -2.2231071783911007e-05\n",
      "[58,   100] train_loss: 0.00231\n",
      "[58,   200] train_loss: 0.00226\n",
      "[58,   300] train_loss: 0.00209\n",
      "[58,   400] train_loss: 0.00203\n",
      "[58,   500] train_loss: 0.00224\n",
      "58 0.002227375933731312\n",
      "58 0.0021795508607472865\n",
      "epoch 58: improvement = 1.2599592289118215e-05\n",
      "[59,   100] train_loss: 0.00246\n",
      "[59,   200] train_loss: 0.00223\n",
      "[59,   300] train_loss: 0.00218\n",
      "[59,   400] train_loss: 0.00208\n",
      "[59,   500] train_loss: 0.00218\n",
      "59 0.002233174583551627\n",
      "59 0.0022542294103298547\n",
      "epoch 59: improvement = -7.467854958256815e-05\n",
      "[60,   100] train_loss: 0.00214\n",
      "[60,   200] train_loss: 0.00225\n",
      "[60,   300] train_loss: 0.00225\n",
      "[60,   400] train_loss: 0.00219\n",
      "[60,   500] train_loss: 0.00226\n",
      "60 0.0022164920683091587\n",
      "60 0.0021832607827686323\n",
      "epoch 60: improvement = -3.709922021345735e-06\n",
      "[61,   100] train_loss: 0.00231\n",
      "[61,   200] train_loss: 0.00238\n",
      "[61,   300] train_loss: 0.00222\n",
      "[61,   400] train_loss: 0.00210\n",
      "[61,   500] train_loss: 0.00217\n",
      "61 0.0022167923336020167\n",
      "61 0.002194323153086378\n",
      "epoch 61: improvement = -1.4772292339091438e-05\n",
      "[62,   100] train_loss: 0.00242\n",
      "[62,   200] train_loss: 0.00225\n",
      "[62,   300] train_loss: 0.00211\n",
      "[62,   400] train_loss: 0.00212\n",
      "[62,   500] train_loss: 0.00213\n",
      "62 0.0022305768972219852\n",
      "62 0.0021867727200990885\n",
      "epoch 62: improvement = -7.2218593518019215e-06\n",
      "[63,   100] train_loss: 0.00204\n",
      "[63,   200] train_loss: 0.00208\n",
      "[63,   300] train_loss: 0.00234\n",
      "[63,   400] train_loss: 0.00209\n",
      "[63,   500] train_loss: 0.00235\n",
      "63 0.0022128993155073867\n",
      "63 0.002174071568891829\n",
      "epoch 63: improvement = 5.4792918554574105e-06\n",
      "[64,   100] train_loss: 0.00220\n",
      "[64,   200] train_loss: 0.00220\n",
      "[64,   300] train_loss: 0.00249\n",
      "[64,   400] train_loss: 0.00195\n",
      "[64,   500] train_loss: 0.00229\n",
      "64 0.0022120774283917823\n",
      "64 0.002182483400411195\n",
      "epoch 64: improvement = -8.411831519365785e-06\n",
      "[65,   100] train_loss: 0.00229\n",
      "[65,   200] train_loss: 0.00208\n",
      "[65,   300] train_loss: 0.00244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65,   400] train_loss: 0.00207\n",
      "[65,   500] train_loss: 0.00228\n",
      "65 0.0022192227322523\n",
      "65 0.0021725524137132593\n",
      "epoch 65: improvement = 1.5191551785698199e-06\n",
      "[66,   100] train_loss: 0.00237\n",
      "[66,   200] train_loss: 0.00207\n",
      "[66,   300] train_loss: 0.00224\n",
      "[66,   400] train_loss: 0.00235\n",
      "[66,   500] train_loss: 0.00207\n",
      "66 0.002208625292897779\n",
      "66 0.002174793440410921\n",
      "epoch 66: improvement = -2.241026697661491e-06\n",
      "[67,   100] train_loss: 0.00231\n",
      "[67,   200] train_loss: 0.00224\n",
      "[67,   300] train_loss: 0.00216\n",
      "[67,   400] train_loss: 0.00220\n",
      "[67,   500] train_loss: 0.00209\n",
      "67 0.002208281660703572\n",
      "67 0.0021654421580185276\n",
      "epoch 67: improvement = 7.110255694731723e-06\n",
      "[68,   100] train_loss: 0.00215\n",
      "[68,   200] train_loss: 0.00215\n",
      "[68,   300] train_loss: 0.00246\n",
      "[68,   400] train_loss: 0.00224\n",
      "[68,   500] train_loss: 0.00241\n",
      "68 0.002257335824768462\n",
      "68 0.002187301787066558\n",
      "epoch 68: improvement = -2.185962904803062e-05\n",
      "[69,   100] train_loss: 0.00217\n",
      "[69,   200] train_loss: 0.00226\n",
      "[69,   300] train_loss: 0.00228\n",
      "[69,   400] train_loss: 0.00231\n",
      "[69,   500] train_loss: 0.00200\n",
      "69 0.0021970927669870202\n",
      "69 0.0021666650711705568\n",
      "epoch 69: improvement = -1.2229131520291939e-06\n",
      "[70,   100] train_loss: 0.00223\n",
      "[70,   200] train_loss: 0.00230\n",
      "[70,   300] train_loss: 0.00208\n",
      "[70,   400] train_loss: 0.00217\n",
      "[70,   500] train_loss: 0.00224\n",
      "70 0.002205114955494936\n",
      "70 0.002162083101996891\n",
      "epoch 70: improvement = 3.359056021636636e-06\n",
      "[71,   100] train_loss: 0.00234\n",
      "[71,   200] train_loss: 0.00187\n",
      "[71,   300] train_loss: 0.00218\n",
      "[71,   400] train_loss: 0.00209\n",
      "[71,   500] train_loss: 0.00244\n",
      "71 0.0021967126687291033\n",
      "71 0.0021646258621162836\n",
      "epoch 71: improvement = -2.5427601193926848e-06\n",
      "[72,   100] train_loss: 0.00241\n",
      "[72,   200] train_loss: 0.00216\n",
      "[72,   300] train_loss: 0.00224\n",
      "[72,   400] train_loss: 0.00226\n",
      "[72,   500] train_loss: 0.00212\n",
      "72 0.0022015667137658835\n",
      "72 0.0021571569492440813\n",
      "epoch 72: improvement = 4.926152752809682e-06\n",
      "[73,   100] train_loss: 0.00212\n",
      "[73,   200] train_loss: 0.00203\n",
      "[73,   300] train_loss: 0.00224\n",
      "[73,   400] train_loss: 0.00250\n",
      "[73,   500] train_loss: 0.00226\n",
      "73 0.002207631648700854\n",
      "73 0.002210062161970539\n",
      "epoch 73: improvement = -5.290521272645777e-05\n",
      "[74,   100] train_loss: 0.00221\n",
      "[74,   200] train_loss: 0.00236\n",
      "[74,   300] train_loss: 0.00213\n",
      "[74,   400] train_loss: 0.00215\n",
      "[74,   500] train_loss: 0.00201\n",
      "74 0.0021964017353920187\n",
      "74 0.002157337354649218\n",
      "epoch 74: improvement = -1.8040540513677636e-07\n",
      "[75,   100] train_loss: 0.00201\n",
      "[75,   200] train_loss: 0.00222\n",
      "[75,   300] train_loss: 0.00233\n",
      "[75,   400] train_loss: 0.00210\n",
      "[75,   500] train_loss: 0.00228\n",
      "75 0.00219815277580614\n",
      "75 0.0021578168010771576\n",
      "epoch 75: improvement = -6.598518330763747e-07\n",
      "[76,   100] train_loss: 0.00235\n",
      "[76,   200] train_loss: 0.00208\n",
      "[76,   300] train_loss: 0.00215\n",
      "[76,   400] train_loss: 0.00225\n",
      "[76,   500] train_loss: 0.00219\n",
      "76 0.0021952249511876755\n",
      "76 0.0021769491570777246\n",
      "epoch 76: improvement = -1.9792207833643338e-05\n",
      "[77,   100] train_loss: 0.00211\n",
      "[77,   200] train_loss: 0.00217\n",
      "[77,   300] train_loss: 0.00231\n",
      "[77,   400] train_loss: 0.00207\n",
      "[77,   500] train_loss: 0.00233\n",
      "77 0.0022111058695853053\n",
      "77 0.0021543790191818927\n",
      "epoch 77: improvement = 2.777930062188537e-06\n",
      "[78,   100] train_loss: 0.00228\n",
      "[78,   200] train_loss: 0.00211\n",
      "[78,   300] train_loss: 0.00232\n",
      "[78,   400] train_loss: 0.00208\n",
      "[78,   500] train_loss: 0.00228\n",
      "78 0.002196909491465177\n",
      "78 0.002172343007255973\n",
      "epoch 78: improvement = -1.79639880740802e-05\n",
      "[79,   100] train_loss: 0.00220\n",
      "[79,   200] train_loss: 0.00210\n",
      "[79,   300] train_loss: 0.00225\n",
      "[79,   400] train_loss: 0.00232\n",
      "[79,   500] train_loss: 0.00223\n",
      "79 0.0021919451643619163\n",
      "79 0.0021521118354326653\n",
      "epoch 79: improvement = 2.2671837492274464e-06\n",
      "[80,   100] train_loss: 0.00203\n",
      "[80,   200] train_loss: 0.00246\n",
      "[80,   300] train_loss: 0.00229\n",
      "[80,   400] train_loss: 0.00191\n",
      "[80,   500] train_loss: 0.00236\n",
      "80 0.0021966778531178665\n",
      "80 0.002178011758625454\n",
      "epoch 80: improvement = -2.589992319278851e-05\n",
      "[81,   100] train_loss: 0.00236\n",
      "[81,   200] train_loss: 0.00213\n",
      "[81,   300] train_loss: 0.00213\n",
      "[81,   400] train_loss: 0.00222\n",
      "[81,   500] train_loss: 0.00213\n",
      "81 0.0022081821158736734\n",
      "81 0.002154878464270879\n",
      "epoch 81: improvement = -2.7666288382136722e-06\n",
      "[82,   100] train_loss: 0.00223\n",
      "[82,   200] train_loss: 0.00217\n",
      "[82,   300] train_loss: 0.00218\n",
      "[82,   400] train_loss: 0.00218\n",
      "[82,   500] train_loss: 0.00209\n",
      "82 0.002184309220756735\n",
      "82 0.002149504668436617\n",
      "epoch 82: improvement = 2.6071669960484566e-06\n",
      "[83,   100] train_loss: 0.00213\n",
      "[83,   200] train_loss: 0.00223\n",
      "[83,   300] train_loss: 0.00217\n",
      "[83,   400] train_loss: 0.00227\n",
      "[83,   500] train_loss: 0.00216\n",
      "83 0.002189393723156304\n",
      "83 0.0021588402109181666\n",
      "epoch 83: improvement = -9.335542481549771e-06\n",
      "[84,   100] train_loss: 0.00201\n",
      "[84,   200] train_loss: 0.00222\n",
      "[84,   300] train_loss: 0.00235\n",
      "[84,   400] train_loss: 0.00222\n",
      "[84,   500] train_loss: 0.00203\n",
      "84 0.002194644086311441\n",
      "84 0.002158383973820886\n",
      "epoch 84: improvement = -8.879305384269353e-06\n",
      "[85,   100] train_loss: 0.00230\n",
      "[85,   200] train_loss: 0.00217\n",
      "[85,   300] train_loss: 0.00208\n",
      "[85,   400] train_loss: 0.00204\n",
      "[85,   500] train_loss: 0.00229\n",
      "85 0.0021858757891971598\n",
      "85 0.002152771192419821\n",
      "epoch 85: improvement = -3.266523983204081e-06\n",
      "[86,   100] train_loss: 0.00218\n",
      "[86,   200] train_loss: 0.00214\n",
      "[86,   300] train_loss: 0.00227\n",
      "[86,   400] train_loss: 0.00230\n",
      "[86,   500] train_loss: 0.00201\n",
      "86 0.0021897837780068172\n",
      "86 0.002236462127571159\n",
      "epoch 86: improvement = -8.695745913454237e-05\n",
      "[87,   100] train_loss: 0.00215\n",
      "[87,   200] train_loss: 0.00227\n",
      "[87,   300] train_loss: 0.00222\n",
      "[87,   400] train_loss: 0.00210\n",
      "[87,   500] train_loss: 0.00225\n",
      "87 0.002188841504314859\n",
      "87 0.0021736246280486516\n",
      "epoch 87: improvement = -2.4119959612034763e-05\n",
      "[88,   100] train_loss: 0.00231\n",
      "[88,   200] train_loss: 0.00207\n",
      "[88,   300] train_loss: 0.00236\n",
      "[88,   400] train_loss: 0.00220\n",
      "[88,   500] train_loss: 0.00206\n",
      "88 0.0021875267149580515\n",
      "88 0.002150249069404159\n",
      "epoch 88: improvement = -7.444009675421602e-07\n",
      "[89,   100] train_loss: 0.00212\n",
      "[89,   200] train_loss: 0.00226\n",
      "[89,   300] train_loss: 0.00217\n",
      "[89,   400] train_loss: 0.00227\n",
      "[89,   500] train_loss: 0.00218\n",
      "89 0.0021968165817163464\n",
      "89 0.002147916328622503\n",
      "epoch 89: improvement = 1.5883398141137636e-06\n",
      "[90,   100] train_loss: 0.00216\n",
      "[90,   200] train_loss: 0.00225\n",
      "[90,   300] train_loss: 0.00217\n",
      "[90,   400] train_loss: 0.00212\n",
      "[90,   500] train_loss: 0.00224\n",
      "90 0.002186709005310768\n",
      "90 0.0021434766479200566\n",
      "epoch 90: improvement = 4.4396807024464115e-06\n",
      "[91,   100] train_loss: 0.00218\n",
      "[91,   200] train_loss: 0.00219\n",
      "[91,   300] train_loss: 0.00221\n",
      "[91,   400] train_loss: 0.00212\n",
      "[91,   500] train_loss: 0.00214\n",
      "91 0.002183701384544772\n",
      "91 0.0021454475265827973\n",
      "epoch 91: improvement = -1.9708786627406667e-06\n",
      "[92,   100] train_loss: 0.00205\n",
      "[92,   200] train_loss: 0.00220\n",
      "[92,   300] train_loss: 0.00224\n",
      "[92,   400] train_loss: 0.00228\n",
      "[92,   500] train_loss: 0.00223\n",
      "92 0.0021846695948256186\n",
      "92 0.0021475158117081473\n",
      "epoch 92: improvement = -4.039163788090681e-06\n",
      "[93,   100] train_loss: 0.00215\n",
      "[93,   200] train_loss: 0.00238\n",
      "[93,   300] train_loss: 0.00215\n",
      "[93,   400] train_loss: 0.00218\n",
      "[93,   500] train_loss: 0.00211\n",
      "93 0.002187366926262671\n",
      "93 0.002144439977215234\n",
      "epoch 93: improvement = -9.63329295177156e-07\n",
      "[94,   100] train_loss: 0.00236\n",
      "[94,   200] train_loss: 0.00220\n",
      "[94,   300] train_loss: 0.00201\n",
      "[94,   400] train_loss: 0.00207\n",
      "[94,   500] train_loss: 0.00218\n",
      "94 0.0021857551122466735\n",
      "94 0.0021411505237955222\n",
      "epoch 94: improvement = 2.326124124534413e-06\n",
      "[95,   100] train_loss: 0.00217\n",
      "[95,   200] train_loss: 0.00197\n",
      "[95,   300] train_loss: 0.00216\n",
      "[95,   400] train_loss: 0.00231\n",
      "[95,   500] train_loss: 0.00233\n",
      "95 0.00218614689086801\n",
      "95 0.0021411765617843762\n",
      "epoch 95: improvement = -2.603798885400868e-08\n",
      "[96,   100] train_loss: 0.00196\n",
      "[96,   200] train_loss: 0.00225\n",
      "[96,   300] train_loss: 0.00230\n",
      "[96,   400] train_loss: 0.00223\n",
      "[96,   500] train_loss: 0.00206\n",
      "96 0.0021796693768336276\n",
      "96 0.00214523577433215\n",
      "epoch 96: improvement = -4.085250536627928e-06\n",
      "[97,   100] train_loss: 0.00207\n",
      "[97,   200] train_loss: 0.00221\n",
      "[97,   300] train_loss: 0.00205\n",
      "[97,   400] train_loss: 0.00236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97,   500] train_loss: 0.00214\n",
      "97 0.002177271993586185\n",
      "97 0.0021409278833143665\n",
      "epoch 97: improvement = 2.2264048115576765e-07\n",
      "[98,   100] train_loss: 0.00218\n",
      "[98,   200] train_loss: 0.00231\n",
      "[98,   300] train_loss: 0.00205\n",
      "[98,   400] train_loss: 0.00207\n",
      "[98,   500] train_loss: 0.00216\n",
      "98 0.0021823604426697684\n",
      "98 0.0021453804934111285\n",
      "epoch 98: improvement = -4.452610096762014e-06\n",
      "[99,   100] train_loss: 0.00213\n",
      "[99,   200] train_loss: 0.00218\n",
      "[99,   300] train_loss: 0.00214\n",
      "[99,   400] train_loss: 0.00221\n",
      "[99,   500] train_loss: 0.00215\n",
      "99 0.0021794885667178595\n",
      "99 0.0021388073448862835\n",
      "epoch 99: improvement = 2.1205384280829215e-06\n",
      "[100,   100] train_loss: 0.00197\n",
      "[100,   200] train_loss: 0.00200\n",
      "[100,   300] train_loss: 0.00244\n",
      "[100,   400] train_loss: 0.00233\n",
      "[100,   500] train_loss: 0.00226\n",
      "100 0.0021930425338403396\n",
      "100 0.002137030838441904\n",
      "epoch 100: improvement = 1.7765064443796968e-06\n",
      "[101,   100] train_loss: 0.00222\n",
      "[101,   200] train_loss: 0.00210\n",
      "[101,   300] train_loss: 0.00216\n",
      "[101,   400] train_loss: 0.00216\n",
      "[101,   500] train_loss: 0.00222\n",
      "101 0.0021821779227266546\n",
      "101 0.002139239114972451\n",
      "epoch 101: improvement = -2.2082765305472297e-06\n",
      "[102,   100] train_loss: 0.00221\n",
      "[102,   200] train_loss: 0.00242\n",
      "[102,   300] train_loss: 0.00210\n",
      "[102,   400] train_loss: 0.00213\n",
      "[102,   500] train_loss: 0.00215\n",
      "102 0.0021736568525794974\n",
      "102 0.002141651437181284\n",
      "epoch 102: improvement = -4.620598739380009e-06\n",
      "[103,   100] train_loss: 0.00221\n",
      "[103,   200] train_loss: 0.00196\n",
      "[103,   300] train_loss: 0.00214\n",
      "[103,   400] train_loss: 0.00221\n",
      "[103,   500] train_loss: 0.00234\n",
      "103 0.002175072858397523\n",
      "103 0.002138200040340779\n",
      "epoch 103: improvement = -1.1692018988753057e-06\n",
      "[104,   100] train_loss: 0.00221\n",
      "[104,   200] train_loss: 0.00225\n",
      "[104,   300] train_loss: 0.00204\n",
      "[104,   400] train_loss: 0.00213\n",
      "[104,   500] train_loss: 0.00231\n",
      "104 0.002174076006196578\n",
      "104 0.002139744430428568\n",
      "epoch 104: improvement = -2.713591986664224e-06\n",
      "[105,   100] train_loss: 0.00200\n",
      "[105,   200] train_loss: 0.00190\n",
      "[105,   300] train_loss: 0.00244\n",
      "[105,   400] train_loss: 0.00218\n",
      "[105,   500] train_loss: 0.00230\n",
      "105 0.002183289668362883\n",
      "105 0.0021639507323360898\n",
      "epoch 105: improvement = -2.6919893894185917e-05\n",
      "[106,   100] train_loss: 0.00207\n",
      "[106,   200] train_loss: 0.00213\n",
      "[106,   300] train_loss: 0.00219\n",
      "[106,   400] train_loss: 0.00222\n",
      "[106,   500] train_loss: 0.00208\n",
      "106 0.002178746685716217\n",
      "106 0.0021514079137953095\n",
      "epoch 106: improvement = -1.4377075353405686e-05\n",
      "[107,   100] train_loss: 0.00208\n",
      "[107,   200] train_loss: 0.00230\n",
      "[107,   300] train_loss: 0.00210\n",
      "[107,   400] train_loss: 0.00224\n",
      "[107,   500] train_loss: 0.00213\n",
      "107 0.002173845994619552\n",
      "107 0.002142099045032143\n",
      "epoch 107: improvement = -5.068206590239273e-06\n",
      "[108,   100] train_loss: 0.00201\n",
      "[108,   200] train_loss: 0.00238\n",
      "[108,   300] train_loss: 0.00201\n",
      "[108,   400] train_loss: 0.00210\n",
      "[108,   500] train_loss: 0.00232\n",
      "108 0.0021841262964619854\n",
      "108 0.0021370417885573445\n",
      "epoch 108: improvement = -1.0950115440690883e-08\n",
      "[109,   100] train_loss: 0.00227\n",
      "[109,   200] train_loss: 0.00212\n",
      "[109,   300] train_loss: 0.00216\n",
      "[109,   400] train_loss: 0.00228\n",
      "[109,   500] train_loss: 0.00214\n",
      "109 0.0021806135984285924\n",
      "109 0.0021366727633992254\n",
      "epoch 109: improvement = 3.580750426784504e-07\n",
      "[110,   100] train_loss: 0.00214\n",
      "[110,   200] train_loss: 0.00212\n",
      "[110,   300] train_loss: 0.00212\n",
      "[110,   400] train_loss: 0.00227\n",
      "[110,   500] train_loss: 0.00207\n",
      "110 0.0021691191878971234\n",
      "110 0.002162091384305342\n",
      "epoch 110: improvement = -2.5418620906116594e-05\n",
      "[111,   100] train_loss: 0.00200\n",
      "[111,   200] train_loss: 0.00216\n",
      "[111,   300] train_loss: 0.00235\n",
      "[111,   400] train_loss: 0.00213\n",
      "[111,   500] train_loss: 0.00217\n",
      "111 0.0021730237567982166\n",
      "111 0.002169638713453789\n",
      "epoch 111: improvement = -3.296595005456348e-05\n",
      "[112,   100] train_loss: 0.00225\n",
      "[112,   200] train_loss: 0.00205\n",
      "[112,   300] train_loss: 0.00202\n",
      "[112,   400] train_loss: 0.00219\n",
      "[112,   500] train_loss: 0.00231\n",
      "112 0.002172058930512688\n",
      "112 0.002138371040980635\n",
      "epoch 112: improvement = -1.6982775814097448e-06\n",
      "[113,   100] train_loss: 0.00236\n",
      "[113,   200] train_loss: 0.00228\n",
      "[113,   300] train_loss: 0.00216\n",
      "[113,   400] train_loss: 0.00206\n",
      "[113,   500] train_loss: 0.00211\n",
      "113 0.0021744604765931565\n",
      "113 0.0021415687957648333\n",
      "epoch 113: improvement = -4.896032365607917e-06\n",
      "[114,   100] train_loss: 0.00209\n",
      "[114,   200] train_loss: 0.00247\n",
      "[114,   300] train_loss: 0.00196\n",
      "[114,   400] train_loss: 0.00226\n",
      "[114,   500] train_loss: 0.00209\n",
      "114 0.002173003795582019\n",
      "114 0.0021584879172211833\n",
      "epoch 114: improvement = -2.181515382195789e-05\n",
      "[115,   100] train_loss: 0.00218\n",
      "[115,   200] train_loss: 0.00229\n",
      "[115,   300] train_loss: 0.00229\n",
      "[115,   400] train_loss: 0.00205\n",
      "[115,   500] train_loss: 0.00215\n",
      "115 0.0021758062495344658\n",
      "115 0.0021457270403413696\n",
      "epoch 115: improvement = -9.054276942144224e-06\n",
      "[116,   100] train_loss: 0.00233\n",
      "[116,   200] train_loss: 0.00203\n",
      "[116,   300] train_loss: 0.00211\n",
      "[116,   400] train_loss: 0.00208\n",
      "[116,   500] train_loss: 0.00235\n",
      "116 0.002170302990977966\n",
      "116 0.002129285267160593\n",
      "epoch 116: improvement = 7.387496238632409e-06\n",
      "[117,   100] train_loss: 0.00202\n",
      "[117,   200] train_loss: 0.00219\n",
      "[117,   300] train_loss: 0.00206\n",
      "[117,   400] train_loss: 0.00214\n",
      "[117,   500] train_loss: 0.00251\n",
      "117 0.002174041402665516\n",
      "117 0.0021351190844896865\n",
      "epoch 117: improvement = -5.833817329093521e-06\n",
      "[118,   100] train_loss: 0.00226\n",
      "[118,   200] train_loss: 0.00240\n",
      "[118,   300] train_loss: 0.00217\n",
      "[118,   400] train_loss: 0.00198\n",
      "[118,   500] train_loss: 0.00221\n",
      "118 0.0021703834901075966\n",
      "118 0.0021562912788466587\n",
      "epoch 118: improvement = -2.7006011686065735e-05\n",
      "[119,   100] train_loss: 0.00197\n",
      "[119,   200] train_loss: 0.00229\n",
      "[119,   300] train_loss: 0.00226\n",
      "[119,   400] train_loss: 0.00223\n",
      "[119,   500] train_loss: 0.00203\n",
      "119 0.002175901299193278\n",
      "119 0.002142853252195817\n",
      "epoch 119: improvement = -1.356798503522388e-05\n",
      "[120,   100] train_loss: 0.00206\n",
      "[120,   200] train_loss: 0.00196\n",
      "[120,   300] train_loss: 0.00234\n",
      "[120,   400] train_loss: 0.00210\n",
      "[120,   500] train_loss: 0.00227\n",
      "120 0.0021710500151926487\n",
      "120 0.0021313239497774035\n",
      "epoch 120: improvement = -2.0386826168105124e-06\n",
      "[121,   100] train_loss: 0.00218\n",
      "[121,   200] train_loss: 0.00230\n",
      "[121,   300] train_loss: 0.00209\n",
      "[121,   400] train_loss: 0.00231\n",
      "[121,   500] train_loss: 0.00202\n",
      "121 0.0021705700783986367\n",
      "121 0.002133879110874118\n",
      "epoch 121: improvement = -4.593843713525045e-06\n",
      "[122,   100] train_loss: 0.00225\n",
      "[122,   200] train_loss: 0.00237\n",
      "[122,   300] train_loss: 0.00221\n",
      "[122,   400] train_loss: 0.00199\n",
      "[122,   500] train_loss: 0.00197\n",
      "122 0.0021648899432486247\n",
      "122 0.002132789849939075\n",
      "epoch 122: improvement = -3.5045827784818163e-06\n",
      "[123,   100] train_loss: 0.00235\n",
      "[123,   200] train_loss: 0.00203\n",
      "[123,   300] train_loss: 0.00210\n",
      "[123,   400] train_loss: 0.00225\n",
      "[123,   500] train_loss: 0.00229\n",
      "123 0.0021693394026411792\n",
      "123 0.0021336663090841994\n",
      "epoch 123: improvement = -4.38104192360643e-06\n",
      "[124,   100] train_loss: 0.00228\n",
      "[124,   200] train_loss: 0.00222\n",
      "[124,   300] train_loss: 0.00211\n",
      "[124,   400] train_loss: 0.00215\n",
      "[124,   500] train_loss: 0.00197\n",
      "124 0.002166493751623233\n",
      "124 0.0021424255448477683\n",
      "epoch 124: improvement = -1.3140277687175364e-05\n",
      "[125,   100] train_loss: 0.00237\n",
      "[125,   200] train_loss: 0.00197\n",
      "[125,   300] train_loss: 0.00213\n",
      "[125,   400] train_loss: 0.00217\n",
      "[125,   500] train_loss: 0.00212\n",
      "125 0.002167725342590021\n",
      "125 0.0021316787852623436\n",
      "epoch 125: improvement = -2.3935181017505774e-06\n",
      "[126,   100] train_loss: 0.00190\n",
      "[126,   200] train_loss: 0.00223\n",
      "[126,   300] train_loss: 0.00239\n",
      "[126,   400] train_loss: 0.00234\n",
      "[126,   500] train_loss: 0.00205\n",
      "126 0.0021651182913599593\n",
      "126 0.0021526924503250246\n",
      "epoch 126: improvement = -2.3407183164431567e-05\n",
      "[127,   100] train_loss: 0.00214\n",
      "[127,   200] train_loss: 0.00243\n",
      "[127,   300] train_loss: 0.00211\n",
      "[127,   400] train_loss: 0.00210\n",
      "[127,   500] train_loss: 0.00211\n",
      "127 0.0021687774769780445\n",
      "127 0.0021265943139858637\n",
      "epoch 127: improvement = 2.6909531747293207e-06\n",
      "[128,   100] train_loss: 0.00217\n",
      "[128,   200] train_loss: 0.00204\n",
      "[128,   300] train_loss: 0.00211\n",
      "[128,   400] train_loss: 0.00245\n",
      "[128,   500] train_loss: 0.00214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 0.0021638768400316587\n",
      "128 0.0021362260153184737\n",
      "epoch 128: improvement = -9.631701332610005e-06\n",
      "[129,   100] train_loss: 0.00230\n",
      "[129,   200] train_loss: 0.00234\n",
      "[129,   300] train_loss: 0.00210\n",
      "[129,   400] train_loss: 0.00200\n",
      "[129,   500] train_loss: 0.00214\n",
      "129 0.002172645776603584\n",
      "129 0.0021257793188624195\n",
      "epoch 129: improvement = 8.149951234441739e-07\n",
      "[130,   100] train_loss: 0.00220\n",
      "[130,   200] train_loss: 0.00208\n",
      "[130,   300] train_loss: 0.00217\n",
      "[130,   400] train_loss: 0.00213\n",
      "[130,   500] train_loss: 0.00238\n",
      "130 0.0021628574480887135\n",
      "130 0.0021261958985837504\n",
      "epoch 130: improvement = -4.165797213308807e-07\n",
      "[131,   100] train_loss: 0.00235\n",
      "[131,   200] train_loss: 0.00198\n",
      "[131,   300] train_loss: 0.00239\n",
      "[131,   400] train_loss: 0.00203\n",
      "[131,   500] train_loss: 0.00195\n",
      "131 0.002163223877190137\n",
      "131 0.002123693764049997\n",
      "epoch 131: improvement = 2.085554812422327e-06\n",
      "[132,   100] train_loss: 0.00213\n",
      "[132,   200] train_loss: 0.00210\n",
      "[132,   300] train_loss: 0.00220\n",
      "[132,   400] train_loss: 0.00227\n",
      "[132,   500] train_loss: 0.00220\n",
      "132 0.002165451126897987\n",
      "132 0.0021578284385891625\n",
      "epoch 132: improvement = -3.413467453916531e-05\n",
      "[133,   100] train_loss: 0.00218\n",
      "[133,   200] train_loss: 0.00213\n",
      "[133,   300] train_loss: 0.00211\n",
      "[133,   400] train_loss: 0.00239\n",
      "[133,   500] train_loss: 0.00200\n",
      "133 0.0021655393458839354\n",
      "133 0.0021297797014578164\n",
      "epoch 133: improvement = -6.085937407819195e-06\n",
      "[134,   100] train_loss: 0.00185\n",
      "[134,   200] train_loss: 0.00216\n",
      "[134,   300] train_loss: 0.00232\n",
      "[134,   400] train_loss: 0.00224\n",
      "[134,   500] train_loss: 0.00223\n",
      "134 0.0021627144250939666\n",
      "134 0.002126332678665521\n",
      "epoch 134: improvement = -2.6389146155236277e-06\n",
      "[135,   100] train_loss: 0.00219\n",
      "[135,   200] train_loss: 0.00212\n",
      "[135,   300] train_loss: 0.00220\n",
      "[135,   400] train_loss: 0.00205\n",
      "[135,   500] train_loss: 0.00218\n",
      "135 0.0021701166357528685\n",
      "135 0.002123584301303395\n",
      "epoch 135: improvement = 1.094627466021944e-07\n",
      "[136,   100] train_loss: 0.00195\n",
      "[136,   200] train_loss: 0.00221\n",
      "[136,   300] train_loss: 0.00213\n",
      "[136,   400] train_loss: 0.00243\n",
      "[136,   500] train_loss: 0.00220\n",
      "136 0.002162894727747985\n",
      "136 0.0021252602766879646\n",
      "epoch 136: improvement = -1.6759753845695807e-06\n",
      "[137,   100] train_loss: 0.00236\n",
      "[137,   200] train_loss: 0.00230\n",
      "[137,   300] train_loss: 0.00223\n",
      "[137,   400] train_loss: 0.00180\n",
      "[137,   500] train_loss: 0.00207\n",
      "137 0.0021578266495676096\n",
      "137 0.0022025869674085225\n",
      "epoch 137: improvement = -7.900266610512754e-05\n",
      "[138,   100] train_loss: 0.00204\n",
      "[138,   200] train_loss: 0.00235\n",
      "[138,   300] train_loss: 0.00205\n",
      "[138,   400] train_loss: 0.00231\n",
      "[138,   500] train_loss: 0.00208\n",
      "138 0.002161937528591234\n",
      "138 0.002131137149481905\n",
      "epoch 138: improvement = -7.552848178510145e-06\n",
      "[139,   100] train_loss: 0.00224\n",
      "[139,   200] train_loss: 0.00194\n",
      "[139,   300] train_loss: 0.00221\n",
      "[139,   400] train_loss: 0.00223\n",
      "[139,   500] train_loss: 0.00228\n",
      "139 0.0021632884590276472\n",
      "139 0.0021232166738060832\n",
      "epoch 139: improvement = 3.6762749731173924e-07\n",
      "[140,   100] train_loss: 0.00220\n",
      "[140,   200] train_loss: 0.00221\n",
      "[140,   300] train_loss: 0.00219\n",
      "[140,   400] train_loss: 0.00230\n",
      "[140,   500] train_loss: 0.00216\n",
      "140 0.0021889054294808702\n",
      "140 0.0021276260408836867\n",
      "epoch 140: improvement = -4.40936707760348e-06\n",
      "[141,   100] train_loss: 0.00206\n",
      "[141,   200] train_loss: 0.00224\n",
      "[141,   300] train_loss: 0.00209\n",
      "[141,   400] train_loss: 0.00218\n",
      "[141,   500] train_loss: 0.00214\n",
      "141 0.0021535062154482425\n",
      "141 0.0021203066717786407\n",
      "epoch 141: improvement = 2.9100020274425153e-06\n",
      "[142,   100] train_loss: 0.00210\n",
      "[142,   200] train_loss: 0.00219\n",
      "[142,   300] train_loss: 0.00227\n",
      "[142,   400] train_loss: 0.00228\n",
      "[142,   500] train_loss: 0.00198\n",
      "142 0.0021544246364829456\n",
      "142 0.0021253180473503074\n",
      "epoch 142: improvement = -5.011375571666726e-06\n",
      "[143,   100] train_loss: 0.00220\n",
      "[143,   200] train_loss: 0.00206\n",
      "[143,   300] train_loss: 0.00226\n",
      "[143,   400] train_loss: 0.00209\n",
      "[143,   500] train_loss: 0.00219\n",
      "143 0.0021580957866807944\n",
      "143 0.002134828805166314\n",
      "epoch 143: improvement = -1.4522133387673188e-05\n",
      "[144,   100] train_loss: 0.00210\n",
      "[144,   200] train_loss: 0.00217\n",
      "[144,   300] train_loss: 0.00215\n",
      "[144,   400] train_loss: 0.00197\n",
      "[144,   500] train_loss: 0.00222\n",
      "144 0.0021558942908549363\n",
      "144 0.0021181612916663633\n",
      "epoch 144: improvement = 2.1453801122774292e-06\n",
      "[145,   100] train_loss: 0.00215\n",
      "[145,   200] train_loss: 0.00248\n",
      "[145,   300] train_loss: 0.00211\n",
      "[145,   400] train_loss: 0.00194\n",
      "[145,   500] train_loss: 0.00203\n",
      "145 0.002161716884252474\n",
      "145 0.0021240648150257883\n",
      "epoch 145: improvement = -5.903523359425053e-06\n",
      "[146,   100] train_loss: 0.00218\n",
      "[146,   200] train_loss: 0.00196\n",
      "[146,   300] train_loss: 0.00201\n",
      "[146,   400] train_loss: 0.00218\n",
      "[146,   500] train_loss: 0.00249\n",
      "146 0.0021571243370132704\n",
      "146 0.0021300043664916075\n",
      "epoch 146: improvement = -1.1843074825244188e-05\n",
      "[147,   100] train_loss: 0.00247\n",
      "[147,   200] train_loss: 0.00212\n",
      "[147,   300] train_loss: 0.00223\n",
      "[147,   400] train_loss: 0.00174\n",
      "[147,   500] train_loss: 0.00225\n",
      "147 0.002160290592771151\n",
      "147 0.0021167022540251814\n",
      "epoch 147: improvement = 1.4590376411819338e-06\n",
      "[148,   100] train_loss: 0.00204\n",
      "[148,   200] train_loss: 0.00216\n",
      "[148,   300] train_loss: 0.00205\n",
      "[148,   400] train_loss: 0.00217\n",
      "[148,   500] train_loss: 0.00232\n",
      "148 0.002157357158595475\n",
      "148 0.002119943193162713\n",
      "epoch 148: improvement = -3.2409391375314706e-06\n",
      "[149,   100] train_loss: 0.00230\n",
      "[149,   200] train_loss: 0.00215\n",
      "[149,   300] train_loss: 0.00205\n",
      "[149,   400] train_loss: 0.00222\n",
      "[149,   500] train_loss: 0.00210\n",
      "149 0.0021549297481552963\n",
      "149 0.00212220948395889\n",
      "epoch 149: improvement = -5.507229933708439e-06\n",
      "[150,   100] train_loss: 0.00210\n",
      "[150,   200] train_loss: 0.00199\n",
      "[150,   300] train_loss: 0.00225\n",
      "[150,   400] train_loss: 0.00228\n",
      "[150,   500] train_loss: 0.00217\n",
      "150 0.0021523839024145423\n",
      "150 0.0021188074179822633\n",
      "epoch 150: improvement = -2.1051639570819278e-06\n",
      "[151,   100] train_loss: 0.00222\n",
      "[151,   200] train_loss: 0.00212\n",
      "[151,   300] train_loss: 0.00219\n",
      "[151,   400] train_loss: 0.00219\n",
      "[151,   500] train_loss: 0.00208\n",
      "151 0.002155159215796278\n",
      "151 0.0021160898703579075\n",
      "epoch 151: improvement = 6.123836672738275e-07\n",
      "[152,   100] train_loss: 0.00229\n",
      "[152,   200] train_loss: 0.00214\n",
      "[152,   300] train_loss: 0.00214\n",
      "[152,   400] train_loss: 0.00216\n",
      "[152,   500] train_loss: 0.00208\n",
      "152 0.0021529599615611257\n",
      "152 0.002122690908867709\n",
      "epoch 152: improvement = -6.601038509801427e-06\n",
      "[153,   100] train_loss: 0.00221\n",
      "[153,   200] train_loss: 0.00219\n",
      "[153,   300] train_loss: 0.00187\n",
      "[153,   400] train_loss: 0.00242\n",
      "[153,   500] train_loss: 0.00194\n",
      "153 0.0021564099366307857\n",
      "153 0.0021186076192329013\n",
      "epoch 153: improvement = -2.5177488749937582e-06\n",
      "[154,   100] train_loss: 0.00195\n",
      "[154,   200] train_loss: 0.00210\n",
      "[154,   300] train_loss: 0.00238\n",
      "[154,   400] train_loss: 0.00240\n",
      "[154,   500] train_loss: 0.00190\n",
      "154 0.0021583330278452397\n",
      "154 0.0021225945926253613\n",
      "epoch 154: improvement = -6.504722267453803e-06\n",
      "[155,   100] train_loss: 0.00211\n",
      "[155,   200] train_loss: 0.00207\n",
      "[155,   300] train_loss: 0.00210\n",
      "[155,   400] train_loss: 0.00212\n",
      "[155,   500] train_loss: 0.00238\n",
      "155 0.0021498620104877806\n",
      "155 0.0021153594448931664\n",
      "epoch 155: improvement = 7.304254647411308e-07\n",
      "[156,   100] train_loss: 0.00213\n",
      "[156,   200] train_loss: 0.00221\n",
      "[156,   300] train_loss: 0.00209\n",
      "[156,   400] train_loss: 0.00214\n",
      "[156,   500] train_loss: 0.00228\n",
      "156 0.0021508863391848337\n",
      "156 0.0021300402494700203\n",
      "epoch 156: improvement = -1.4680804576853914e-05\n",
      "[157,   100] train_loss: 0.00226\n",
      "[157,   200] train_loss: 0.00214\n",
      "[157,   300] train_loss: 0.00210\n",
      "[157,   400] train_loss: 0.00207\n",
      "[157,   500] train_loss: 0.00223\n",
      "157 0.0021554183029450316\n",
      "157 0.002115005507734077\n",
      "epoch 157: improvement = 3.5393715908929244e-07\n",
      "[158,   100] train_loss: 0.00227\n",
      "[158,   200] train_loss: 0.00212\n",
      "[158,   300] train_loss: 0.00215\n",
      "[158,   400] train_loss: 0.00214\n",
      "[158,   500] train_loss: 0.00201\n",
      "158 0.002151249216549917\n",
      "158 0.0021236188210808373\n",
      "epoch 158: improvement = -8.613313346760194e-06\n",
      "[159,   100] train_loss: 0.00205\n",
      "[159,   200] train_loss: 0.00201\n",
      "[159,   300] train_loss: 0.00230\n",
      "[159,   400] train_loss: 0.00224\n",
      "[159,   500] train_loss: 0.00228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 0.0021633999741922276\n",
      "159 0.00211173246630071\n",
      "epoch 159: improvement = 3.2730414333669716e-06\n",
      "[160,   100] train_loss: 0.00203\n",
      "[160,   200] train_loss: 0.00203\n",
      "[160,   300] train_loss: 0.00222\n",
      "[160,   400] train_loss: 0.00210\n",
      "[160,   500] train_loss: 0.00237\n",
      "160 0.0021483735605421096\n",
      "160 0.0021180367804455656\n",
      "epoch 160: improvement = -6.304314144855473e-06\n",
      "[161,   100] train_loss: 0.00214\n",
      "[161,   200] train_loss: 0.00203\n",
      "[161,   300] train_loss: 0.00213\n",
      "[161,   400] train_loss: 0.00231\n",
      "[161,   500] train_loss: 0.00217\n",
      "161 0.00215106224520134\n",
      "161 0.0021106380988850214\n",
      "epoch 161: improvement = 1.0943674156886803e-06\n",
      "[162,   100] train_loss: 0.00206\n",
      "[162,   200] train_loss: 0.00206\n",
      "[162,   300] train_loss: 0.00210\n",
      "[162,   400] train_loss: 0.00204\n",
      "[162,   500] train_loss: 0.00236\n",
      "162 0.0021475227163626574\n",
      "162 0.002126670359214798\n",
      "epoch 162: improvement = -1.6032260329776424e-05\n",
      "[163,   100] train_loss: 0.00217\n",
      "[163,   200] train_loss: 0.00238\n",
      "[163,   300] train_loss: 0.00199\n",
      "[163,   400] train_loss: 0.00211\n",
      "[163,   500] train_loss: 0.00211\n",
      "163 0.002146490511675592\n",
      "163 0.0021098101078129206\n",
      "epoch 163: improvement = 8.279910721008081e-07\n",
      "[164,   100] train_loss: 0.00221\n",
      "[164,   200] train_loss: 0.00211\n",
      "[164,   300] train_loss: 0.00222\n",
      "[164,   400] train_loss: 0.00209\n",
      "[164,   500] train_loss: 0.00206\n",
      "164 0.002149721019166721\n",
      "164 0.0021179159410361875\n",
      "epoch 164: improvement = -8.105833223266863e-06\n",
      "[165,   100] train_loss: 0.00215\n",
      "[165,   200] train_loss: 0.00204\n",
      "[165,   300] train_loss: 0.00191\n",
      "[165,   400] train_loss: 0.00218\n",
      "[165,   500] train_loss: 0.00241\n",
      "165 0.002148204589613063\n",
      "165 0.0021145606114396725\n",
      "epoch 165: improvement = -4.750503626751827e-06\n",
      "[166,   100] train_loss: 0.00216\n",
      "[166,   200] train_loss: 0.00235\n",
      "[166,   300] train_loss: 0.00223\n",
      "[166,   400] train_loss: 0.00197\n",
      "[166,   500] train_loss: 0.00220\n",
      "166 0.0021571619140419696\n",
      "166 0.0021062720781494897\n",
      "epoch 166: improvement = 3.5380296634309784e-06\n",
      "[167,   100] train_loss: 0.00219\n",
      "[167,   200] train_loss: 0.00206\n",
      "[167,   300] train_loss: 0.00224\n",
      "[167,   400] train_loss: 0.00215\n",
      "[167,   500] train_loss: 0.00210\n",
      "167 0.0021459732599805733\n",
      "167 0.002106692553375821\n",
      "epoch 167: improvement = -4.204752263314848e-07\n",
      "[168,   100] train_loss: 0.00222\n",
      "[168,   200] train_loss: 0.00216\n",
      "[168,   300] train_loss: 0.00218\n",
      "[168,   400] train_loss: 0.00194\n",
      "[168,   500] train_loss: 0.00220\n",
      "168 0.0021436000124436284\n",
      "168 0.002123214017770259\n",
      "epoch 168: improvement = -1.694193962076913e-05\n",
      "[169,   100] train_loss: 0.00197\n",
      "[169,   200] train_loss: 0.00211\n",
      "[169,   300] train_loss: 0.00204\n",
      "[169,   400] train_loss: 0.00239\n",
      "[169,   500] train_loss: 0.00217\n",
      "169 0.0021590709855260535\n",
      "169 0.0021945300226896814\n",
      "epoch 169: improvement = -8.825794454019173e-05\n",
      "[170,   100] train_loss: 0.00227\n",
      "[170,   200] train_loss: 0.00210\n",
      "[170,   300] train_loss: 0.00227\n",
      "[170,   400] train_loss: 0.00187\n",
      "[170,   500] train_loss: 0.00228\n",
      "170 0.002140403845676198\n",
      "170 0.0021022058687393194\n",
      "epoch 170: improvement = 4.066209410170246e-06\n",
      "[171,   100] train_loss: 0.00233\n",
      "[171,   200] train_loss: 0.00202\n",
      "[171,   300] train_loss: 0.00209\n",
      "[171,   400] train_loss: 0.00193\n",
      "[171,   500] train_loss: 0.00218\n",
      "171 0.002141259161468631\n",
      "171 0.0021339168622792034\n",
      "epoch 171: improvement = -3.171099353988394e-05\n",
      "[172,   100] train_loss: 0.00203\n",
      "[172,   200] train_loss: 0.00205\n",
      "[172,   300] train_loss: 0.00237\n",
      "[172,   400] train_loss: 0.00215\n",
      "[172,   500] train_loss: 0.00208\n",
      "172 0.0021406627813344513\n",
      "172 0.0021095402330831097\n",
      "epoch 172: improvement = -7.3343643437902804e-06\n",
      "[173,   100] train_loss: 0.00223\n",
      "[173,   200] train_loss: 0.00218\n",
      "[173,   300] train_loss: 0.00196\n",
      "[173,   400] train_loss: 0.00207\n",
      "[173,   500] train_loss: 0.00221\n",
      "173 0.0021435885908146165\n",
      "173 0.002099652701812381\n",
      "epoch 173: improvement = 2.5531669269386083e-06\n",
      "[174,   100] train_loss: 0.00199\n",
      "[174,   200] train_loss: 0.00212\n",
      "[174,   300] train_loss: 0.00225\n",
      "[174,   400] train_loss: 0.00229\n",
      "[174,   500] train_loss: 0.00201\n",
      "174 0.0021365274874503067\n",
      "174 0.002108665866743784\n",
      "epoch 174: improvement = -9.01316493140316e-06\n",
      "[175,   100] train_loss: 0.00220\n",
      "[175,   200] train_loss: 0.00205\n",
      "[175,   300] train_loss: 0.00203\n",
      "[175,   400] train_loss: 0.00232\n",
      "[175,   500] train_loss: 0.00204\n",
      "175 0.002135586874699963\n",
      "175 0.0021057317861405004\n",
      "epoch 175: improvement = -6.079084328119554e-06\n",
      "[176,   100] train_loss: 0.00197\n",
      "[176,   200] train_loss: 0.00200\n",
      "[176,   300] train_loss: 0.00225\n",
      "[176,   400] train_loss: 0.00227\n",
      "[176,   500] train_loss: 0.00208\n",
      "176 0.0021416931446279837\n",
      "176 0.002115810447875674\n",
      "epoch 176: improvement = -1.6157746063293326e-05\n",
      "[177,   100] train_loss: 0.00211\n",
      "[177,   200] train_loss: 0.00219\n",
      "[177,   300] train_loss: 0.00222\n",
      "[177,   400] train_loss: 0.00202\n",
      "[177,   500] train_loss: 0.00213\n",
      "177 0.002140584055322458\n",
      "177 0.002101422744836253\n",
      "epoch 177: improvement = -1.770043023872122e-06\n",
      "[178,   100] train_loss: 0.00226\n",
      "[178,   200] train_loss: 0.00213\n",
      "[178,   300] train_loss: 0.00220\n",
      "[178,   400] train_loss: 0.00209\n",
      "[178,   500] train_loss: 0.00217\n",
      "178 0.002139056032550008\n",
      "178 0.0020953061582587116\n",
      "epoch 178: improvement = 4.3465435536692125e-06\n",
      "[179,   100] train_loss: 0.00226\n",
      "[179,   200] train_loss: 0.00213\n",
      "[179,   300] train_loss: 0.00210\n",
      "[179,   400] train_loss: 0.00208\n",
      "[179,   500] train_loss: 0.00201\n",
      "179 0.0021337122911184244\n",
      "179 0.002158594437537565\n",
      "epoch 179: improvement = -6.32882792788534e-05\n",
      "[180,   100] train_loss: 0.00227\n",
      "[180,   200] train_loss: 0.00207\n",
      "[180,   300] train_loss: 0.00211\n",
      "[180,   400] train_loss: 0.00217\n",
      "[180,   500] train_loss: 0.00221\n",
      "180 0.0021358471257282327\n",
      "180 0.0021501331595954916\n",
      "epoch 180: improvement = -5.482700133677999e-05\n",
      "[181,   100] train_loss: 0.00218\n",
      "[181,   200] train_loss: 0.00209\n",
      "[181,   300] train_loss: 0.00214\n",
      "[181,   400] train_loss: 0.00207\n",
      "[181,   500] train_loss: 0.00215\n",
      "181 0.0021443278756864198\n",
      "181 0.002103628325023013\n",
      "epoch 181: improvement = -8.32216676430143e-06\n",
      "[182,   100] train_loss: 0.00202\n",
      "[182,   200] train_loss: 0.00206\n",
      "[182,   300] train_loss: 0.00214\n",
      "[182,   400] train_loss: 0.00191\n",
      "[182,   500] train_loss: 0.00248\n",
      "182 0.00212792493282685\n",
      "182 0.0021055775945690637\n",
      "epoch 182: improvement = -1.0271436310352062e-05\n",
      "[183,   100] train_loss: 0.00222\n",
      "[183,   200] train_loss: 0.00219\n",
      "[183,   300] train_loss: 0.00210\n",
      "[183,   400] train_loss: 0.00194\n",
      "[183,   500] train_loss: 0.00225\n",
      "183 0.0021323997007965763\n",
      "183 0.002089931143380953\n",
      "epoch 183: improvement = 5.375014877758485e-06\n",
      "[184,   100] train_loss: 0.00200\n",
      "[184,   200] train_loss: 0.00201\n",
      "[184,   300] train_loss: 0.00218\n",
      "[184,   400] train_loss: 0.00241\n",
      "[184,   500] train_loss: 0.00199\n",
      "184 0.002125555331982463\n",
      "184 0.002094405643685485\n",
      "epoch 184: improvement = -4.474500304531715e-06\n",
      "[185,   100] train_loss: 0.00228\n",
      "[185,   200] train_loss: 0.00206\n",
      "[185,   300] train_loss: 0.00200\n",
      "[185,   400] train_loss: 0.00229\n",
      "[185,   500] train_loss: 0.00211\n",
      "185 0.0021281892658562545\n",
      "185 0.0021005545716644676\n",
      "epoch 185: improvement = -1.0623428283514525e-05\n",
      "[186,   100] train_loss: 0.00223\n",
      "[186,   200] train_loss: 0.00202\n",
      "[186,   300] train_loss: 0.00214\n",
      "[186,   400] train_loss: 0.00203\n",
      "[186,   500] train_loss: 0.00229\n",
      "186 0.0021312534242349194\n",
      "186 0.002086858456279691\n",
      "epoch 186: improvement = 3.0726871012619686e-06\n",
      "[187,   100] train_loss: 0.00189\n",
      "[187,   200] train_loss: 0.00213\n",
      "[187,   300] train_loss: 0.00231\n",
      "[187,   400] train_loss: 0.00211\n",
      "[187,   500] train_loss: 0.00211\n",
      "187 0.0021234958261791424\n",
      "187 0.0020965148738247437\n",
      "epoch 187: improvement = -9.6564175450526e-06\n",
      "[188,   100] train_loss: 0.00199\n",
      "[188,   200] train_loss: 0.00209\n",
      "[188,   300] train_loss: 0.00234\n",
      "[188,   400] train_loss: 0.00201\n",
      "[188,   500] train_loss: 0.00216\n",
      "188 0.002125315645364235\n",
      "188 0.002110053546818845\n",
      "epoch 188: improvement = -2.3195090539154025e-05\n",
      "[189,   100] train_loss: 0.00217\n",
      "[189,   200] train_loss: 0.00201\n",
      "[189,   300] train_loss: 0.00228\n",
      "[189,   400] train_loss: 0.00217\n",
      "[189,   500] train_loss: 0.00196\n",
      "189 0.0021286956423042184\n",
      "189 0.002093926443509669\n",
      "epoch 189: improvement = -7.067987229977812e-06\n",
      "[190,   100] train_loss: 0.00183\n",
      "[190,   200] train_loss: 0.00250\n",
      "[190,   300] train_loss: 0.00197\n",
      "[190,   400] train_loss: 0.00224\n",
      "[190,   500] train_loss: 0.00218\n",
      "190 0.002118985873747418\n",
      "190 0.002080462377711334\n",
      "epoch 190: improvement = 6.396078568357208e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[191,   100] train_loss: 0.00205\n",
      "[191,   200] train_loss: 0.00228\n",
      "[191,   300] train_loss: 0.00195\n",
      "[191,   400] train_loss: 0.00222\n",
      "[191,   500] train_loss: 0.00204\n",
      "191 0.0021185358000656825\n",
      "191 0.002080874723503925\n",
      "epoch 191: improvement = -4.1234579259115736e-07\n",
      "[192,   100] train_loss: 0.00219\n",
      "[192,   200] train_loss: 0.00209\n",
      "[192,   300] train_loss: 0.00214\n",
      "[192,   400] train_loss: 0.00190\n",
      "[192,   500] train_loss: 0.00219\n",
      "192 0.0021206408942387035\n",
      "192 0.00209094730687733\n",
      "epoch 192: improvement = -1.0484929165996155e-05\n",
      "[193,   100] train_loss: 0.00205\n",
      "[193,   200] train_loss: 0.00200\n",
      "[193,   300] train_loss: 0.00214\n",
      "[193,   400] train_loss: 0.00209\n",
      "[193,   500] train_loss: 0.00222\n",
      "193 0.002118642523797455\n",
      "193 0.0020897025878425382\n",
      "epoch 193: improvement = -9.240210131204307e-06\n",
      "[194,   100] train_loss: 0.00212\n",
      "[194,   200] train_loss: 0.00204\n",
      "[194,   300] train_loss: 0.00213\n",
      "[194,   400] train_loss: 0.00205\n",
      "[194,   500] train_loss: 0.00225\n",
      "194 0.002117355602870396\n",
      "194 0.002081110021213005\n",
      "epoch 194: improvement = -6.476435016711073e-07\n",
      "[195,   100] train_loss: 0.00223\n",
      "[195,   200] train_loss: 0.00196\n",
      "[195,   300] train_loss: 0.00212\n",
      "[195,   400] train_loss: 0.00213\n",
      "[195,   500] train_loss: 0.00217\n",
      "195 0.0021183674747470463\n",
      "195 0.0020841315467839093\n",
      "epoch 195: improvement = -3.6691690725753658e-06\n",
      "[196,   100] train_loss: 0.00213\n",
      "[196,   200] train_loss: 0.00219\n",
      "[196,   300] train_loss: 0.00212\n",
      "[196,   400] train_loss: 0.00201\n",
      "[196,   500] train_loss: 0.00202\n",
      "196 0.00211493848553017\n",
      "196 0.002072065552419948\n",
      "epoch 196: improvement = 8.396825291385936e-06\n",
      "[197,   100] train_loss: 0.00219\n",
      "[197,   200] train_loss: 0.00206\n",
      "[197,   300] train_loss: 0.00205\n",
      "[197,   400] train_loss: 0.00221\n",
      "[197,   500] train_loss: 0.00212\n",
      "197 0.0021098334145946717\n",
      "197 0.0020829716004628924\n",
      "epoch 197: improvement = -1.0906048042944386e-05\n",
      "[198,   100] train_loss: 0.00218\n",
      "[198,   200] train_loss: 0.00207\n",
      "[198,   300] train_loss: 0.00218\n",
      "[198,   400] train_loss: 0.00206\n",
      "[198,   500] train_loss: 0.00225\n",
      "198 0.0021171927561484142\n",
      "198 0.002094929071465193\n",
      "epoch 198: improvement = -2.2863519045244855e-05\n",
      "[199,   100] train_loss: 0.00206\n",
      "[199,   200] train_loss: 0.00204\n",
      "[199,   300] train_loss: 0.00209\n",
      "[199,   400] train_loss: 0.00214\n",
      "[199,   500] train_loss: 0.00215\n",
      "199 0.002112835037184758\n",
      "199 0.002084268362785627\n",
      "epoch 199: improvement = -1.220281036567886e-05\n",
      "[200,   100] train_loss: 0.00207\n",
      "[200,   200] train_loss: 0.00197\n",
      "[200,   300] train_loss: 0.00227\n",
      "[200,   400] train_loss: 0.00224\n",
      "[200,   500] train_loss: 0.00209\n",
      "200 0.002108398442937905\n",
      "200 0.0020737322367818346\n",
      "epoch 200: improvement = -1.6666843618865987e-06\n",
      "[201,   100] train_loss: 0.00220\n",
      "[201,   200] train_loss: 0.00206\n",
      "[201,   300] train_loss: 0.00207\n",
      "[201,   400] train_loss: 0.00211\n",
      "[201,   500] train_loss: 0.00220\n",
      "201 0.0021103558415915555\n",
      "201 0.0020815409505761787\n",
      "epoch 201: improvement = -9.47539815623075e-06\n",
      "[202,   100] train_loss: 0.00201\n",
      "[202,   200] train_loss: 0.00215\n",
      "[202,   300] train_loss: 0.00222\n",
      "[202,   400] train_loss: 0.00212\n",
      "[202,   500] train_loss: 0.00210\n",
      "202 0.0021113195431308163\n",
      "202 0.002076639259078016\n",
      "epoch 202: improvement = -4.5737066580679026e-06\n",
      "[203,   100] train_loss: 0.00189\n",
      "[203,   200] train_loss: 0.00225\n",
      "[203,   300] train_loss: 0.00223\n",
      "[203,   400] train_loss: 0.00218\n",
      "[203,   500] train_loss: 0.00210\n",
      "203 0.0021082531843830917\n",
      "203 0.002062498144221648\n",
      "epoch 203: improvement = 9.56740819830007e-06\n",
      "[204,   100] train_loss: 0.00198\n",
      "[204,   200] train_loss: 0.00197\n",
      "[204,   300] train_loss: 0.00222\n",
      "[204,   400] train_loss: 0.00209\n",
      "[204,   500] train_loss: 0.00224\n",
      "204 0.0021016499846838307\n",
      "204 0.0020726659673779005\n",
      "epoch 204: improvement = -1.0167823156252524e-05\n",
      "[205,   100] train_loss: 0.00224\n",
      "[205,   200] train_loss: 0.00224\n",
      "[205,   300] train_loss: 0.00217\n",
      "[205,   400] train_loss: 0.00186\n",
      "[205,   500] train_loss: 0.00195\n",
      "205 0.0021009414382781666\n",
      "205 0.002080846485313588\n",
      "epoch 205: improvement = -1.834834109193991e-05\n",
      "[206,   100] train_loss: 0.00225\n",
      "[206,   200] train_loss: 0.00201\n",
      "[206,   300] train_loss: 0.00234\n",
      "[206,   400] train_loss: 0.00197\n",
      "[206,   500] train_loss: 0.00201\n",
      "206 0.0021035680029003376\n",
      "206 0.002062899289319995\n",
      "epoch 206: improvement = -4.011450983470999e-07\n",
      "[207,   100] train_loss: 0.00201\n",
      "[207,   200] train_loss: 0.00225\n",
      "[207,   300] train_loss: 0.00221\n",
      "[207,   400] train_loss: 0.00211\n",
      "[207,   500] train_loss: 0.00192\n",
      "207 0.002100794984597879\n",
      "207 0.0020812715415797906\n",
      "epoch 207: improvement = -1.8773397358142624e-05\n",
      "[208,   100] train_loss: 0.00205\n",
      "[208,   200] train_loss: 0.00203\n",
      "[208,   300] train_loss: 0.00223\n",
      "[208,   400] train_loss: 0.00189\n",
      "[208,   500] train_loss: 0.00224\n",
      "208 0.0020992220844044695\n",
      "208 0.0020625416592747296\n",
      "epoch 208: improvement = -4.351505308169967e-08\n",
      "[209,   100] train_loss: 0.00201\n",
      "[209,   200] train_loss: 0.00231\n",
      "[209,   300] train_loss: 0.00196\n",
      "[209,   400] train_loss: 0.00202\n",
      "[209,   500] train_loss: 0.00207\n",
      "209 0.0020987133444710223\n",
      "209 0.0020740910269453038\n",
      "epoch 209: improvement = -1.1592882723655829e-05\n",
      "[210,   100] train_loss: 0.00220\n",
      "[210,   200] train_loss: 0.00213\n",
      "[210,   300] train_loss: 0.00194\n",
      "[210,   400] train_loss: 0.00221\n",
      "[210,   500] train_loss: 0.00203\n",
      "210 0.002094952371656194\n",
      "210 0.002059862208701244\n",
      "epoch 210: improvement = 2.6359355204037295e-06\n",
      "[211,   100] train_loss: 0.00208\n",
      "[211,   200] train_loss: 0.00222\n",
      "[211,   300] train_loss: 0.00197\n",
      "[211,   400] train_loss: 0.00221\n",
      "[211,   500] train_loss: 0.00214\n",
      "211 0.0020957935566355036\n",
      "211 0.0020803270226823598\n",
      "epoch 211: improvement = -2.046481398111557e-05\n",
      "[212,   100] train_loss: 0.00205\n",
      "[212,   200] train_loss: 0.00203\n",
      "[212,   300] train_loss: 0.00217\n",
      "[212,   400] train_loss: 0.00197\n",
      "[212,   500] train_loss: 0.00226\n",
      "212 0.0020921470194636615\n",
      "212 0.0020516171134153274\n",
      "epoch 212: improvement = 8.245095285916784e-06\n",
      "[213,   100] train_loss: 0.00187\n",
      "[213,   200] train_loss: 0.00221\n",
      "[213,   300] train_loss: 0.00206\n",
      "[213,   400] train_loss: 0.00217\n",
      "[213,   500] train_loss: 0.00213\n",
      "213 0.002093143684872224\n",
      "213 0.002051691715978778\n",
      "epoch 213: improvement = -7.460256345060562e-08\n",
      "[214,   100] train_loss: 0.00204\n",
      "[214,   200] train_loss: 0.00213\n",
      "[214,   300] train_loss: 0.00233\n",
      "[214,   400] train_loss: 0.00196\n",
      "[214,   500] train_loss: 0.00210\n",
      "214 0.0020919119804792513\n",
      "214 0.0020575590407130276\n",
      "epoch 214: improvement = -5.9419272977001905e-06\n",
      "[215,   100] train_loss: 0.00204\n",
      "[215,   200] train_loss: 0.00199\n",
      "[215,   300] train_loss: 0.00222\n",
      "[215,   400] train_loss: 0.00206\n",
      "[215,   500] train_loss: 0.00206\n",
      "215 0.002089615742914871\n",
      "215 0.002061116757596805\n",
      "epoch 215: improvement = -9.499644181477744e-06\n",
      "[216,   100] train_loss: 0.00194\n",
      "[216,   200] train_loss: 0.00204\n",
      "[216,   300] train_loss: 0.00239\n",
      "[216,   400] train_loss: 0.00210\n",
      "[216,   500] train_loss: 0.00194\n",
      "216 0.0020865580272595168\n",
      "216 0.0020542481771128967\n",
      "epoch 216: improvement = -2.6310636975692553e-06\n",
      "[217,   100] train_loss: 0.00214\n",
      "[217,   200] train_loss: 0.00200\n",
      "[217,   300] train_loss: 0.00199\n",
      "[217,   400] train_loss: 0.00207\n",
      "[217,   500] train_loss: 0.00215\n",
      "217 0.0020858609785774696\n",
      "217 0.002042666212349118\n",
      "epoch 217: improvement = 8.95090106620925e-06\n",
      "[218,   100] train_loss: 0.00194\n",
      "[218,   200] train_loss: 0.00211\n",
      "[218,   300] train_loss: 0.00212\n",
      "[218,   400] train_loss: 0.00213\n",
      "[218,   500] train_loss: 0.00198\n",
      "218 0.002083026478026897\n",
      "218 0.0020571655359798804\n",
      "epoch 218: improvement = -1.4499323630762229e-05\n",
      "[219,   100] train_loss: 0.00224\n",
      "[219,   200] train_loss: 0.00220\n",
      "[219,   300] train_loss: 0.00217\n",
      "[219,   400] train_loss: 0.00201\n",
      "[219,   500] train_loss: 0.00192\n",
      "219 0.0020839796193622413\n",
      "219 0.002049805391515011\n",
      "epoch 219: improvement = -7.139179165892816e-06\n",
      "[220,   100] train_loss: 0.00203\n",
      "[220,   200] train_loss: 0.00223\n",
      "[220,   300] train_loss: 0.00186\n",
      "[220,   400] train_loss: 0.00222\n",
      "[220,   500] train_loss: 0.00211\n",
      "220 0.0020872290358408354\n",
      "220 0.0020409006065395426\n",
      "epoch 220: improvement = 1.7656058095756079e-06\n",
      "[221,   100] train_loss: 0.00194\n",
      "[221,   200] train_loss: 0.00199\n",
      "[221,   300] train_loss: 0.00215\n",
      "[221,   400] train_loss: 0.00225\n",
      "[221,   500] train_loss: 0.00207\n",
      "221 0.002078779331849168\n",
      "221 0.0020631629850239993\n",
      "epoch 221: improvement = -2.2262378484456773e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[222,   100] train_loss: 0.00213\n",
      "[222,   200] train_loss: 0.00199\n",
      "[222,   300] train_loss: 0.00208\n",
      "[222,   400] train_loss: 0.00182\n",
      "[222,   500] train_loss: 0.00217\n",
      "222 0.002078604682524862\n",
      "222 0.002055544335942918\n",
      "epoch 222: improvement = -1.464372940337532e-05\n",
      "[223,   100] train_loss: 0.00204\n",
      "[223,   200] train_loss: 0.00214\n",
      "[223,   300] train_loss: 0.00195\n",
      "[223,   400] train_loss: 0.00205\n",
      "[223,   500] train_loss: 0.00231\n",
      "223 0.002082726465527441\n",
      "223 0.0020439563257871104\n",
      "epoch 223: improvement = -3.0557192475678258e-06\n",
      "[224,   100] train_loss: 0.00206\n",
      "[224,   200] train_loss: 0.00192\n",
      "[224,   300] train_loss: 0.00222\n",
      "[224,   400] train_loss: 0.00197\n",
      "[224,   500] train_loss: 0.00218\n",
      "224 0.0020743295461975535\n",
      "224 0.00204865813612487\n",
      "epoch 224: improvement = -7.757529585327577e-06\n",
      "[225,   100] train_loss: 0.00207\n",
      "[225,   200] train_loss: 0.00206\n",
      "[225,   300] train_loss: 0.00192\n",
      "[225,   400] train_loss: 0.00238\n",
      "[225,   500] train_loss: 0.00196\n",
      "225 0.0020772007522835327\n",
      "225 0.002033863106414875\n",
      "epoch 225: improvement = 7.037500124667476e-06\n",
      "[226,   100] train_loss: 0.00218\n",
      "[226,   200] train_loss: 0.00209\n",
      "[226,   300] train_loss: 0.00185\n",
      "[226,   400] train_loss: 0.00216\n",
      "[226,   500] train_loss: 0.00198\n",
      "226 0.0020708118181367477\n",
      "226 0.002031215569682452\n",
      "epoch 226: improvement = 2.6475367324229823e-06\n",
      "[227,   100] train_loss: 0.00205\n",
      "[227,   200] train_loss: 0.00202\n",
      "[227,   300] train_loss: 0.00199\n",
      "[227,   400] train_loss: 0.00223\n",
      "[227,   500] train_loss: 0.00210\n",
      "227 0.002069972223107023\n",
      "227 0.0020359903771863387\n",
      "epoch 227: improvement = -4.774807503886598e-06\n",
      "[228,   100] train_loss: 0.00189\n",
      "[228,   200] train_loss: 0.00203\n",
      "[228,   300] train_loss: 0.00229\n",
      "[228,   400] train_loss: 0.00215\n",
      "[228,   500] train_loss: 0.00212\n",
      "228 0.002077536809838601\n",
      "228 0.002031547292460908\n",
      "epoch 228: improvement = -3.3172277845569184e-07\n",
      "[229,   100] train_loss: 0.00205\n",
      "[229,   200] train_loss: 0.00203\n",
      "[229,   300] train_loss: 0.00208\n",
      "[229,   400] train_loss: 0.00206\n",
      "[229,   500] train_loss: 0.00216\n",
      "229 0.002065887215473631\n",
      "229 0.0020315305384759376\n",
      "epoch 229: improvement = -3.1496879348549486e-07\n",
      "[230,   100] train_loss: 0.00205\n",
      "[230,   200] train_loss: 0.00212\n",
      "[230,   300] train_loss: 0.00234\n",
      "[230,   400] train_loss: 0.00187\n",
      "[230,   500] train_loss: 0.00205\n",
      "230 0.0020741632204940943\n",
      "230 0.0020358703568213008\n",
      "epoch 230: improvement = -4.654787138848662e-06\n",
      "[231,   100] train_loss: 0.00215\n",
      "[231,   200] train_loss: 0.00213\n",
      "[231,   300] train_loss: 0.00205\n",
      "[231,   400] train_loss: 0.00193\n",
      "[231,   500] train_loss: 0.00211\n",
      "231 0.0020631872668693913\n",
      "231 0.0020347526801192755\n",
      "epoch 231: improvement = -3.537110436823386e-06\n",
      "[232,   100] train_loss: 0.00205\n",
      "[232,   200] train_loss: 0.00221\n",
      "[232,   300] train_loss: 0.00197\n",
      "[232,   400] train_loss: 0.00206\n",
      "[232,   500] train_loss: 0.00207\n",
      "232 0.0020648399057709666\n",
      "232 0.0020311280945504735\n",
      "epoch 232: improvement = 8.747513197864334e-08\n",
      "[233,   100] train_loss: 0.00197\n",
      "[233,   200] train_loss: 0.00216\n",
      "[233,   300] train_loss: 0.00205\n",
      "[233,   400] train_loss: 0.00218\n",
      "[233,   500] train_loss: 0.00193\n",
      "233 0.0020652816006267974\n",
      "233 0.0020236115992387795\n",
      "epoch 233: improvement = 7.51649531169395e-06\n",
      "[234,   100] train_loss: 0.00188\n",
      "[234,   200] train_loss: 0.00232\n",
      "[234,   300] train_loss: 0.00200\n",
      "[234,   400] train_loss: 0.00197\n",
      "[234,   500] train_loss: 0.00220\n",
      "234 0.0020636812440106666\n",
      "234 0.00203219234805939\n",
      "epoch 234: improvement = -8.58074882061034e-06\n",
      "[235,   100] train_loss: 0.00194\n",
      "[235,   200] train_loss: 0.00212\n",
      "[235,   300] train_loss: 0.00201\n",
      "[235,   400] train_loss: 0.00219\n",
      "[235,   500] train_loss: 0.00209\n",
      "235 0.002059694051262205\n",
      "235 0.0020299478909655143\n",
      "epoch 235: improvement = -6.336291726734798e-06\n",
      "[236,   100] train_loss: 0.00192\n",
      "[236,   200] train_loss: 0.00196\n",
      "[236,   300] train_loss: 0.00224\n",
      "[236,   400] train_loss: 0.00206\n",
      "[236,   500] train_loss: 0.00208\n",
      "236 0.0020584011968541385\n",
      "236 0.002035773925532672\n",
      "epoch 236: improvement = -1.2162326293892536e-05\n",
      "[237,   100] train_loss: 0.00198\n",
      "[237,   200] train_loss: 0.00204\n",
      "[237,   300] train_loss: 0.00230\n",
      "[237,   400] train_loss: 0.00207\n",
      "[237,   500] train_loss: 0.00194\n",
      "237 0.002058775639412697\n",
      "237 0.0020400265263552497\n",
      "epoch 237: improvement = -1.6414927116470152e-05\n",
      "[238,   100] train_loss: 0.00208\n",
      "[238,   200] train_loss: 0.00218\n",
      "[238,   300] train_loss: 0.00201\n",
      "[238,   400] train_loss: 0.00185\n",
      "[238,   500] train_loss: 0.00221\n",
      "238 0.0020587251004816724\n",
      "238 0.002028055052813746\n",
      "epoch 238: improvement = -4.443453574966456e-06\n",
      "[239,   100] train_loss: 0.00190\n",
      "[239,   200] train_loss: 0.00203\n",
      "[239,   300] train_loss: 0.00198\n",
      "[239,   400] train_loss: 0.00230\n",
      "[239,   500] train_loss: 0.00191\n",
      "239 0.002054522012293877\n",
      "239 0.002017951005144215\n",
      "epoch 239: improvement = 5.6605940945645555e-06\n",
      "[240,   100] train_loss: 0.00193\n",
      "[240,   200] train_loss: 0.00220\n",
      "[240,   300] train_loss: 0.00185\n",
      "[240,   400] train_loss: 0.00204\n",
      "[240,   500] train_loss: 0.00217\n",
      "240 0.002052421085343576\n",
      "240 0.002032317661238971\n",
      "epoch 240: improvement = -1.4366656094756246e-05\n",
      "[241,   100] train_loss: 0.00208\n",
      "[241,   200] train_loss: 0.00184\n",
      "[241,   300] train_loss: 0.00208\n",
      "[241,   400] train_loss: 0.00195\n",
      "[241,   500] train_loss: 0.00222\n",
      "241 0.0020502405818327127\n",
      "241 0.002012478124296718\n",
      "epoch 241: improvement = 5.472880847496794e-06\n",
      "[242,   100] train_loss: 0.00208\n",
      "[242,   200] train_loss: 0.00206\n",
      "[242,   300] train_loss: 0.00207\n",
      "[242,   400] train_loss: 0.00202\n",
      "[242,   500] train_loss: 0.00210\n",
      "242 0.0020487095575389244\n",
      "242 0.0020147290885589838\n",
      "epoch 242: improvement = -2.2509642622656027e-06\n",
      "[243,   100] train_loss: 0.00215\n",
      "[243,   200] train_loss: 0.00215\n",
      "[243,   300] train_loss: 0.00204\n",
      "[243,   400] train_loss: 0.00213\n",
      "[243,   500] train_loss: 0.00187\n",
      "243 0.0020539196040987493\n",
      "243 0.0020647657660116396\n",
      "epoch 243: improvement = -5.228764171492147e-05\n",
      "[244,   100] train_loss: 0.00202\n",
      "[244,   200] train_loss: 0.00198\n",
      "[244,   300] train_loss: 0.00210\n",
      "[244,   400] train_loss: 0.00185\n",
      "[244,   500] train_loss: 0.00232\n",
      "244 0.0020483949609123677\n",
      "244 0.002016420419520512\n",
      "epoch 244: improvement = -3.94229522379403e-06\n",
      "[245,   100] train_loss: 0.00207\n",
      "[245,   200] train_loss: 0.00211\n",
      "[245,   300] train_loss: 0.00206\n",
      "[245,   400] train_loss: 0.00199\n",
      "[245,   500] train_loss: 0.00188\n",
      "245 0.002045535322149973\n",
      "245 0.00202893922714418\n",
      "epoch 245: improvement = -1.646110284746205e-05\n",
      "[246,   100] train_loss: 0.00197\n",
      "[246,   200] train_loss: 0.00208\n",
      "[246,   300] train_loss: 0.00215\n",
      "[246,   400] train_loss: 0.00210\n",
      "[246,   500] train_loss: 0.00194\n",
      "246 0.0020437882220508447\n",
      "246 0.0020121716654954917\n",
      "epoch 246: improvement = 3.0645880122644187e-07\n",
      "[247,   100] train_loss: 0.00216\n",
      "[247,   200] train_loss: 0.00197\n",
      "[247,   300] train_loss: 0.00206\n",
      "[247,   400] train_loss: 0.00184\n",
      "[247,   500] train_loss: 0.00208\n",
      "247 0.0020429687897134292\n",
      "247 0.0020066084353284803\n",
      "epoch 247: improvement = 5.563230167011414e-06\n",
      "[248,   100] train_loss: 0.00194\n",
      "[248,   200] train_loss: 0.00205\n",
      "[248,   300] train_loss: 0.00202\n",
      "[248,   400] train_loss: 0.00187\n",
      "[248,   500] train_loss: 0.00229\n",
      "248 0.002042828659396945\n",
      "248 0.002003439996616307\n",
      "epoch 248: improvement = 3.168438712173368e-06\n",
      "[249,   100] train_loss: 0.00218\n",
      "[249,   200] train_loss: 0.00193\n",
      "[249,   300] train_loss: 0.00206\n",
      "[249,   400] train_loss: 0.00206\n",
      "[249,   500] train_loss: 0.00197\n",
      "249 0.002038997714375191\n",
      "249 0.0020013780621875596\n",
      "epoch 249: improvement = 2.061934428747303e-06\n",
      "[250,   100] train_loss: 0.00203\n",
      "[250,   200] train_loss: 0.00184\n",
      "[250,   300] train_loss: 0.00216\n",
      "[250,   400] train_loss: 0.00227\n",
      "[250,   500] train_loss: 0.00200\n",
      "250 0.0020392308493688543\n",
      "250 0.0020033772539115972\n",
      "epoch 250: improvement = -1.999191724037612e-06\n",
      "[251,   100] train_loss: 0.00214\n",
      "[251,   200] train_loss: 0.00215\n",
      "[251,   300] train_loss: 0.00195\n",
      "[251,   400] train_loss: 0.00199\n",
      "[251,   500] train_loss: 0.00205\n",
      "251 0.002035042289424668\n",
      "251 0.0020086704129069315\n",
      "epoch 251: improvement = -7.292350719371836e-06\n",
      "[252,   100] train_loss: 0.00205\n",
      "[252,   200] train_loss: 0.00200\n",
      "[252,   300] train_loss: 0.00197\n",
      "[252,   400] train_loss: 0.00229\n",
      "[252,   500] train_loss: 0.00185\n",
      "252 0.002038676171279338\n",
      "252 0.00200214657049715\n",
      "epoch 252: improvement = -7.685083095905487e-07\n",
      "[253,   100] train_loss: 0.00197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[253,   200] train_loss: 0.00199\n",
      "[253,   300] train_loss: 0.00202\n",
      "[253,   400] train_loss: 0.00202\n",
      "[253,   500] train_loss: 0.00205\n",
      "253 0.0020351078409015103\n",
      "253 0.002036834905894654\n",
      "epoch 253: improvement = -3.545684370709432e-05\n",
      "[254,   100] train_loss: 0.00212\n",
      "[254,   200] train_loss: 0.00217\n",
      "[254,   300] train_loss: 0.00209\n",
      "[254,   400] train_loss: 0.00199\n",
      "[254,   500] train_loss: 0.00195\n",
      "254 0.0020339323537080893\n",
      "254 0.001996941641288037\n",
      "epoch 254: improvement = 4.436420899522724e-06\n",
      "[255,   100] train_loss: 0.00186\n",
      "[255,   200] train_loss: 0.00200\n",
      "[255,   300] train_loss: 0.00203\n",
      "[255,   400] train_loss: 0.00205\n",
      "[255,   500] train_loss: 0.00214\n",
      "255 0.0020339322079164905\n",
      "255 0.0019975257268976783\n",
      "epoch 255: improvement = -5.840856096414257e-07\n",
      "[256,   100] train_loss: 0.00221\n",
      "[256,   200] train_loss: 0.00208\n",
      "[256,   300] train_loss: 0.00210\n",
      "[256,   400] train_loss: 0.00190\n",
      "[256,   500] train_loss: 0.00186\n",
      "256 0.0020289700941403133\n",
      "256 0.0019957039874372376\n",
      "epoch 256: improvement = 1.2376538507993112e-06\n",
      "[257,   100] train_loss: 0.00182\n",
      "[257,   200] train_loss: 0.00217\n",
      "[257,   300] train_loss: 0.00198\n",
      "[257,   400] train_loss: 0.00200\n",
      "[257,   500] train_loss: 0.00220\n",
      "257 0.0020304845419491014\n",
      "257 0.0019971731616903\n",
      "epoch 257: improvement = -1.4691742530625364e-06\n",
      "[258,   100] train_loss: 0.00218\n",
      "[258,   200] train_loss: 0.00187\n",
      "[258,   300] train_loss: 0.00197\n",
      "[258,   400] train_loss: 0.00203\n",
      "[258,   500] train_loss: 0.00189\n",
      "258 0.002031531546900666\n",
      "258 0.0019973964696577296\n",
      "epoch 258: improvement = -1.6924822204920226e-06\n",
      "[259,   100] train_loss: 0.00194\n",
      "[259,   200] train_loss: 0.00209\n",
      "[259,   300] train_loss: 0.00206\n",
      "[259,   400] train_loss: 0.00214\n",
      "[259,   500] train_loss: 0.00195\n",
      "259 0.002027231075687046\n",
      "259 0.001997912260935143\n",
      "epoch 259: improvement = -2.2082734979053838e-06\n",
      "[260,   100] train_loss: 0.00203\n",
      "[260,   200] train_loss: 0.00237\n",
      "[260,   300] train_loss: 0.00217\n",
      "[260,   400] train_loss: 0.00188\n",
      "[260,   500] train_loss: 0.00170\n",
      "260 0.0020275170084238132\n",
      "260 0.001993234875527045\n",
      "epoch 260: improvement = 2.4691119101926534e-06\n",
      "[261,   100] train_loss: 0.00213\n",
      "[261,   200] train_loss: 0.00224\n",
      "[261,   300] train_loss: 0.00177\n",
      "[261,   400] train_loss: 0.00189\n",
      "[261,   500] train_loss: 0.00216\n",
      "261 0.0020256067080906587\n",
      "261 0.0019958599221018805\n",
      "epoch 261: improvement = -2.6250465748355913e-06\n",
      "[262,   100] train_loss: 0.00205\n",
      "[262,   200] train_loss: 0.00206\n",
      "[262,   300] train_loss: 0.00206\n",
      "[262,   400] train_loss: 0.00210\n",
      "[262,   500] train_loss: 0.00200\n",
      "262 0.002041698846316945\n",
      "262 0.0019842650826590864\n",
      "epoch 262: improvement = 8.969792867958518e-06\n",
      "[263,   100] train_loss: 0.00189\n",
      "[263,   200] train_loss: 0.00185\n",
      "[263,   300] train_loss: 0.00206\n",
      "[263,   400] train_loss: 0.00205\n",
      "[263,   500] train_loss: 0.00217\n",
      "263 0.002017114808208645\n",
      "263 0.0019881148251197204\n",
      "epoch 263: improvement = -3.8497424606339856e-06\n",
      "[264,   100] train_loss: 0.00216\n",
      "[264,   200] train_loss: 0.00208\n",
      "[264,   300] train_loss: 0.00187\n",
      "[264,   400] train_loss: 0.00209\n",
      "[264,   500] train_loss: 0.00202\n",
      "264 0.0020184519045493053\n",
      "264 0.0019882487997522295\n",
      "epoch 264: improvement = -3.98371709314305e-06\n",
      "[265,   100] train_loss: 0.00212\n",
      "[265,   200] train_loss: 0.00194\n",
      "[265,   300] train_loss: 0.00220\n",
      "[265,   400] train_loss: 0.00188\n",
      "[265,   500] train_loss: 0.00199\n",
      "265 0.00202521498583786\n",
      "265 0.001989620116029424\n",
      "epoch 265: improvement = -5.3550333703375995e-06\n",
      "[266,   100] train_loss: 0.00190\n",
      "[266,   200] train_loss: 0.00200\n",
      "[266,   300] train_loss: 0.00220\n",
      "[266,   400] train_loss: 0.00196\n",
      "[266,   500] train_loss: 0.00194\n",
      "266 0.00201771747755679\n",
      "266 0.0019896048761035186\n",
      "epoch 266: improvement = -5.339793444432157e-06\n",
      "[267,   100] train_loss: 0.00201\n",
      "[267,   200] train_loss: 0.00212\n",
      "[267,   300] train_loss: 0.00199\n",
      "[267,   400] train_loss: 0.00200\n",
      "[267,   500] train_loss: 0.00206\n",
      "267 0.002018388239398687\n",
      "267 0.001984258474525988\n",
      "epoch 267: improvement = 6.608133098443686e-09\n",
      "[268,   100] train_loss: 0.00195\n",
      "[268,   200] train_loss: 0.00205\n",
      "[268,   300] train_loss: 0.00225\n",
      "[268,   400] train_loss: 0.00188\n",
      "[268,   500] train_loss: 0.00207\n",
      "268 0.0020313118669582145\n",
      "268 0.0019830928971077168\n",
      "epoch 268: improvement = 1.1655774182712034e-06\n",
      "[269,   100] train_loss: 0.00204\n",
      "[269,   200] train_loss: 0.00202\n",
      "[269,   300] train_loss: 0.00202\n",
      "[269,   400] train_loss: 0.00198\n",
      "[269,   500] train_loss: 0.00196\n",
      "269 0.0020135238108194794\n",
      "269 0.0019911246652149243\n",
      "epoch 269: improvement = -8.031768107207536e-06\n",
      "[270,   100] train_loss: 0.00200\n",
      "[270,   200] train_loss: 0.00203\n",
      "[270,   300] train_loss: 0.00208\n",
      "[270,   400] train_loss: 0.00174\n",
      "[270,   500] train_loss: 0.00216\n",
      "270 0.002016349700705368\n",
      "270 0.0019788948115908368\n",
      "epoch 270: improvement = 4.198085516879998e-06\n",
      "[271,   100] train_loss: 0.00195\n",
      "[271,   200] train_loss: 0.00201\n",
      "[271,   300] train_loss: 0.00202\n",
      "[271,   400] train_loss: 0.00212\n",
      "[271,   500] train_loss: 0.00187\n",
      "271 0.0020132927638205336\n",
      "271 0.001978741716898904\n",
      "epoch 271: improvement = 1.5309469193281258e-07\n",
      "[272,   100] train_loss: 0.00199\n",
      "[272,   200] train_loss: 0.00208\n",
      "[272,   300] train_loss: 0.00213\n",
      "[272,   400] train_loss: 0.00195\n",
      "[272,   500] train_loss: 0.00180\n",
      "272 0.0020112825055037674\n",
      "272 0.0019780108639927863\n",
      "epoch 272: improvement = 7.308529061176954e-07\n",
      "[273,   100] train_loss: 0.00196\n",
      "[273,   200] train_loss: 0.00213\n",
      "[273,   300] train_loss: 0.00188\n",
      "[273,   400] train_loss: 0.00201\n",
      "[273,   500] train_loss: 0.00219\n",
      "273 0.0020122614894966056\n",
      "273 0.00197489477590572\n",
      "epoch 273: improvement = 3.1160880870663432e-06\n",
      "[274,   100] train_loss: 0.00210\n",
      "[274,   200] train_loss: 0.00188\n",
      "[274,   300] train_loss: 0.00175\n",
      "[274,   400] train_loss: 0.00228\n",
      "[274,   500] train_loss: 0.00212\n",
      "274 0.002017425624907906\n",
      "274 0.0019738683152951035\n",
      "epoch 274: improvement = 1.0264606106163802e-06\n",
      "[275,   100] train_loss: 0.00189\n",
      "[275,   200] train_loss: 0.00201\n",
      "[275,   300] train_loss: 0.00195\n",
      "[275,   400] train_loss: 0.00221\n",
      "[275,   500] train_loss: 0.00195\n",
      "275 0.002010104100910503\n",
      "275 0.001996901712094152\n",
      "epoch 275: improvement = -2.3033396799048474e-05\n",
      "[276,   100] train_loss: 0.00194\n",
      "[276,   200] train_loss: 0.00192\n",
      "[276,   300] train_loss: 0.00222\n",
      "[276,   400] train_loss: 0.00206\n",
      "[276,   500] train_loss: 0.00205\n",
      "276 0.0020104209270958105\n",
      "276 0.0019887126411294153\n",
      "epoch 276: improvement = -1.4844325834311757e-05\n",
      "[277,   100] train_loss: 0.00230\n",
      "[277,   200] train_loss: 0.00200\n",
      "[277,   300] train_loss: 0.00197\n",
      "[277,   400] train_loss: 0.00205\n",
      "[277,   500] train_loss: 0.00185\n",
      "277 0.002005878757271802\n",
      "277 0.001969618487849123\n",
      "epoch 277: improvement = 4.249827445980663e-06\n",
      "[278,   100] train_loss: 0.00195\n",
      "[278,   200] train_loss: 0.00202\n",
      "[278,   300] train_loss: 0.00222\n",
      "[278,   400] train_loss: 0.00184\n",
      "[278,   500] train_loss: 0.00215\n",
      "278 0.0020132803329552403\n",
      "278 0.0019717323676675977\n",
      "epoch 278: improvement = -2.113879818474787e-06\n",
      "[279,   100] train_loss: 0.00182\n",
      "[279,   200] train_loss: 0.00179\n",
      "[279,   300] train_loss: 0.00220\n",
      "[279,   400] train_loss: 0.00214\n",
      "[279,   500] train_loss: 0.00209\n",
      "279 0.0020031870712489876\n",
      "279 0.001970611170455966\n",
      "epoch 279: improvement = -9.926826068430185e-07\n",
      "[280,   100] train_loss: 0.00204\n",
      "[280,   200] train_loss: 0.00201\n",
      "[280,   300] train_loss: 0.00193\n",
      "[280,   400] train_loss: 0.00210\n",
      "[280,   500] train_loss: 0.00201\n",
      "280 0.0020036602848360822\n",
      "280 0.0019832044261909702\n",
      "epoch 280: improvement = -1.3585938341847351e-05\n",
      "[281,   100] train_loss: 0.00190\n",
      "[281,   200] train_loss: 0.00191\n",
      "[281,   300] train_loss: 0.00207\n",
      "[281,   400] train_loss: 0.00216\n",
      "[281,   500] train_loss: 0.00198\n",
      "281 0.002007753857980595\n",
      "281 0.0020154560764041657\n",
      "epoch 281: improvement = -4.583758855504277e-05\n",
      "[282,   100] train_loss: 0.00197\n",
      "[282,   200] train_loss: 0.00191\n",
      "[282,   300] train_loss: 0.00220\n",
      "[282,   400] train_loss: 0.00190\n",
      "[282,   500] train_loss: 0.00209\n",
      "282 0.0020018591809407185\n",
      "282 0.0019682507030910922\n",
      "epoch 282: improvement = 1.367784758030642e-06\n",
      "[283,   100] train_loss: 0.00195\n",
      "[283,   200] train_loss: 0.00206\n",
      "[283,   300] train_loss: 0.00218\n",
      "[283,   400] train_loss: 0.00186\n",
      "[283,   500] train_loss: 0.00206\n",
      "283 0.002002678902645569\n",
      "283 0.0019768664556767896\n",
      "epoch 283: improvement = -8.615752585697332e-06\n",
      "[284,   100] train_loss: 0.00185\n",
      "[284,   200] train_loss: 0.00218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[284,   300] train_loss: 0.00195\n",
      "[284,   400] train_loss: 0.00209\n",
      "[284,   500] train_loss: 0.00192\n",
      "284 0.0019998742748119297\n",
      "284 0.001968308494540603\n",
      "epoch 284: improvement = -5.779144951088486e-08\n",
      "[285,   100] train_loss: 0.00183\n",
      "[285,   200] train_loss: 0.00206\n",
      "[285,   300] train_loss: 0.00208\n",
      "[285,   400] train_loss: 0.00206\n",
      "[285,   500] train_loss: 0.00197\n",
      "285 0.0019991744212141463\n",
      "285 0.002016459176759742\n",
      "epoch 285: improvement = -4.820847366864955e-05\n",
      "[286,   100] train_loss: 0.00214\n",
      "[286,   200] train_loss: 0.00196\n",
      "[286,   300] train_loss: 0.00215\n",
      "[286,   400] train_loss: 0.00180\n",
      "[286,   500] train_loss: 0.00195\n",
      "286 0.002001836454386483\n",
      "286 0.0019643387383615108\n",
      "epoch 286: improvement = 3.9119647295814824e-06\n",
      "[287,   100] train_loss: 0.00190\n",
      "[287,   200] train_loss: 0.00198\n",
      "[287,   300] train_loss: 0.00199\n",
      "[287,   400] train_loss: 0.00197\n",
      "[287,   500] train_loss: 0.00205\n",
      "287 0.0019972942201005204\n",
      "287 0.001962645071036786\n",
      "epoch 287: improvement = 1.6936673247246528e-06\n",
      "[288,   100] train_loss: 0.00189\n",
      "[288,   200] train_loss: 0.00203\n",
      "[288,   300] train_loss: 0.00205\n",
      "[288,   400] train_loss: 0.00195\n",
      "[288,   500] train_loss: 0.00210\n",
      "288 0.001998373360856635\n",
      "288 0.001961860035094256\n",
      "epoch 288: improvement = 7.850359425303106e-07\n",
      "[289,   100] train_loss: 0.00200\n",
      "[289,   200] train_loss: 0.00232\n",
      "[289,   300] train_loss: 0.00190\n",
      "[289,   400] train_loss: 0.00191\n",
      "[289,   500] train_loss: 0.00189\n",
      "289 0.002000606709507145\n",
      "289 0.0019675139241242014\n",
      "epoch 289: improvement = -5.653889029945576e-06\n",
      "[290,   100] train_loss: 0.00199\n",
      "[290,   200] train_loss: 0.00215\n",
      "[290,   300] train_loss: 0.00206\n",
      "[290,   400] train_loss: 0.00184\n",
      "[290,   500] train_loss: 0.00187\n",
      "290 0.001999472033922783\n",
      "290 0.001967673929879338\n",
      "epoch 290: improvement = -5.813894785082312e-06\n",
      "[291,   100] train_loss: 0.00196\n",
      "[291,   200] train_loss: 0.00210\n",
      "[291,   300] train_loss: 0.00190\n",
      "[291,   400] train_loss: 0.00194\n",
      "[291,   500] train_loss: 0.00211\n",
      "291 0.0019940968399444177\n",
      "291 0.0019660965896621664\n",
      "epoch 291: improvement = -4.2365545679106005e-06\n",
      "[292,   100] train_loss: 0.00214\n",
      "[292,   200] train_loss: 0.00196\n",
      "[292,   300] train_loss: 0.00200\n",
      "[292,   400] train_loss: 0.00200\n",
      "[292,   500] train_loss: 0.00187\n",
      "292 0.0019975992070459282\n",
      "292 0.001990380274192712\n",
      "epoch 292: improvement = -2.8520239098456072e-05\n",
      "[293,   100] train_loss: 0.00194\n",
      "[293,   200] train_loss: 0.00198\n",
      "[293,   300] train_loss: 0.00199\n",
      "[293,   400] train_loss: 0.00200\n",
      "[293,   500] train_loss: 0.00193\n",
      "293 0.001994343672135985\n",
      "293 0.0019677522982181604\n",
      "epoch 293: improvement = -5.892263123904572e-06\n",
      "[294,   100] train_loss: 0.00194\n",
      "[294,   200] train_loss: 0.00221\n",
      "[294,   300] train_loss: 0.00191\n",
      "[294,   400] train_loss: 0.00207\n",
      "[294,   500] train_loss: 0.00185\n",
      "294 0.0019935603332714115\n",
      "294 0.0019858790606036777\n",
      "epoch 294: improvement = -2.4019025509421955e-05\n",
      "[295,   100] train_loss: 0.00210\n",
      "[295,   200] train_loss: 0.00203\n",
      "[295,   300] train_loss: 0.00182\n",
      "[295,   400] train_loss: 0.00202\n",
      "[295,   500] train_loss: 0.00203\n",
      "295 0.001992120452346501\n",
      "295 0.0019639945938874225\n",
      "epoch 295: improvement = -2.1345587931666764e-06\n",
      "[296,   100] train_loss: 0.00189\n",
      "[296,   200] train_loss: 0.00195\n",
      "[296,   300] train_loss: 0.00215\n",
      "[296,   400] train_loss: 0.00206\n",
      "[296,   500] train_loss: 0.00206\n",
      "296 0.001990701330648252\n",
      "296 0.0019515584789917986\n",
      "epoch 296: improvement = 1.0301556102457156e-05\n",
      "[297,   100] train_loss: 0.00185\n",
      "[297,   200] train_loss: 0.00199\n",
      "[297,   300] train_loss: 0.00232\n",
      "[297,   400] train_loss: 0.00189\n",
      "[297,   500] train_loss: 0.00196\n",
      "297 0.001989362509648733\n",
      "297 0.0019725416194694167\n",
      "epoch 297: improvement = -2.0983140477618062e-05\n",
      "[298,   100] train_loss: 0.00216\n",
      "[298,   200] train_loss: 0.00200\n",
      "[298,   300] train_loss: 0.00182\n",
      "[298,   400] train_loss: 0.00187\n",
      "[298,   500] train_loss: 0.00198\n",
      "298 0.001994894367054531\n",
      "298 0.0019551797295237696\n",
      "epoch 298: improvement = -3.621250531970913e-06\n",
      "[299,   100] train_loss: 0.00191\n",
      "[299,   200] train_loss: 0.00198\n",
      "[299,   300] train_loss: 0.00211\n",
      "[299,   400] train_loss: 0.00194\n",
      "[299,   500] train_loss: 0.00201\n",
      "299 0.001987436844437212\n",
      "299 0.0019577120244513926\n",
      "epoch 299: improvement = -6.153545459594003e-06\n",
      "[300,   100] train_loss: 0.00217\n",
      "[300,   200] train_loss: 0.00207\n",
      "[300,   300] train_loss: 0.00195\n",
      "[300,   400] train_loss: 0.00185\n",
      "[300,   500] train_loss: 0.00205\n",
      "300 0.001987425687258882\n",
      "300 0.001959253030723431\n",
      "epoch 300: improvement = -7.694551731632316e-06\n",
      "[301,   100] train_loss: 0.00223\n",
      "[301,   200] train_loss: 0.00176\n",
      "[301,   300] train_loss: 0.00203\n",
      "[301,   400] train_loss: 0.00208\n",
      "[301,   500] train_loss: 0.00175\n",
      "301 0.001993099238969973\n",
      "301 0.001955331867723762\n",
      "epoch 301: improvement = -3.7733887319633865e-06\n",
      "[302,   100] train_loss: 0.00203\n",
      "[302,   200] train_loss: 0.00188\n",
      "[302,   300] train_loss: 0.00206\n",
      "[302,   400] train_loss: 0.00211\n",
      "[302,   500] train_loss: 0.00187\n",
      "302 0.0019847035843191855\n",
      "302 0.001962693386069336\n",
      "epoch 302: improvement = -1.1134907077537315e-05\n",
      "[303,   100] train_loss: 0.00214\n",
      "[303,   200] train_loss: 0.00185\n",
      "[303,   300] train_loss: 0.00199\n",
      "[303,   400] train_loss: 0.00186\n",
      "[303,   500] train_loss: 0.00211\n",
      "303 0.0019866292197379307\n",
      "303 0.001953820004054533\n",
      "epoch 303: improvement = -2.2615250627343233e-06\n",
      "[304,   100] train_loss: 0.00195\n",
      "[304,   200] train_loss: 0.00185\n",
      "[304,   300] train_loss: 0.00183\n",
      "[304,   400] train_loss: 0.00208\n",
      "[304,   500] train_loss: 0.00223\n",
      "304 0.0019839961505044813\n",
      "304 0.001973646943678678\n",
      "epoch 304: improvement = -2.208846468687944e-05\n",
      "[305,   100] train_loss: 0.00187\n",
      "[305,   200] train_loss: 0.00206\n",
      "[305,   300] train_loss: 0.00199\n",
      "[305,   400] train_loss: 0.00205\n",
      "[305,   500] train_loss: 0.00196\n",
      "305 0.0019848064201110362\n",
      "305 0.0019612353271957245\n",
      "epoch 305: improvement = -9.676848203925845e-06\n",
      "[306,   100] train_loss: 0.00194\n",
      "[306,   200] train_loss: 0.00196\n",
      "[306,   300] train_loss: 0.00198\n",
      "[306,   400] train_loss: 0.00197\n",
      "[306,   500] train_loss: 0.00216\n",
      "306 0.0019830990603052928\n",
      "306 0.0019492790627401604\n",
      "epoch 306: improvement = 2.279416251638189e-06\n",
      "[307,   100] train_loss: 0.00206\n",
      "[307,   200] train_loss: 0.00218\n",
      "[307,   300] train_loss: 0.00181\n",
      "[307,   400] train_loss: 0.00207\n",
      "[307,   500] train_loss: 0.00166\n",
      "307 0.0019807746546139236\n",
      "307 0.001976011760433496\n",
      "epoch 307: improvement = -2.673269769333541e-05\n",
      "[308,   100] train_loss: 0.00215\n",
      "[308,   200] train_loss: 0.00204\n",
      "[308,   300] train_loss: 0.00210\n",
      "[308,   400] train_loss: 0.00177\n",
      "[308,   500] train_loss: 0.00193\n",
      "308 0.0019845202714209955\n",
      "308 0.0019545614428872825\n",
      "epoch 308: improvement = -5.282380147122014e-06\n",
      "[309,   100] train_loss: 0.00172\n",
      "[309,   200] train_loss: 0.00213\n",
      "[309,   300] train_loss: 0.00212\n",
      "[309,   400] train_loss: 0.00195\n",
      "[309,   500] train_loss: 0.00205\n",
      "309 0.001979549310059938\n",
      "309 0.0019523192883331459\n",
      "epoch 309: improvement = -3.0402255929854252e-06\n",
      "[310,   100] train_loss: 0.00209\n",
      "[310,   200] train_loss: 0.00185\n",
      "[310,   300] train_loss: 0.00198\n",
      "[310,   400] train_loss: 0.00213\n",
      "[310,   500] train_loss: 0.00194\n",
      "310 0.001982065336166692\n",
      "310 0.0019520675751993496\n",
      "epoch 310: improvement = -2.788512459189112e-06\n",
      "[311,   100] train_loss: 0.00190\n",
      "[311,   200] train_loss: 0.00207\n",
      "[311,   300] train_loss: 0.00197\n",
      "[311,   400] train_loss: 0.00192\n",
      "[311,   500] train_loss: 0.00213\n",
      "311 0.0019778900512375766\n",
      "311 0.001951920551971052\n",
      "epoch 311: improvement = -2.6414892308915337e-06\n",
      "[312,   100] train_loss: 0.00200\n",
      "[312,   200] train_loss: 0.00189\n",
      "[312,   300] train_loss: 0.00195\n",
      "[312,   400] train_loss: 0.00204\n",
      "[312,   500] train_loss: 0.00198\n",
      "312 0.0019773047184500664\n",
      "312 0.0019520642849398355\n",
      "epoch 312: improvement = -2.7852221996750495e-06\n",
      "[313,   100] train_loss: 0.00204\n",
      "[313,   200] train_loss: 0.00191\n",
      "[313,   300] train_loss: 0.00197\n",
      "[313,   400] train_loss: 0.00224\n",
      "[313,   500] train_loss: 0.00188\n",
      "313 0.0019763535576813713\n",
      "313 0.0019482461622638326\n",
      "epoch 313: improvement = 1.0329004763278207e-06\n",
      "[314,   100] train_loss: 0.00217\n",
      "[314,   200] train_loss: 0.00190\n",
      "[314,   300] train_loss: 0.00193\n",
      "[314,   400] train_loss: 0.00185\n",
      "[314,   500] train_loss: 0.00205\n",
      "314 0.0019772608708117432\n",
      "314 0.001945951247635381\n",
      "epoch 314: improvement = 2.294914628451659e-06\n",
      "[315,   100] train_loss: 0.00207\n",
      "[315,   200] train_loss: 0.00190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[315,   300] train_loss: 0.00193\n",
      "[315,   400] train_loss: 0.00224\n",
      "[315,   500] train_loss: 0.00176\n",
      "315 0.0019765266893116934\n",
      "315 0.0019595436713165234\n",
      "epoch 315: improvement = -1.3592423681142458e-05\n",
      "[316,   100] train_loss: 0.00180\n",
      "[316,   200] train_loss: 0.00184\n",
      "[316,   300] train_loss: 0.00221\n",
      "[316,   400] train_loss: 0.00204\n",
      "[316,   500] train_loss: 0.00206\n",
      "316 0.0019849319637004594\n",
      "316 0.0019676219153461775\n",
      "epoch 316: improvement = -2.1670667710796485e-05\n",
      "[317,   100] train_loss: 0.00198\n",
      "[317,   200] train_loss: 0.00194\n",
      "[317,   300] train_loss: 0.00196\n",
      "[317,   400] train_loss: 0.00205\n",
      "[317,   500] train_loss: 0.00193\n",
      "317 0.0019721602803268617\n",
      "317 0.0019525221688316474\n",
      "epoch 317: improvement = -6.570921196266417e-06\n",
      "[318,   100] train_loss: 0.00216\n",
      "[318,   200] train_loss: 0.00198\n",
      "[318,   300] train_loss: 0.00178\n",
      "[318,   400] train_loss: 0.00188\n",
      "[318,   500] train_loss: 0.00188\n",
      "318 0.001973181778807377\n",
      "318 0.0019719626549180103\n",
      "epoch 318: improvement = -2.6011407282629326e-05\n",
      "[319,   100] train_loss: 0.00195\n",
      "[319,   200] train_loss: 0.00218\n",
      "[319,   300] train_loss: 0.00205\n",
      "[319,   400] train_loss: 0.00194\n",
      "[319,   500] train_loss: 0.00171\n",
      "319 0.0019754000367271313\n",
      "319 0.0019823250868926247\n",
      "epoch 319: improvement = -3.63738392572437e-05\n",
      "[320,   100] train_loss: 0.00191\n",
      "[320,   200] train_loss: 0.00202\n",
      "[320,   300] train_loss: 0.00176\n",
      "[320,   400] train_loss: 0.00196\n",
      "[320,   500] train_loss: 0.00226\n",
      "320 0.0019724148189277217\n",
      "320 0.0019476298196018493\n",
      "epoch 320: improvement = -1.6785719664682945e-06\n",
      "[321,   100] train_loss: 0.00203\n",
      "[321,   200] train_loss: 0.00198\n",
      "[321,   300] train_loss: 0.00208\n",
      "[321,   400] train_loss: 0.00195\n",
      "[321,   500] train_loss: 0.00203\n",
      "321 0.001976453937261359\n",
      "321 0.0019422278483978512\n",
      "epoch 321: improvement = 3.7233992375297186e-06\n",
      "[322,   100] train_loss: 0.00194\n",
      "[322,   200] train_loss: 0.00206\n",
      "[322,   300] train_loss: 0.00201\n",
      "[322,   400] train_loss: 0.00193\n",
      "[322,   500] train_loss: 0.00194\n",
      "322 0.00197024379347937\n",
      "322 0.0019527832920252669\n",
      "epoch 322: improvement = -1.0555443627415602e-05\n",
      "[323,   100] train_loss: 0.00188\n",
      "[323,   200] train_loss: 0.00191\n",
      "[323,   300] train_loss: 0.00180\n",
      "[323,   400] train_loss: 0.00208\n",
      "[323,   500] train_loss: 0.00209\n",
      "323 0.0019729397777393807\n",
      "323 0.0019549826489890453\n",
      "epoch 323: improvement = -1.2754800591194054e-05\n",
      "[324,   100] train_loss: 0.00211\n",
      "[324,   200] train_loss: 0.00205\n",
      "[324,   300] train_loss: 0.00178\n",
      "[324,   400] train_loss: 0.00176\n",
      "[324,   500] train_loss: 0.00209\n",
      "324 0.00197182951540733\n",
      "324 0.0019452089855951102\n",
      "epoch 324: improvement = -2.9811371972589696e-06\n",
      "[325,   100] train_loss: 0.00176\n",
      "[325,   200] train_loss: 0.00216\n",
      "[325,   300] train_loss: 0.00195\n",
      "[325,   400] train_loss: 0.00195\n",
      "[325,   500] train_loss: 0.00212\n",
      "325 0.001968356708164576\n",
      "325 0.0019475461612997011\n",
      "epoch 325: improvement = -5.3183129018498625e-06\n",
      "[326,   100] train_loss: 0.00191\n"
     ]
    }
   ],
   "source": [
    "# Start training and saving the start time\n",
    "start_time = datetime.datetime.now()\n",
    "print(\"starting training at: \" + str(start_time))\n",
    "\n",
    "# initialize the ESPCN\n",
    "net = Net(r, C)\n",
    "net.double()\n",
    "\n",
    "if use_gpu:\n",
    "    net = net.cuda()\n",
    "\n",
    "# define loss fuction\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=adam_learning_rate) #adam optimizer, less sensitive to hyperparams\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "epoch = 0\n",
    "last_epoch_loss_test = float(\"inf\")\n",
    "last_epoch_loss_train = float(\"inf\")\n",
    "ni_counter = 0  # counts the amount of epochs no where no improvement has been made\n",
    "\n",
    "# creating a new folder for saving the models and losses\n",
    "dt_string = start_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "models_folder = \"models\"\n",
    "model_name = \"{}_espcnn_r{}\".format(dt_string, r)\n",
    "\n",
    "try:\n",
    "    os.mkdir(models_folder + '/' + model_name)\n",
    "except:\n",
    "    print(\"Folder {} already exists, overwritting model data\".format(models_folder + '/' + model_name))\n",
    "model_dest = models_folder + '/' + model_name + \"/model_epoch_\"\n",
    "best_model_dest = models_folder + '/' + model_name + \"/best_model\"\n",
    "\n",
    "\n",
    "\n",
    "best_test_loss = float(\"inf\")  # start with dummy value, keep track of best loss on test dataset\n",
    "best_epoch = 0\n",
    "try:\n",
    "    while True:  # loop over the dataset multiple times\n",
    "        epoch_loss_train = 0.0\n",
    "        running_loss_train = 0.0\n",
    "        for i, data in enumerate(train_dataloader, 0): # loop through the training data\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.double())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            epoch_loss_train += outputs.shape[0] * loss.item()\n",
    "            running_loss_train += loss.item()\n",
    "            if i % logging_interval == logging_interval - 1:\n",
    "                print('[%d, %5d] train_loss: %.5f' %\n",
    "                      (epoch + 1, i + 1, running_loss_train / logging_interval))\n",
    "                running_loss_train = 0.0\n",
    "        epoch_loss_train = epoch_loss_train / len(train_dataloader.dataset)\n",
    "        print(epoch + 1, epoch_loss_train)\n",
    "\n",
    "        epoch_loss_test = 0.0\n",
    "        running_loss_test = 0.0\n",
    "        for i, data in enumerate(test_dataloader, 0):  # loop through the test data and calculate the test error\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.double())\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # print statistics\n",
    "            epoch_loss_test += outputs.shape[0] * loss.item()\n",
    "            running_loss_test += loss.item()\n",
    "            if i % logging_interval == logging_interval - 1:\n",
    "                print('[%d, %5d] test_loss: %.5f' %\n",
    "                      (epoch + 1, i + 1, running_loss_test / logging_interval))\n",
    "                running_loss_test = 0.0\n",
    "        epoch_loss_test = epoch_loss_test / len(test_dataloader.dataset)\n",
    "        print(epoch + 1, epoch_loss_test)\n",
    "\n",
    "        # Calculating the improvement\n",
    "        improvement = best_test_loss - epoch_loss_test\n",
    "\n",
    "        if epoch_loss_test < best_test_loss:  # save best model, 'best' meaning lowest loss on test set\n",
    "            best_test_loss = epoch_loss_test\n",
    "            torch.save(net.state_dict(),\n",
    "                       best_model_dest)  # overwrite best model so the best model filename doesn't change\n",
    "            best_epoch = epoch\n",
    "            best_epoch_train_loss = epoch_loss_train\n",
    "\n",
    "        # stop training if no improvement has been made for 'repeats' epochs\n",
    "        print(\"epoch \" + str(epoch + 1) + \": improvement = \" + str(improvement))\n",
    "        if improvement < no_learning_threshold:\n",
    "            ni_counter += 1\n",
    "        else:\n",
    "            ni_counter = 0\n",
    "\n",
    "        if ni_counter >= repeats:\n",
    "            break\n",
    "\n",
    "        # save losses\n",
    "        losses_train.append(epoch_loss_train)\n",
    "        losses_test.append(epoch_loss_test)\n",
    "        last_epoch_loss_train = epoch_loss_train\n",
    "        last_epoch_loss_test = epoch_loss_test\n",
    "\n",
    "        # every 'epch_save_interval' save the current model\n",
    "        if epoch % epoch_save_interval == 0:\n",
    "            torch.save(net.state_dict(), model_dest + str(epoch + 1))\n",
    "        epoch += 1\n",
    "except KeyboardInterrupt: # this allows for manually stopping the training without losing the progress\n",
    "    print(\"Press Ctrl-C to terminate while statement\")\n",
    "    pass\n",
    "\n",
    "    print('Saving train and test loss')\n",
    "    np.save(models_folder + '/' + model_name + '/loss_train', losses_train)\n",
    "    np.save(models_folder + '/' + model_name + '/loss_test', losses_test)\n",
    "\n",
    "# Show training duration\n",
    "end_time = datetime.datetime.now()\n",
    "print('Finished training at: ' + str(end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSNR\n",
    "Define PSNR (Peak Signal to Noise Ratio) in order to validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(original, compressed):\n",
    "    mse = np.mean((original - compressed) ** 2)\n",
    "    if (mse == 0):  # MSE is zero means no noise is present in the signal .\n",
    "        # Therefore PSNR have no importance.\n",
    "        return 100\n",
    "    max_pixel = 255.0\n",
    "    psnr = 20 * math.log10(max_pixel / math.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "\n",
    "def average_PSNR(folder, net, r, gaussianSigma):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = plt.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            img = resize(img, ((img.shape[0] // r) * r, (img.shape[1] // r) * r))\n",
    "            images.append(img)\n",
    "\n",
    "    sumPSNR = 0\n",
    "    for og_img in images:\n",
    "        img_blurred = gaussian(og_img, sigma=gaussianSigma,\n",
    "                               multichannel=True)  # multichannel blurr so that 3rd channel is not blurred\n",
    "        img = resize(img_blurred, (img_blurred.shape[0] // r, img_blurred.shape[1] // r))\n",
    "        if (len(img.shape) == 2):  # convert image to rgb if it is grayscale\n",
    "            img = np.stack((img, img, img), axis=2)\n",
    "            og_img = np.stack((og_img, og_img, og_img), axis=2)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = torch.Tensor(img).unsqueeze(0).double()\n",
    "        result = net(img).detach().numpy()\n",
    "        sumPSNR += PSNR(PS(result[0], r) * 255, og_img * 255)\n",
    "\n",
    "    return sumPSNR / len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging and saving results\n",
    "Printing and saving the settings and results to console and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train and test loss\n",
      "Finished validation \n",
      "\n",
      "dataset:               T91\n",
      "psnr Set5:             22.537537580714048\n",
      "psnr Set14:            21.95499197706312\n",
      "best epoch:            1\n",
      "loss on training set:  0.014531448482027323\n",
      "loss on test set:      0.01198686770245012\n",
      "r:                     3\n",
      "blur:                  0.25\n",
      "lr_start:              0.01\n",
      "lr_end:                0.0001\n",
      "mu:                    1e-06\n",
      "no_learning_threshold: 1e-08\n",
      "epochs:                3\n",
      "training duration:     0:00:47.687587\n",
      "batch_size:            1\n",
      "train_test_fraction:   0.8\n",
      "model:                 2020-04-12_12-55-07_espcnn_r3\n"
     ]
    }
   ],
   "source": [
    "print('Saving train and test loss')\n",
    "np.save(models_folder + '/' + model_name + '/loss_train', losses_train)\n",
    "np.save(models_folder + '/' + model_name + '/loss_test', losses_test)\n",
    "\n",
    "net.load_state_dict(torch.load(best_model_dest))\n",
    "net.eval()\n",
    "\n",
    "net.cpu()\n",
    "set5_PSNR = average_PSNR(\"./datasets/testing/Set5\", net, r, blur)\n",
    "set14_PSNR = average_PSNR(\"./datasets/testing/Set14\", net, r, blur)\n",
    "\n",
    "print(\"Finished validation \\n\")\n",
    "\n",
    "print(\"dataset:               \" + dataset)\n",
    "print(\"psnr Set5:             \" + str(set5_PSNR))\n",
    "print(\"psnr Set14:            \" + str(set14_PSNR))\n",
    "print(\"best epoch:            \" + str(best_epoch))  # epoch with the lowest loss on the test dataset\n",
    "print(\"loss on training set:  \" + str(best_epoch_train_loss))  # loss for the best epoch\n",
    "print(\"loss on test set:      \" + str(best_test_loss))  # loss for the best epoch\n",
    "print(\"r:                     \" + str(r))\n",
    "print(\"blur:                  \" + str(blur))\n",
    "print(\"lr_start:              \" + str(lr_start))\n",
    "print(\"lr_end:                \" + str(lr_end))\n",
    "print(\"mu:                    \" + str(mu))\n",
    "print(\"no_learning_threshold: \" + str(no_learning_threshold))\n",
    "print(\"epochs:                \" + str(epoch + 1))\n",
    "print(\"training duration:     \" + str(end_time - start_time))\n",
    "print(\"batch_size:            \" + str(batch_size))\n",
    "print(\"train_test_fraction:   \" + str(train_test_fraction))\n",
    "print(\"model:                 \" + model_name)\n",
    "\n",
    "with open(models_folder + '/' + model_name + '/results.csv', mode='w') as csv_file:\n",
    "    fieldnames = ['dataset', 'psnr_Set5', 'psnr_Set14', 'best_epoch', 'training_loss', 'test_loss', 'r', 'blur', 'lr_start', 'lr_end', 'mu', 'no_learning_threshold', 'epochs', 'training_duration', 'batch_size', 'train_test_fraction', 'model']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({\n",
    "        'dataset': dataset,\n",
    "        'psnr_Set5': set5_PSNR,\n",
    "        'psnr_Set14': set14_PSNR,\n",
    "        'best_epoch': best_epoch,\n",
    "        'training_loss': best_epoch_train_loss,\n",
    "        'test_loss': best_test_loss,\n",
    "        'r': r,\n",
    "        'blur': blur,\n",
    "        'lr_start': lr_start,\n",
    "        'lr_end': lr_end,\n",
    "        'mu': mu,\n",
    "        'no_learning_threshold': no_learning_threshold,\n",
    "        'epochs': (epoch + 1),\n",
    "        'training_duration': (end_time - start_time),\n",
    "        'batch_size': batch_size,\n",
    "        'train_test_fraction': train_test_fraction,\n",
    "        'model': model_name})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NB69kyUtGzRF"
   },
   "source": [
    "## Resulting images\n",
    "While we were unable to achieve a PSNR similar to the one achieved in the paper we were trying to reproduce, the network did generate some interesting results. Some of these results on the original test set are shown below. From left to right, the images are the **original image**, upscaled using a **bi-cubic filter** and upscaled using **our ESPCN**.\n",
    "![figure 3](rsc/summary.png)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "blogpost.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "#Reproducing ESPCN\n",
     "\n",
     "In this notebook we reproduce 91 generated images from the paper **Real-Time Single Image and Video Super-Resolution \n",
     "Using an Efficient Sub-Pixel Convolutional Neural Network**. The code has been created following the paper with gaps \n",
     "filled in by ourselves.\n",
     "The paper presents a convulutional neural network capable of real-time Super-Resolution (SR).\n",
     "They designed a CNN architecture where the feature maps are extracted in the Low-Resolution(LR) space, \n",
     "and introducing an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale\n",
     "the final LR feature maps into the High-Resolution(HR) output.\n",
     "\n",
     "---\n",
     "### SRCNN\n",
     "Recovers SR output image from an upscaled and interpolated version of LR image. \n",
     "Uses bicubic interpolation (as a special case of the deconvolution layer) to upscale image.\n",
     "Deconvolution layer a.k.a. Transposed convolution layer:\n",
     "_insert image_\n",
     "\n",
     "\n",
     "### ESPCN\n",
     "First apply a  layer CNN directly to LR image, then apply a sub-pixel convolution layer that upscales the LR feature \n",
     "maps to produce the SR output image. (to avoid upscaling LR before feeding it into the network)\n",
     "_insert image_\n",
     "For a network composed of L layers, the first L−1 layers can be described as follows:\n",
     "$f^1(\\boldsymbol{I}^{LR};W_1, b_1)= \\phi (W_1 \\ast \\boldsymbol{I}^{LR}+b_1)$\n",
     "$f^l(\\boldsymbol{I}^{LR};W_{1:l}, b_{1:l})= \\phi (W_1 \\ast f^{l-1}(\\boldsymbol{I}^{LR})+b_l)$\n",
     "Where $W_l , b_l , l \\in (1, L - 1)$ are learnable network weights and biases respectively.  \n",
     "$W_l$ is a 2D convolution tensor of size $n_{l-1} \\times n_l \\times k_l \\times k_l$ , where $n_l$ is the number of \n",
     "features at layer $l$, $n_0 = C$, and $k_l$ is the filter size at layer $l$. \n",
     "The biases $b_l$ are vectors of length $n_l$ . The nonlinearity function (or activation function) $\\phi$ is applied \n",
     "element-wise and is fixed. The last layer $f^L$ has to convert the LR feature maps to a HR image $\\boldsymble{I}^{SR}$.\n",
     "\n",
     "---\n",
     "\n",
     "## Experiment Setup \n",
     "\n",
     "Input = H x W x C\n",
     "Output = rH x rW x C\n",
     "\n",
     "Apply l layer CNN directly to Low Resolution (LR)\n",
     "Apply sub-pixel convolution layer upscaling LR feature maps\n",
     "\n",
     "Each layer except the last:\n",
     "$f^1(\\boldsymbol{I}^{LR};W_1, b_1)= \\phi (W_1 \\ast \\boldsymbol{I}^{LR}+b_1)$\n",
     "$f^l(\\boldsymbol{I}^{LR};W_{1:l}, b_{1:l})= \\phi (W_1 \\ast f^{l-1}(\\boldsymbol{I}^{LR})+b_l)$\n",
     "\n",
     "W & b learnable parameters\n",
     "W is a 2D convolution tensor of size n_(l-1) x n_l  x k_l x k_l\n",
     "    n_l is the amount of features of layer l, n_0 = C, k_l is the filter size at l\n",
     "    biases are of length n_l\n",
     "    activation function is fixed and element-wise\n",
     "\n",
     "Last layer converts LR to HR\n",
     "\n",
     "Efficient sub-pixel convolution layer: (biggest addition)\n",
     "Convolution with stride 1/r over LR with filter W_s of size k_s and weight spacing 1/r\n",
     "Weights between pixels are not activated and do not need to be calculated\n",
     "The number of activation patterns is exactly r^2.\n",
     "Each pattern has at most ceil(k_s/r)^2 weights\n",
     "Patterns are periodically activated during convolution of the filter depending on the subpixel location mod(x, r), mod(y,r), where x and y are the output pixel coordinates in HR.\n",
     "Solution for mod(k_s, r) = 0\n",
     "Last layer: note NO ACTIVATION\n",
     " $$I^{SR} = f^L(I^{LR}) = PS(W_L * f^{L-1}(I^{LR})+b_L)$$\n",
     "PS is a periodic shuffling operator (sort of mapping)\n",
     "_formule naar latex_\n",
     "W_L has shape n_(L-1) x r^2C x k_L x k_L\n",
     "k_L = k_s/r and mod(k_s, r) = 0\n",
     "\n",
     "Loss function: (mean squared error)\n",
     "_formule naar latex_\n",
     "Preshuffle training data avoiding the use of PS\n",
     "\n",
     "---\n",
     "# The model\n",
     "\n",
     "Everything we need is imported\n",
     "_imports block_\n",
     "\n",
     "---\n",
     "\n",
     "hyperparameters to set\n",
     "\n",
     "_hyperparameters block_"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
