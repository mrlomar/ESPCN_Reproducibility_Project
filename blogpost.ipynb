{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlyY-v4OEIDw"
   },
   "source": [
    "# Reproducing 'Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network'\n",
    "\n",
    "In this notebook, we reproduce the fourth column of Table 1 from the paper [**Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network**](https://arxiv.org/pdf/1609.05158v2.pdf). The code has been created following the paper with gaps \n",
    "filled in by ourselves.\n",
    "The paper presents a convolutional neural network capable of real-time Super-Resolution (SR).\n",
    "They designed a CNN architecture where the feature maps are extracted in the Low-Resolution(LR) space \n",
    "and introducing an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale\n",
    "the final LR feature maps into the High-Resolution(HR) output.\n",
    "A researcher would like a reproduction of this to test on his high-res dataset.\n",
    "\n",
    "\n",
    "---\n",
    "### ESPCN\n",
    "For the problem of single image super-resolution (SISR), the authors of this paper set out to improve the network known as SRCN, discussed in [this paper](https://ieeexplore.ieee.org/abstract/document/7115171). SRCNN approaches the problem by increasing the resolution of the LR image before the first layer of the network. The authors argue this is inefficient since the processing speed of convolutional networks directly depends on the resolution of the input image. Therefore they propose a faster network called ESPCN.\n",
    "\n",
    "The new architecture improves this by increasing the resolution from LR to HR only at the very end of the network and super-resolves HR data from LR feature maps, by first applying a multi-layer CNN directly to LR image and then applying a sub-pixel convolution layer that upscales the LR feature maps to produce the SR output image.  Figure 1 (retrieved from [the original paper](https://arxiv.org/pdf/1609.05158v2.pdf)) shows a visualisation of the network, the rest of this section is dedicated to the mathematical description of the network.\n",
    "\n",
    "![figure 1](rsc/ESPCN.PNG)\n",
    "\n",
    "For a network composed of L layers, $n_{L-1}$ upscaling filters for the $n_{L-1}$ feature maps are learned. The first L−1 layers can be described as follows:  \n",
    "  \n",
    "$f^1(\\boldsymbol{I}^{LR};W_1, b_1)= \\phi (W_1 \\ast \\boldsymbol{I}^{LR}+b_1)$  \n",
    "$f^l(\\boldsymbol{I}^{LR};W_{1:l}, b_{1:l})= \\phi (W_1 \\ast f^{l-1}(\\boldsymbol{I}^{LR})+b_l)$  \n",
    "  \n",
    "Where $W_l , b_l , l \\in (1, L - 1)$ are learnable network weights and biases respectively.  \n",
    "  \n",
    "$W_l$ is a 2D convolution tensor of size $n_{l-1} \\times n_l \\times k_l \\times k_l$ , where $n_l$ is the number of \n",
    "features at layer $l$, $n_0 = C$, and $k_l$ is the filter size at layer $l$.  \n",
    "The biases $b_l$ are vectors of length $n_l$ . The nonlinearity function (or activation function) $\\phi$ is applied \n",
    "element-wise and is fixed. The last layer $f^L$ converts the LR feature maps to an HR image $\\boldsymbol{I}^{SR}$.  \n",
    "  \n",
    "The last layer is an efficient sub-pixel convolution layer. The authors propose a convolution with stride $\\frac{1}{r}$ in de LR space with a filter $W_s$ of size $k_s$ with a weight spacing $\\frac{1}{r}$. This way, each activation pattern is periodically activated during the convolution of the filter depending on different sub-pixel locations. A way to implement this can be described in the following way:  \n",
    "$\\boldsymbol{I}^{SR}=f^L (\\boldsymbol{I}^{LR}) = \\mathcal{PS} (W_L \\ast f^{L-1} (\\boldsymbol{I}^{LR}) + b_L)$\n",
    "  \n",
    "Where $\\mathcal{PS}$ is an periodic shuffling operator that rearranges the elements of a $H \\times W \\times C \\cdot r^2$ tensor to a tensor of shape $rH \\times rW \\times C$. \n",
    "The periodic shuffle can be denoted as follows:\n",
    "$\\mathcal{PS} (T)_{x,y,c} = T_{\\lfloor x/r \\rfloor, \\lfloor y/r \\rfloor , C\\cdot r \\cdot mod(y,r) + C\\cdot mod(x,r) + c}$.\n",
    "The convolution operator $W_L$ thus has shape $n_{L - 1} \\times C \\cdot r^2 \\times k_L \\times k_L$. \n",
    "\n",
    "The proposed network can be referred to as an efficient sub-pixel convolutional neural network, also referred to as *ESPCN*. \n",
    "\n",
    "A pixel-wise mean squared error function is used to train the network:\n",
    "$\\ell(W_{1:L}, b_{1:L}) = \\frac{1}{r^2HW}\\sum^{rH}_{x=1}\\sum^{rW}_{x=1} (\\boldsymbol{I}^{HR}_{x,y} - f^L_{x,y}(\\boldsymbol{I}^{LR}))^2$.  \n",
    "  \n",
    "To increase the performance while learning the periodic shuffling at the end can be avoided during training since the labelled training data can be inversely shuffled to match the output of the last layer before it is shuffled, therefore avoiding unnecessary calculations.\n",
    "\n",
    "---\n",
    "\n",
    "## Experiment Setup \n",
    "\n",
    "Reproducing the results consists of two main steps, namely:  \n",
    "1. generating the training data\n",
    "2. training the network on said data  \n",
    "\n",
    "Both of these steps are discussed below individually.\n",
    "\n",
    "### 1. Generating the training data\n",
    "Since the T91 data set only consists of 91 images, the images need to be transformed in various ways to generate a sufficient amount of training data. Three steps are applied to the T91 data set in order to generate the training data.\n",
    "1. A gaussian blur (sigma \\= \\[$0.1$ - $1$\\]) is applied to the original images to simulate optical blur of a low-resolution camera and the images are downscaled by a factor $3$.\n",
    "2. $17$ by $17$ training images are subsampled from the downscaled images, corresponding to $17r$ by $17r$ pixel subsamples of the original HR images\n",
    "3. An inverse periodic shuffle is applied on the $17r$ by $17r$ pixel subsamples of original HR images to obtain training data labels\n",
    "\n",
    "![figure 2](rsc/ss_hr_to_lr.png)\n",
    "\n",
    "### 2. Training the network\n",
    "Once the training data is generated we can begin training. The model is trained to predict the inverse shuffled HR subsamples from the corresponding blurred LR subsamples. To reproduce the original paper as accurately as possible, the exact implementation details from their paper were implemented. However, some of the information necessary for reproducing these results was missing. This missing information and our approach are detailed further in *Discussion*.  \n",
    "\n",
    "### Results\n",
    "From the losses shown in the figure below we can conclude that our network is able to learn on the generated data.\n",
    "\n",
    "![figure 3](rsc/train_and_test_loss.png)\n",
    "\n",
    "After training for approximately 3000 epochs, we achieved the results in the *Our ESPCN* column:\n",
    "\n",
    "| Data Set | Scale | Bi-cubic | SRCNN (91)| Paper ESPCN | Our ESPCN |\n",
    "|----------|-------|----------|-----------|-------------|-----------|\n",
    "|Set5      | 3     | 27.53    | 32.39     | 32.55       | 32.25     |\n",
    "|Set14     | 3     | 25.25    | 29.00     | 29.08       | 28.50     |\n",
    "|BSD300    | 3     | 25.57    | 28.21     | 28.26       | 28.02     |\n",
    "|BSD500    | 3     | 25.54    | 28.28     | 28.34       | 28.08     |\n",
    "\n",
    "The results are similar to the *Paper ESPCN*, but since our model is not an improvement over *SRCNN* we do not feel that we were able to reproduce the results from the paper. The reason why we were unable to reproduce the results and our approach to optimizing our own results are discussed in *Discussion*.\n",
    "\n",
    "---\n",
    "## Discussion\n",
    "We did not manage to reproduce the results from Table 1 from the [original paper](https://arxiv.org/pdf/1609.05158v2.pdf), column ESPCN (91). When trying to recreate the results of the paper, we recreated the model as described in the paper. However, some hyperparameters and architectural decisions were not given. The following information was missing from the paper and made it harder to reproduce the results:\n",
    "* **Optimizer**: It is not stated what optimizer is used when training the network.\n",
    "* **How the learning rate changes from high to low**: It is not stated how the learning rate changes between its start- and end-value. A parameter ‘mu’ is mentioned, where the learning rate decreases with an unknown amount when the improvement is smaller than ‘mu’. However, the value of ‘mu’ is not given.\n",
    "* **Gaussian blur**: To simulate the downscale image when training in a realistic manner, the training images are blurred. However, the intensity of this blur is not given.\n",
    "\n",
    "In order to find optimal values for these parameters, we export the values of these parameters and their respective results to a *.csv* file after each training session. This allowed us to easily run the training multiple times with different parameter sets and compare their respective results. Though we were able to optimize our network by finding better parameters, we were still unable to reproduce the results from the paper.\n",
    "\n",
    "When implementing and training *ESPCN* we encountered a few other parameters that were not necessarily missing from the paper, but would have been beneficial to have more information on, namely:\n",
    "\n",
    "* **Batch size**: The batch size was not given, nor was it clear from the paper whether batches were used at all. This information would have been useful to interpret their training times and compare them to ours.\n",
    "* **Model validation**: The T91 dataset was used for training. However, it was not specified whether this dataset was split up into a train, test and validation set. Was (k-fold) cross-validation used, was some other validation method used or did they train on the full dataset without validation?\n",
    "\n",
    "### Chosen parameters values\n",
    "After experimenting with multiple runs these were the observations we made w.r.t. the impact and optimal values of the missing (hyper)parameters:\n",
    "\n",
    "__Optimizer__: Adam is used instead of SGD as proposed in the paper. For SGD the results obtained with the high-level description in the paper were far below the results in the paper - SGD needs to be fine-tuned so it is important the paper should have mentioned the specific parameters and method. The adam optimizer works in a similar way but the only parameter to be set is the final learning rate which is set to $1e-4$ as in the paper. \n",
    "\n",
    "__Gaussian blur sigma__: values between $0$ and $2$ were tried for the gaussian sigma, with values between $0.25$ and $1$ showing similar performance, a sigma of $1$ achieved the highest performance which is used for our final reproduction model.\n",
    "\n",
    "A __no-learning threshold__ of $1e-7$ is chosen meaning the training stops after $100$ epochs where the max improvement per epoch is below this threshold. Choosing a higher threshold causes the training process to end with suboptimal results and experiments run with a smaller threshold did not finish within a day.\n",
    "\n",
    "A __batch size__ of $10$ was chosen to speed up training. Using lower batch sizes (below $4$) considerably increased the training time and much higher batch sizes (e.g. $50$, $100$) resulted in a higher loss while training.\n",
    "\n",
    "A __train/test split__ is not mentioned by the paper, we tried splits $70$-$30$, $80$-$20$, $90$-$10$ with the $90$-$10$ split giving the best PSNR.\n",
    "\n",
    "---\n",
    "# The model\n",
    "Below the complete code for creating the network and loading the data is discussed. The concepts used are\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xw7N_3dGGdtk"
   },
   "source": [
    "First, all dependencies are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuwZj7xuGAix"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.filters import *\n",
    "from skimage.transform import *\n",
    "import os\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YE5tg_8GkFF"
   },
   "source": [
    "### Hyper parameters\n",
    "Then all hyperparameters and parameters, as well as constants are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmxbwLXHGMwY"
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "C = 3 # amount of colour channels\n",
    "dataset = \"T91\" # the folder containing all images used for training\n",
    "epoch_save_interval = 100 # the interval after which an intermediate save is made\n",
    "logging_interval = 100 # the interval after which we log the losses the the console\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# HYPERPARAMETERS and PARAMETERS\n",
    "r = 3  # upscaling ratio\n",
    "blur = 0.25  # gaussian blur\n",
    "lr_start = 0.01 # initial learning rate\n",
    "lr_end = 0.0001 # final learning rate\n",
    "mu = 1e-6  # threshold for lowering the learning reate\n",
    "no_learning_threshold = 1e-8  # threshold for stopping training when no improvement has been made for 'repeats' epochs\n",
    "repeats = 100 # the number of consecutive epochs without (enough) improvement necessary to stop training \n",
    "batch_size = 1 # size of batches\n",
    "train_test_fraction = 0.8 # the fraction of the data that is used for training, the remainder is used as a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6GNLWCmGnz9"
   },
   "source": [
    "### Periodic Shuffle\n",
    "In the code below the Periodic Shuffle and its inverse are defined as functions to be used when loading in the data.\n",
    "As mentioned before, the Periodic Shuffle is defined as follows:\n",
    "$\\mathcal{PS} (T)_{x,y,c} = T_{\\lfloor x/r \\rfloor, \\lfloor y/r \\rfloor , C\\cdot r \\cdot mod(y,r) + C\\cdot mod(x,r) + c}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PS(T, r):\n",
    "    T = np.transpose(T, (1, 2, 0))\n",
    "    rW = r * len(T)\n",
    "    rH = r * len(T[0])\n",
    "    C = len(T[0][0]) / (r * r)\n",
    "\n",
    "    # make sure C is an integer and cast if this is the case\n",
    "    assert (C == int(C))\n",
    "    C = int(C)\n",
    "\n",
    "    res = np.zeros((rW, rH, C))\n",
    "\n",
    "    for x in range(len(res)):\n",
    "        for y in range(len(res[x])):\n",
    "            for c in range(len(res[x][y])):\n",
    "                res[x][y][c] = \\\n",
    "                    T[x // r][y // r][C * r * (y % r) + C * (x % r) + c]\n",
    "    return res\n",
    "\n",
    "\n",
    "def PS_inv(img, r):\n",
    "    r2 = r * r\n",
    "    W = len(img) / r\n",
    "    H = len(img[0]) / r\n",
    "    C = len(img[0][0])\n",
    "    Cr2 = C * r2\n",
    "\n",
    "    # Make sure H and W are integers\n",
    "    assert (int(H) == H and int(W) == W)\n",
    "    H, W = int(H), int(W)\n",
    "\n",
    "    res = np.zeros((W, H, Cr2))\n",
    "\n",
    "    for x in range(len(img)):\n",
    "        for y in range(len(img[x])):\n",
    "            for c in range(len(img[x][y])):\n",
    "                res[x // r][y // r][C * r * (y % r) + C * (x % r) + c] = img[x][y][c]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "The images from T91 are loaded in, then they are used to create a train and test set. This process is described under *Experiment Setup*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKgQGMS9GM8r"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-befc383934b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchDataloader_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./datasets/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_test_fraction\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Downsample images\n",
    "- gaussian blur\n",
    "- resize by downsample factor (using interpolation)\n",
    "How To Use:\n",
    "    function lr_dataset_from_path takes a path to the dataset of HR image png files and returns an ndarray to use for training the model\n",
    "For debugging/showing examples:\n",
    "    (see bottom of file)\n",
    "    save_png set to True to save resulting lr images in specified directory.\n",
    "    !check the param_ varaiables\n",
    "\"\"\"\n",
    "\n",
    "SUBSAMPLING_STRIDE_SIZE = 14\n",
    "SUBSAMPLING_SAMPLE_SIZE = 17\n",
    "\n",
    "\n",
    "# hr_dataset_path: dir to the hr_dataset png files\n",
    "# downscale: downscale factor, e.g. if original image 64*64 and downscale=2 then result will be 32*32\n",
    "# returns list of numpy.ndarray representing the lr_images\n",
    "def lr_dataset_from_path(hr_dataset_path, downscale):\n",
    "    original_filenames = os.listdir(hr_dataset_path)\n",
    "    original_images = []\n",
    "    for file in original_filenames:\n",
    "        original_images.append(plt.imread(hr_dataset_path + '/' + file))\n",
    "    return lr_images(original_images, downscale)  # ndarray of images\n",
    "\n",
    "\n",
    "def torchDataloader_from_path(hr_dataset_path, downscale, gaussian_sigma, batch_size):\n",
    "    original_filenames = os.listdir(hr_dataset_path)\n",
    "    original_images = []\n",
    "    for file in original_filenames:\n",
    "        original_images.append(plt.imread(hr_dataset_path + '/' + file))\n",
    "\n",
    "    # subsample\n",
    "    subsamples_hr = []\n",
    "    subsamples_hr_rev_shuff = []\n",
    "    for i in range(len(original_images)):\n",
    "        temp_subsamples = subsample(original_images[i], downscale)\n",
    "        subsamples_hr += temp_subsamples\n",
    "        for sample_indx in range(len(temp_subsamples)):\n",
    "            subsamples_hr_rev_shuff.append(PS_inv(temp_subsamples[sample_indx], downscale))  # labels\n",
    "    lr_dataset = lr_images(subsamples_hr, downscale, gaussian_sigma)  # ndarray of images\n",
    "    return toDataloader(lr_dataset, subsamples_hr_rev_shuff, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Takes list of images and provide LR images in form of numpy array\n",
    "def lr_images(images_real, downscale, gaussianSigma):\n",
    "    lr_images = []\n",
    "    for img in range(len(images_real)):\n",
    "        img_blurred = gaussian(images_real[img], sigma=gaussianSigma,\n",
    "                               multichannel=True)  # multichannel blurr so that 3rd channel is not blurred\n",
    "        lr_images.append(resize(img_blurred, (img_blurred.shape[0] // downscale, img_blurred.shape[1] // downscale)))\n",
    "    return lr_images\n",
    "\n",
    "\n",
    "# extract a 17r*17r subsample from original image, no overlap so every pixel appears at most once in output\n",
    "def subsample(image_real, downscale):\n",
    "    subsample_size = SUBSAMPLING_SAMPLE_SIZE * downscale\n",
    "    subsample_stride = SUBSAMPLING_STRIDE_SIZE * downscale\n",
    "    subsamples = []\n",
    "    for y in range(math.floor((image_real.shape[0] - (subsample_size - subsample_stride)) / subsample_stride)):\n",
    "        for x in range(math.floor((image_real.shape[1] - (subsample_size - subsample_stride)) / subsample_stride)):\n",
    "            ss = image_real[(y * subsample_stride):(y * subsample_stride) + subsample_size,\n",
    "                 (x * subsample_stride):(x * subsample_stride) + subsample_size]\n",
    "            subsamples.append(ss)\n",
    "\n",
    "    return subsamples\n",
    "\n",
    "\n",
    "# returns a torch Dataloader (to iterate over training data) using the training data samples and traing data labels\n",
    "def toDataloader(train_data, train_labels, batch_size):\n",
    "    labeled_data = []\n",
    "    for i in range(len(train_data)):\n",
    "        labeled_data.append([np.transpose(train_data[i], (2, 0, 1)), np.transpose(train_labels[i], (2, 0, 1))])\n",
    "    trainDataloader = DataLoader(labeled_data, batch_size=batch_size, shuffle=True)\n",
    "    return trainDataloader\n",
    "\n",
    "# Load the data\n",
    "dataloader = torchDataloader_from_path('./datasets/' + dataset, r, blur, batch_size)\n",
    "train_size = int(train_test_fraction * len(dataloader.dataset))\n",
    "test_size = len(dataloader.dataset) - train_size\n",
    "train_set, test_set = torch.utils.data.random_split(dataloader.dataset, [train_size, test_size])\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "print(\"Data loaded\")\n",
    "\n",
    "# show first image from the dat\n",
    "plt.imshow(np.transpose(dataloader.dataset[0][0], (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J017G3b9Grk6"
   },
   "source": [
    "### The Network\n",
    "A CNN is defined with 3 layers. The first layer takes in the coloured image (C = 3). This layer has 64 feature maps with a kernel size of 5. The second layer has 32 feature maps with a kernel size of 3. The last layer has C($r^2$) feature maps with a kernel size of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-dbqVPjGNCd"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, r, C):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(C, 64, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, r * r * C, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQYuI340GvRZ"
   },
   "source": [
    "### The Training\n",
    "In the following code, the network is trained. During this training the losses, some models and the best model are saved. On intervals, the current progress is logged also to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_UAB7-mzGM_0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training at: 2020-04-12 12:55:07.466966\n",
      "[1,   100] train_loss: 0.11775\n",
      "[1,   200] train_loss: 0.05125\n",
      "[1,   300] train_loss: 0.04705\n",
      "[1,   400] train_loss: 0.04249\n",
      "[1,   500] train_loss: 0.04044\n",
      "[1,   600] train_loss: 0.04705\n",
      "[1,   700] train_loss: 0.03493\n",
      "[1,   800] train_loss: 0.03528\n",
      "[1,   900] train_loss: 0.03414\n",
      "[1,  1000] train_loss: 0.02885\n",
      "[1,  1100] train_loss: 0.02902\n",
      "[1,  1200] train_loss: 0.02763\n",
      "[1,  1300] train_loss: 0.02608\n",
      "[1,  1400] train_loss: 0.02545\n",
      "[1,  1500] train_loss: 0.02216\n",
      "[1,  1600] train_loss: 0.02256\n",
      "[1,  1700] train_loss: 0.02133\n",
      "[1,  1800] train_loss: 0.01950\n",
      "[1,  1900] train_loss: 0.02107\n",
      "1 0.03626285321413215\n",
      "[1,   100] test_loss: 0.02094\n",
      "[1,   200] test_loss: 0.01977\n",
      "[1,   300] test_loss: 0.01892\n",
      "[1,   400] test_loss: 0.01865\n",
      "1 0.019571838728717022\n",
      "epoch 1: improvement = inf\n",
      "[2,   100] train_loss: 0.01828\n",
      "[2,   200] train_loss: 0.01819\n",
      "[2,   300] train_loss: 0.01754\n",
      "[2,   400] train_loss: 0.01873\n",
      "[2,   500] train_loss: 0.01487\n",
      "[2,   600] train_loss: 0.01552\n",
      "[2,   700] train_loss: 0.01678\n",
      "[2,   800] train_loss: 0.01508\n",
      "[2,   900] train_loss: 0.01435\n",
      "[2,  1000] train_loss: 0.01412\n",
      "[2,  1100] train_loss: 0.01343\n",
      "[2,  1200] train_loss: 0.01318\n",
      "[2,  1300] train_loss: 0.01323\n",
      "[2,  1400] train_loss: 0.01401\n",
      "[2,  1500] train_loss: 0.01204\n",
      "[2,  1600] train_loss: 0.01199\n",
      "[2,  1700] train_loss: 0.01349\n",
      "[2,  1800] train_loss: 0.01084\n",
      "[2,  1900] train_loss: 0.01219\n",
      "2 0.014531448482027323\n",
      "[2,   100] test_loss: 0.01243\n",
      "[2,   200] test_loss: 0.01289\n",
      "[2,   300] test_loss: 0.01054\n",
      "[2,   400] test_loss: 0.01253\n",
      "2 0.01198686770245012\n",
      "epoch 2: improvement = 0.007584971026266901\n",
      "[3,   100] train_loss: 0.01123\n",
      "[3,   200] train_loss: 0.01142\n",
      "[3,   300] train_loss: 0.01066\n",
      "[3,   400] train_loss: 0.01140\n",
      "[3,   500] train_loss: 0.01090\n",
      "Press Ctrl-C to terminate while statement\n",
      "Saving train and test loss\n",
      "Finished training at: 2020-04-12 12:55:55.154553\n"
     ]
    }
   ],
   "source": [
    "# Start training and saving the start time\n",
    "start_time = datetime.datetime.now()\n",
    "print(\"starting training at: \" + str(start_time))\n",
    "\n",
    "# initialize the ESPCN\n",
    "net = Net(r, C)\n",
    "net.double()\n",
    "\n",
    "if use_gpu:\n",
    "    net = net.cuda()\n",
    "\n",
    "# define loss fuction\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr_start, momentum=0)\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "epoch = 0\n",
    "last_epoch_loss_test = float(\"inf\")\n",
    "last_epoch_loss_train = float(\"inf\")\n",
    "ni_counter = 0  # counts the amount of epochs no where no improvement has been made\n",
    "\n",
    "# creating a new folder for saving the models and losses\n",
    "dt_string = start_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "models_folder = \"models\"\n",
    "model_name = \"{}_espcnn_r{}\".format(dt_string, r)\n",
    "\n",
    "try:\n",
    "    os.mkdir(models_folder + '/' + model_name)\n",
    "except:\n",
    "    print(\"Folder {} already exists, overwritting model data\".format(models_folder + '/' + model_name))\n",
    "model_dest = models_folder + '/' + model_name + \"/model_epoch_\"\n",
    "best_model_dest = models_folder + '/' + model_name + \"/best_model\"\n",
    "\n",
    "\n",
    "lr = lr_start\n",
    "\n",
    "best_test_loss = float(\"inf\")  # start with dummy value, keep track of best loss on test dataset\n",
    "best_epoch = 0\n",
    "try:\n",
    "    while True:  # loop over the dataset multiple times\n",
    "        epoch_loss_train = 0.0\n",
    "        running_loss_train = 0.0\n",
    "        for i, data in enumerate(train_dataloader, 0): # loop through the training data\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.double())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            epoch_loss_train += outputs.shape[0] * loss.item()\n",
    "            running_loss_train += loss.item()\n",
    "            if i % logging_interval == logging_interval - 1:\n",
    "                print('[%d, %5d] train_loss: %.5f' %\n",
    "                      (epoch + 1, i + 1, running_loss_train / logging_interval))\n",
    "                running_loss_train = 0.0\n",
    "        epoch_loss_train = epoch_loss_train / len(train_dataloader.dataset)\n",
    "        print(epoch + 1, epoch_loss_train)\n",
    "\n",
    "        epoch_loss_test = 0.0\n",
    "        running_loss_test = 0.0\n",
    "        for i, data in enumerate(test_dataloader, 0):  # loop through the test data and calculate the test error\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.double())\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # print statistics\n",
    "            epoch_loss_test += outputs.shape[0] * loss.item()\n",
    "            running_loss_test += loss.item()\n",
    "            if i % logging_interval == logging_interval - 1:\n",
    "                print('[%d, %5d] test_loss: %.5f' %\n",
    "                      (epoch + 1, i + 1, running_loss_test / logging_interval))\n",
    "                running_loss_test = 0.0\n",
    "        epoch_loss_test = epoch_loss_test / len(test_dataloader.dataset)\n",
    "        print(epoch + 1, epoch_loss_test)\n",
    "\n",
    "        # Calculating the improvement\n",
    "        improvement = best_test_loss - epoch_loss_test\n",
    "\n",
    "        if epoch_loss_test < best_test_loss:  # save best model, 'best' meaning lowest loss on test set\n",
    "            best_test_loss = epoch_loss_test\n",
    "            torch.save(net.state_dict(),\n",
    "                       best_model_dest)  # overwrite best model so the best model filename doesn't change\n",
    "            best_epoch = epoch\n",
    "            best_epoch_train_loss = epoch_loss_train\n",
    "\n",
    "        # stop training if no improvement has been made for 'repeats' epochs\n",
    "        print(\"epoch \" + str(epoch + 1) + \": improvement = \" + str(improvement))\n",
    "        if improvement < no_learning_threshold:\n",
    "            ni_counter += 1\n",
    "        else:\n",
    "            ni_counter = 0\n",
    "\n",
    "        if ni_counter >= repeats:\n",
    "            break\n",
    "\n",
    "        # If  the improvement is too small, make the learning rate smaller with a factor 10\n",
    "        if improvement < mu and lr > lr_end:\n",
    "            lr = lr / 10\n",
    "            print(\"Learning rate decreased to:\", lr)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "        # save losses\n",
    "        losses_train.append(epoch_loss_train)\n",
    "        losses_test.append(epoch_loss_test)\n",
    "        last_epoch_loss_train = epoch_loss_train\n",
    "        last_epoch_loss_test = epoch_loss_test\n",
    "\n",
    "        # every 'epch_save_interval' save the current model\n",
    "        if epoch % epoch_save_interval == 0:\n",
    "            torch.save(net.state_dict(), model_dest + str(epoch + 1))\n",
    "        epoch += 1\n",
    "except KeyboardInterrupt: # this allows for manually stopping the training without losing the progress\n",
    "    print(\"Press Ctrl-C to terminate while statement\")\n",
    "    pass\n",
    "\n",
    "    print('Saving train and test loss')\n",
    "    np.save(models_folder + '/' + model_name + '/loss_train', losses_train)\n",
    "    np.save(models_folder + '/' + model_name + '/loss_test', losses_test)\n",
    "\n",
    "# Show training duration\n",
    "end_time = datetime.datetime.now()\n",
    "print('Finished training at: ' + str(end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSNR\n",
    "Define PSNR (Peak Signal to Noise Ratio) in order to validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(original, compressed):\n",
    "    mse = np.mean((original - compressed) ** 2)\n",
    "    if (mse == 0):  # MSE is zero means no noise is present in the signal .\n",
    "        # Therefore PSNR have no importance.\n",
    "        return 100\n",
    "    max_pixel = 255.0\n",
    "    psnr = 20 * math.log10(max_pixel / math.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "\n",
    "def average_PSNR(folder, net, r, gaussianSigma):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = plt.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            img = resize(img, ((img.shape[0] // r) * r, (img.shape[1] // r) * r))\n",
    "            images.append(img)\n",
    "\n",
    "    sumPSNR = 0\n",
    "    for og_img in images:\n",
    "        img_blurred = gaussian(og_img, sigma=gaussianSigma,\n",
    "                               multichannel=True)  # multichannel blurr so that 3rd channel is not blurred\n",
    "        img = resize(img_blurred, (img_blurred.shape[0] // r, img_blurred.shape[1] // r))\n",
    "        if (len(img.shape) == 2):  # convert image to rgb if it is grayscale\n",
    "            img = np.stack((img, img, img), axis=2)\n",
    "            og_img = np.stack((og_img, og_img, og_img), axis=2)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = torch.Tensor(img).unsqueeze(0).double()\n",
    "        result = net(img).detach().numpy()\n",
    "        sumPSNR += PSNR(PS(result[0], r) * 255, og_img * 255)\n",
    "\n",
    "    return sumPSNR / len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging and saving results\n",
    "Printing and saving the settings and results to console and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train and test loss\n",
      "Finished validation \n",
      "\n",
      "dataset:               T91\n",
      "psnr Set5:             22.537537580714048\n",
      "psnr Set14:            21.95499197706312\n",
      "best epoch:            1\n",
      "loss on training set:  0.014531448482027323\n",
      "loss on test set:      0.01198686770245012\n",
      "r:                     3\n",
      "blur:                  0.25\n",
      "lr_start:              0.01\n",
      "lr_end:                0.0001\n",
      "mu:                    1e-06\n",
      "no_learning_threshold: 1e-08\n",
      "epochs:                3\n",
      "training duration:     0:00:47.687587\n",
      "batch_size:            1\n",
      "train_test_fraction:   0.8\n",
      "model:                 2020-04-12_12-55-07_espcnn_r3\n"
     ]
    }
   ],
   "source": [
    "print('Saving train and test loss')\n",
    "np.save(models_folder + '/' + model_name + '/loss_train', losses_train)\n",
    "np.save(models_folder + '/' + model_name + '/loss_test', losses_test)\n",
    "\n",
    "net.load_state_dict(torch.load(best_model_dest))\n",
    "net.eval()\n",
    "\n",
    "net.cpu()\n",
    "set5_PSNR = average_PSNR(\"./datasets/testing/Set5\", net, r, blur)\n",
    "set14_PSNR = average_PSNR(\"./datasets/testing/Set14\", net, r, blur)\n",
    "\n",
    "print(\"Finished validation \\n\")\n",
    "\n",
    "print(\"dataset:               \" + dataset)\n",
    "print(\"psnr Set5:             \" + str(set5_PSNR))\n",
    "print(\"psnr Set14:            \" + str(set14_PSNR))\n",
    "print(\"best epoch:            \" + str(best_epoch))  # epoch with the lowest loss on the test dataset\n",
    "print(\"loss on training set:  \" + str(best_epoch_train_loss))  # loss for the best epoch\n",
    "print(\"loss on test set:      \" + str(best_test_loss))  # loss for the best epoch\n",
    "print(\"r:                     \" + str(r))\n",
    "print(\"blur:                  \" + str(blur))\n",
    "print(\"lr_start:              \" + str(lr_start))\n",
    "print(\"lr_end:                \" + str(lr_end))\n",
    "print(\"mu:                    \" + str(mu))\n",
    "print(\"no_learning_threshold: \" + str(no_learning_threshold))\n",
    "print(\"epochs:                \" + str(epoch + 1))\n",
    "print(\"training duration:     \" + str(end_time - start_time))\n",
    "print(\"batch_size:            \" + str(batch_size))\n",
    "print(\"train_test_fraction:   \" + str(train_test_fraction))\n",
    "print(\"model:                 \" + model_name)\n",
    "\n",
    "with open(models_folder + '/' + model_name + '/results.csv', mode='w') as csv_file:\n",
    "    fieldnames = ['dataset', 'psnr_Set5', 'psnr_Set14', 'best_epoch', 'training_loss', 'test_loss', 'r', 'blur', 'lr_start', 'lr_end', 'mu', 'no_learning_threshold', 'epochs', 'training_duration', 'batch_size', 'train_test_fraction', 'model']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({\n",
    "        'dataset': dataset,\n",
    "        'psnr_Set5': set5_PSNR,\n",
    "        'psnr_Set14': set14_PSNR,\n",
    "        'best_epoch': best_epoch,\n",
    "        'training_loss': best_epoch_train_loss,\n",
    "        'test_loss': best_test_loss,\n",
    "        'r': r,\n",
    "        'blur': blur,\n",
    "        'lr_start': lr_start,\n",
    "        'lr_end': lr_end,\n",
    "        'mu': mu,\n",
    "        'no_learning_threshold': no_learning_threshold,\n",
    "        'epochs': (epoch + 1),\n",
    "        'training_duration': (end_time - start_time),\n",
    "        'batch_size': batch_size,\n",
    "        'train_test_fraction': train_test_fraction,\n",
    "        'model': model_name})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NB69kyUtGzRF"
   },
   "source": [
    "## Resulting images\n",
    "While we were unable to achieve a PSNR similar to the one achieved in the paper we were trying to reproduce, the network did generate some interesting results. Some of these results on the original test set are shown below. From left to right, the images are the **original image**, upscaled using a **bi-cubic filter** and upscaled using **our ESPCN**.\n",
    "![](rsc/summary.PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "blogpost.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "#Reproducing ESPCN\n",
     "\n",
     "In this notebook we reproduce 91 generated images from the paper **Real-Time Single Image and Video Super-Resolution \n",
     "Using an Efficient Sub-Pixel Convolutional Neural Network**. The code has been created following the paper with gaps \n",
     "filled in by ourselves.\n",
     "The paper presents a convulutional neural network capable of real-time Super-Resolution (SR).\n",
     "They designed a CNN architecture where the feature maps are extracted in the Low-Resolution(LR) space, \n",
     "and introducing an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale\n",
     "the final LR feature maps into the High-Resolution(HR) output.\n",
     "\n",
     "---\n",
     "### SRCNN\n",
     "Recovers SR output image from an upscaled and interpolated version of LR image. \n",
     "Uses bicubic interpolation (as a special case of the deconvolution layer) to upscale image.\n",
     "Deconvolution layer a.k.a. Transposed convolution layer:\n",
     "_insert image_\n",
     "\n",
     "\n",
     "### ESPCN\n",
     "First apply a  layer CNN directly to LR image, then apply a sub-pixel convolution layer that upscales the LR feature \n",
     "maps to produce the SR output image. (to avoid upscaling LR before feeding it into the network)\n",
     "_insert image_\n",
     "For a network composed of L layers, the first L−1 layers can be described as follows:\n",
     "$f^1(\\boldsymbol{I}^{LR};W_1, b_1)= \\phi (W_1 \\ast \\boldsymbol{I}^{LR}+b_1)$\n",
     "$f^l(\\boldsymbol{I}^{LR};W_{1:l}, b_{1:l})= \\phi (W_1 \\ast f^{l-1}(\\boldsymbol{I}^{LR})+b_l)$\n",
     "Where $W_l , b_l , l \\in (1, L - 1)$ are learnable network weights and biases respectively.  \n",
     "$W_l$ is a 2D convolution tensor of size $n_{l-1} \\times n_l \\times k_l \\times k_l$ , where $n_l$ is the number of \n",
     "features at layer $l$, $n_0 = C$, and $k_l$ is the filter size at layer $l$. \n",
     "The biases $b_l$ are vectors of length $n_l$ . The nonlinearity function (or activation function) $\\phi$ is applied \n",
     "element-wise and is fixed. The last layer $f^L$ has to convert the LR feature maps to a HR image $\\boldsymble{I}^{SR}$.\n",
     "\n",
     "---\n",
     "\n",
     "## Experiment Setup \n",
     "\n",
     "Input = H x W x C\n",
     "Output = rH x rW x C\n",
     "\n",
     "Apply l layer CNN directly to Low Resolution (LR)\n",
     "Apply sub-pixel convolution layer upscaling LR feature maps\n",
     "\n",
     "Each layer except the last:\n",
     "$f^1(\\boldsymbol{I}^{LR};W_1, b_1)= \\phi (W_1 \\ast \\boldsymbol{I}^{LR}+b_1)$\n",
     "$f^l(\\boldsymbol{I}^{LR};W_{1:l}, b_{1:l})= \\phi (W_1 \\ast f^{l-1}(\\boldsymbol{I}^{LR})+b_l)$\n",
     "\n",
     "W & b learnable parameters\n",
     "W is a 2D convolution tensor of size n_(l-1) x n_l  x k_l x k_l\n",
     "    n_l is the amount of features of layer l, n_0 = C, k_l is the filter size at l\n",
     "    biases are of length n_l\n",
     "    activation function is fixed and element-wise\n",
     "\n",
     "Last layer converts LR to HR\n",
     "\n",
     "Efficient sub-pixel convolution layer: (biggest addition)\n",
     "Convolution with stride 1/r over LR with filter W_s of size k_s and weight spacing 1/r\n",
     "Weights between pixels are not activated and do not need to be calculated\n",
     "The number of activation patterns is exactly r^2.\n",
     "Each pattern has at most ceil(k_s/r)^2 weights\n",
     "Patterns are periodically activated during convolution of the filter depending on the subpixel location mod(x, r), mod(y,r), where x and y are the output pixel coordinates in HR.\n",
     "Solution for mod(k_s, r) = 0\n",
     "Last layer: note NO ACTIVATION\n",
     " $$I^{SR} = f^L(I^{LR}) = PS(W_L * f^{L-1}(I^{LR})+b_L)$$\n",
     "PS is a periodic shuffling operator (sort of mapping)\n",
     "_formule naar latex_\n",
     "W_L has shape n_(L-1) x r^2C x k_L x k_L\n",
     "k_L = k_s/r and mod(k_s, r) = 0\n",
     "\n",
     "Loss function: (mean squared error)\n",
     "_formule naar latex_\n",
     "Preshuffle training data avoiding the use of PS\n",
     "\n",
     "---\n",
     "# The model\n",
     "\n",
     "Everything we need is imported\n",
     "_imports block_\n",
     "\n",
     "---\n",
     "\n",
     "hyperparameters to set\n",
     "\n",
     "_hyperparameters block_"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
